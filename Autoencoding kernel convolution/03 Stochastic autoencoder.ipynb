{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Stochastic autoencoder 3.ipynb","version":"0.3.2","provenance":[{"file_id":"1XZvLnmtu4QlHAXGCbmO2TFPTYFBrkfk1","timestamp":1548824028507},{"file_id":"1rlqZ2AX-ttlaa3rLSvr4zmN0Azj_Jf4m","timestamp":1548804679627}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"S2dPTjE1EOcR","colab_type":"code","outputId":"bc731853-cd5f-42f7-f612-2dc5ae07aae1","executionInfo":{"status":"ok","timestamp":1548876292845,"user_tz":480,"elapsed":8546,"user":{"displayName":"Amol Kelkar","photoUrl":"","userId":"07278259258766517376"}},"colab":{"base_uri":"https://localhost:8080/","height":82}},"cell_type":"code","source":["# http://pytorch.org/\n","from os.path import exists\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n","accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n","\n","#!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n","!pip install torch\n","import torch"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.0)\n"],"name":"stdout"}]},{"metadata":{"id":"fjVU5rmg7Phz","colab_type":"text"},"cell_type":"markdown","source":["# Stochastic autoencoder 3\n","This notebook implements part of the [eager model](https://docs.google.com/drawings/d/1czjcBtDQGS8X6bnIbYU4wmFvv1AfZt5wwSRk9oyQGw0/edit). Here we continue where we left off in [part 2](https://colab.research.google.com/drive/1XZvLnmtu4QlHAXGCbmO2TFPTYFBrkfk1#scrollTo=o5DfRieOB3rq&uniqifier=1).\n","\n","Let's put together a cell with train(), up(), down() functions"]},{"metadata":{"id":"aU-YTik28sU6","colab_type":"text"},"cell_type":"markdown","source":["## Basics"]},{"metadata":{"id":"F_z2dpGjETHC","colab_type":"code","colab":{}},"cell_type":"code","source":["import math\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import pdb\n","import numpy as np\n","import random\n","from scipy.ndimage.filters import gaussian_filter\n","from scipy import stats\n","from scipy.stats import norm\n","import os\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","from skimage.draw import line_aa\n","%matplotlib inline\n","plt.style.use('classic')\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# TODO: Use torch.normal(mean, std=1.0, out=None) \n","class NormalDistributionTable(object):\n","    def __init__(self, resolution, var=0.07, table_resolution=100):\n","      self.resolution = resolution\n","      self.var = var\n","      self.table_resolution = table_resolution\n","      self.gaussians = torch.tensor([norm.pdf(np.arange(0, 1, 1.0 / self.resolution), mean, self.var) for mean in np.linspace(0, 1, self.table_resolution)])\n","      self.gaussians = self.gaussians.transpose(0, 1)\n","      self.gaussians = self.gaussians / self.gaussians.sum(dim=0)\n","      self.gaussians = self.gaussians.transpose(0, 1)\n","\n","    def lookup(self, mean):\n","      assert mean >= 0 and mean <= 1, \"mean must be between 0 and 1\"\n","      index = math.floor(mean * self.table_resolution)\n","      if index == self.table_resolution:\n","        index = self.table_resolution - 1\n","      return self.gaussians[index]\n","\n","    def to_pdf(self, images):\n","      element_count = np.prod(images.shape)\n","      images_shape = images.shape\n","      images_view = images.contiguous().view((element_count,))\n","      images_pdf = torch.stack([self.lookup(mean.item()) for mean in images_view])\n","      images_pdf = images_pdf.view(images_shape[:-1] + (images_shape[-1] * self.resolution, ))\n","      return images_pdf\n","\n","\n","def generate_images(width, height, count=100):\n","    images = []\n","    for _ in range(100):\n","        image = np.zeros((width, height))\n","        rr, cc, val = line_aa(random.randint(0, height-1), random.randint(0, width-1), random.randint(0, height-1), random.randint(0, width-1))\n","        image[rr, cc] = val\n","        image=gaussian_filter(image, 0.5)\n","        images.append(image)\n","\n","    return torch.as_tensor(images).to(device)\n","\n","def generate_moving_line(width, height, count=100):\n","  images = []\n","  for i in range(int(count/2)):\n","    image = np.zeros((width, height))\n","    rr, cc, val = line_aa(2, 3-i, width-2, height-1-i)\n","    image[rr, cc] = val\n","    image=gaussian_filter(image, 0.5)\n","    images.append(image)\n","\n","  for i in range(int(count/2)):\n","    image = np.zeros((width, height))\n","    rr, cc, val = line_aa(width-1-i, 2-i, 4-i, height-2-i)\n","    image[rr, cc] = val\n","    image=gaussian_filter(image, 0.5)\n","    images.append(image)\n","\n","  return torch.as_tensor(images).to(device)\n","\n","    \n","def show_image(image, vmin=None, vmax=None, title=None, print_values=False):\n","    #print(\"image \", image.shape)\n","    image = image.cpu().numpy()\n","    fig, ax1 = plt.subplots(figsize=(20, 8))\n","    if title:\n","        plt.title(title)\n","    #i = image.reshape((height, width))\n","    #print(\"i \", i.shape)\n","    ax1.imshow(image, vmin=vmin, vmax=vmax, interpolation='none', cmap=plt.cm.plasma)\n","    plt.show()\n","    if print_values:\n","        print(image)\n","        \n","def sample_from_pdf1(pdf):\n","    assert pdf.shape == (resolution, )\n","\n","    pk = pdf.copy()\n","    xk = np.arange(resolution)\n","    pk[pk<0] = 0\n","    sum_pk = sum(pk)\n","    if sum(pk) > 0:\n","        pk = pk / sum_pk\n","        custm = stats.rv_discrete(name='custm', values=(xk, pk))\n","        value = custm.rvs(size=1) / resolution\n","        # apply scale (conflates value and confidence!)\n","        value = value * sum_pk\n","        return value\n","    else:\n","        return [0]\n","\n","def sample_from_pdf(pdf):\n","    assert pdf.shape == (resolution, )\n","    #print(\"pdf \", pdf)\n","\n","    sum_pdf = sum(pdf)\n","    #print(\"sum_pdf \", sum_pdf)\n","\n","    if sum_pdf > 0:\n","        v = random.random()\n","        #print(\"v \", v)\n","\n","        s = 0\n","        index = 0\n","        while s < v and index < resolution:\n","          s += pdf[index] / sum_pdf\n","          index += 1\n","          #print(\"  s \", s)\n","          #print(\"  index \", index)\n","          \n","        # apply scale (conflates value and confidence!)\n","        return [(index - 1) * sum_pdf / resolution]\n","    else:\n","        return [0]\n","\n","\n","def sample_from_images__(images__):\n","    assert len(images__.shape) == 3\n","\n","    # reshape images__ from (image count, height, width*resolution) into (image count*height*width, resolution)\n","    s = images__.shape\n","    flattened_images__ = images__.view(s[0], s[1], int(s[2] / resolution), resolution)\n","    s = flattened_images__.shape\n","    flattened_images__ = flattened_images__.view(s[0] * s[1] * s[2], s[3])\n","\n","    # sample single value from each distributions into (image count*height*width, 1)\n","    sampled_pixels = torch.Tensor([sample_from_pdf(item.cpu().numpy()) for item in flattened_images__])\n","\n","    # reshape back into (image count, height, width)\n","    sampled_images = sampled_pixels.view(s[0], s[1], s[2])\n","\n","    return sampled_images\n","\n","\n","def averaged_sample_from_images__(images__, count=10):\n","    sampled_images = torch.stack([sample_from_images__(images__) for i in range(count)])\n","    return sampled_images.mean(dim=0)\n","\n","\n","def aggregate_to_pdf(mu_bar, image_count, samples_per_image, iH, iW, resolution):\n","  #print(\"aggregate_to_pdf mu_bar\", mu_bar.shape)\n","  # mu_bar                          (image_count * samples_per_image, iH, iW)\n","  # mu_bar_per_image                (image_count,  samples_per_image, iH, iW)\n","  mu_bar = mu_bar.clamp(0, 1)\n","  mu_bar_per_image = mu_bar.view(image_count, samples_per_image, iH, iW)\n","\n","  # mu_bar_per_image_flattened      (image_count,  iH,  iW, samples_per_image)\n","  mu_bar_per_image_flattened = mu_bar_per_image.permute(0, 2, 3, 1).contiguous()\n","  # mu_bar_per_image_flattened      (image_count * iH * iW, samples_per_image)\n","  mu_bar_per_image_flattened = mu_bar_per_image_flattened.view(image_count * iH * iW, samples_per_image)\n","\n","\n","  # mu_bar_flattened__              (image_count * iH * iW, resolution)\n","  mu_bar_flattened__ = torch.zeros((image_count * iH * iW, resolution))\n","  assert mu_bar_per_image_flattened.shape[0] == mu_bar_flattened__.shape[0]\n","\n","  for sample_index in range(samples_per_image):\n","    #print(\"mu_bar_per_image_flattened[:, sample_index] \", mu_bar_per_image_flattened[:, sample_index])\n","    histogram_indices = (mu_bar_per_image_flattened[:, sample_index] * resolution).long().cpu()\n","    for item_index in range(mu_bar_per_image_flattened.shape[0]): # TODO: Vectorize!\n","      mu_bar_flattened__[item_index][histogram_indices[item_index]] += 1\n","\n","  # mu_bar__                        (image_count, iH, iW * resolution)\n","  mu_bar__ = mu_bar_flattened__.view((image_count, iH, iW,  resolution))\n","  mu_bar__ = torch.nn.functional.normalize(mu_bar__, p=1, dim=3)\n","  mu_bar__ = mu_bar__.view(          (image_count, iH, iW * resolution))\n","\n","  return mu_bar__\n","\n","# Assume input (samples, feature maps, height, width) and that \n","# features maps is a perfect squere, e.g. 9, of an integer 'a', e.g. 3 in this case\n","# Output (samples, height * a, width * a)\n","def flatten_feature_maps(f):\n","    s = f.shape\n","    f = f.permute(0, 2, 3, 1) # move features to the end\n","    s = f.shape\n","    a = int(s[3] ** 0.5)  # feature maps are at pos 3 now that we want to first split into a square of size (a X a)\n","    assert a * a == s[3], \"Feature map count must be a perfect square\"\n","    f = f.view(s[0], s[1], s[2], a, a)\n","    f = f.permute(0, 1, 3, 2, 4).contiguous() # frame count, height, sqr(features), width, sqr(features)\n","    s = f.shape\n","    f = f.view(s[0], s[1] * s[2], s[3] * s[4]) # each point becomes a square of features\n","    return f\n","  \n","# Assume input (samples, height * a, width * a)\n","# Output (samples, feature maps, height, width)\n","def unflatten_feature_maps(f, a):\n","    s = f.shape\n","    f = f.view(s[0], int(s[1] / a), a, int(s[2] / a), a)\n","    \n","    f = f.permute(0, 1, 3, 2, 4).contiguous() # move features to the end\n","    s = f.shape\n","    f = f.view(s[0], s[1], s[2], a * a).permute(0, 3, 1, 2)\n","    return f\n","\n","class EMA:\n","  def __init__(self, mu):\n","    super(EMA, self).__init__()\n","    self.mu = mu\n","\n","  def forward(self,x, last_average):\n","    new_average = self.mu*x + (1-self.mu)*last_average\n","    return new_average\n","  \n","\n","resolution = 10\n","var = 0.05\n","normal_distribution_table = NormalDistributionTable(resolution=resolution, var=var)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QSSDHTtfG3tU","colab_type":"text"},"cell_type":"markdown","source":["## Autoencoder"]},{"metadata":{"id":"efSLelurHIIY","colab_type":"code","colab":{}},"cell_type":"code","source":["class AutoEncoder(nn.Module):\n","  def __init__(self, a=3):\n","    super(AutoEncoder, self).__init__()\n","    self.a = a\n","    self.encoder = nn.Sequential(                                             # b, 1, w, h\n","      nn.Conv2d(1, 2 * a * a, 3, stride=1, padding=1),                        # b, 2 * a * a, w, h\n","      nn.ReLU(True),\n","      nn.MaxPool2d(2, stride=2),                                              # b, 2 * a * a, w/2, h/2\n","      nn.Conv2d(2 * a * a, a * a, 3, stride=1, padding=1),                    # b, a * a, w/2, h/2\n","      nn.ReLU(True),\n","      nn.MaxPool2d(2, stride=2),                                              # b, a * a, w/4, h/4\n","      nn.MaxPool2d(2, stride=2),                                              # b, a * a, w/8, h/8\n","      nn.Sigmoid(),\n","    )\n","    self.decoder = nn.Sequential(\n","      nn.ConvTranspose2d(a * a, 2 * a * a, 3, stride=2, padding=1, output_padding=1), # b, 2 * a * a, w/4, h/4\n","      nn.ReLU(True),\n","      nn.ConvTranspose2d(2 * a * a, 2 * a * a, 3, stride=2, padding=1, output_padding=1), # b, 2 * a * a, w/2, h/2\n","      nn.ReLU(True),\n","      nn.ConvTranspose2d(2 * a * a, 1, 3, stride=2, padding=1, output_padding=1),     # b, 1, w, h\n","      nn.Sigmoid()\n","    )\n","\n","    self.encoder_output = None\n","\n","  def forward(self, x):\n","    assert x.shape[-1] % 4 == 0, \"Width and height must be a multiple of 4\"\n","    x = self.encoder_output = self.encoder(x)\n","    x = self.decoder(x)\n","    return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YvAY2FazH2TT","colab_type":"text"},"cell_type":"markdown","source":["## Unit"]},{"metadata":{"id":"ryNllYG_H4hj","colab_type":"code","colab":{}},"cell_type":"code","source":["class Unit:\n","  def __init__(self, unit_index, samples_per_image=25, a=3, resolution=10):\n","    self.unit_index = unit_index\n","    self.samples_per_image = samples_per_image\n","    self.a = a\n","    self.model = AutoEncoder(a=a).to(device)\n","    self.ema = EMA(0.5)\n","    self.image_count = None\n","    self.image_size = None\n","    self.resolution = resolution\n","    self.trained = False\n","\n","    if os.path.exists(self.save_path()):\n","      self.model.load_state_dict(torch.load(self.save_path()))\n","      self.model.eval()\n","      self.trained = True\n","\n","  def up(self):\n","    if self.model.encoder_output is None:\n","      raise Error(\"must call train() before up()\")\n","      return\n","    \n","    h1 = self.model.encoder_output\n","    h1_flattened = flatten_feature_maps(h1)\n","    #print(\"images        \", images.shape)\n","    #print(\"mu1__         \", mu1__.shape)\n","    #print(\"h1            \", h1.shape)\n","    #print(\"h1_flattened  \", h1_flattened.shape)\n","\n","    h1__ = normal_distribution_table.to_pdf(h1_flattened)\n","\n","    last_average = h1__[0].clone()\n","    for index in range(1, h1__.shape[0]):\n","      h1__[index] = last_average = self.ema.forward(h1__[index], last_average)\n","\n","    #print(\"h1__          \", h1__.shape)\n","    return h1__\n","  \n","  \n","  def down(self, u2_bar__):\n","    if self.model.encoder_output is None:\n","      raise Error(\"must call train() before down()\")\n","      return\n","\n","    sampled_h1 = sample_from_images__(u2_bar__)\n","    #print(\"sampled_h1             \", sampled_h1.shape)\n","    unflattened_sampled_h1 = unflatten_feature_maps(sampled_h1, self.a).to(device)\n","    #print(\"**unflattened_sampled_h1 \", unflattened_sampled_h1.shape)\n","    h1 = self.model.encoder_output\n","    #print(\"**h1 \", h1.shape)\n","\n","    #show_image(sampled_h1[0].detach(), title=f\"sampled_h1 {0}\", vmin=0, vmax=1)\n","    #show_image(unflattened_sampled_h1[0, 0].detach(), title=f\"unflattened_sampled_h1 {0}\", vmin=0, vmax=1)\n","    #show_image(h1[0, 0].detach(), title=f\"h1 {0}\", vmin=0, vmax=1)\n","\n","\n","    #merged_h1 = (unflattened_sampled_h1 + h1) / 2.0\n","    #merged_h1 = unflattened_sampled_h1 * 0.5 + h1 * 0.5\n","    merged_h1 = unflattened_sampled_h1 * h1\n","    \n","    decoded_mu1 = self.model.decoder.forward(merged_h1)\n","    decoded_mu1 = decoded_mu1[:, 0, :, :]\n","    #print(\"decoded_mu1            \", decoded_mu1.shape)\n","    #print(\"image_count            \", self.image_count)\n","    #print(\"samples_per_image      \", self.samples_per_image)\n","    #print(\"image_size             \", self.image_size)\n","\n","    mu1_bar__ = aggregate_to_pdf(mu_bar=decoded_mu1, image_count=self.image_count, samples_per_image=self.samples_per_image, iH=self.image_size, iW=self.image_size, resolution=self.resolution)\n","    \n","    return mu1_bar__\n","\n","    \n","  def train(self, mu1__, num_epochs=3000):\n","    self.image_count, self.image_size, _ = mu1__.shape\n","\n","    learning_rate = 1e-3\n","    criterion = nn.MSELoss()\n","    optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate,\n","                                 weight_decay=1e-5)\n","\n","    #print(\"mu__\", mu1__.shape)\n","    mu1_duplicated__ = torch.stack([mu__.clone() for mu__ in mu1__ for _ in range(self.samples_per_image)])\n","    #print(\"Duplicated PDFs for images in animation: \", mu1__.shape)\n","    mu1 = sample_from_images__(mu1_duplicated__)\n","    #print(\"mu1: Sampled images in animation: \", mu1.shape)\n","\n","    input = mu1[:, None, :, :].to(device)\n","\n","    if self.trained:\n","      output = self.model(input)\n","    else:\n","      done = False\n","      epoch = 0\n","      while not done:\n","        output = self.model(input)\n","        loss = criterion(output, input)\n","        # ===================backward====================\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if epoch % int(num_epochs / 10) == 0:\n","         print('epoch [{}/{}], loss:{:.4f}'\n","               .format(epoch+1, num_epochs, loss.item()))\n","\n","        if (loss.item() < 0.01 and epoch > 1000) or epoch > num_epochs:\n","          done = True\n","          \n","        epoch += 1\n","\n","\n","      self.trained = True\n","      torch.save(self.model.state_dict(), self.save_path())\n","\n","    return mu1, output[:,0,:,:]\n","  \n","  def save_path(self):\n","    return f\"unit_{self.unit_index}.pt\"\n","\n","\n","class UnitStack:\n","  def __init__(self, resolution=resolution):\n","    self.units = []\n","    self.resolution = resolution\n","    \n","  def append_unit(self):\n","    if len(self.units) == 0:\n","      samples_per_image  = 10\n","    else:\n","      samples_per_image  = 1\n","    \n","    unit = Unit(len(self.units), samples_per_image=samples_per_image, a=4, resolution=self.resolution)\n","    self.units.append(unit)\n","    return unit\n","\n","  def process(self, mu1__):\n","    return self.process_unit(0, mu1__)\n","\n","  def process_unit(self, unit_index, mu1__):\n","    unit = self.units[unit_index]\n","\n","    print(f\"mu{unit_index}__        :\", mu1__.shape)\n","    unit.train(mu1__, num_epochs=3000)\n","\n","    h1__ = unit.up()\n","    print(f\"h{unit_index}__         :\", h1__.shape)\n","    \n","    if unit_index < len(self.units) - 1:\n","      unext_bar__ = self.process_unit(unit_index + 1, h1__)\n","    else:\n","      print(\"No next unit\")\n","      unext_bar__ = h1__\n","\n","    mu1_bar__ = unit.down(unext_bar__)\n","    print(f\"mu{unit_index}_bar__    :\", mu1_bar__.shape)\n","    \n","    return mu1_bar__\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7S9Yqr8wbruT","colab_type":"text"},"cell_type":"markdown","source":["## Example"]},{"metadata":{"id":"XaYE0tslbzMQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":232},"outputId":"e4e2bd9b-2782-4f4a-9691-39f020ae3a0f","executionInfo":{"status":"error","timestamp":1548969934276,"user_tz":480,"elapsed":1628,"user":{"displayName":"Amol Kelkar","photoUrl":"","userId":"07278259258766517376"}}},"cell_type":"code","source":["image_size = 16\n","image_count = image_size\n","np.random.seed(0)\n","torch.manual_seed(0)\n","\n","images = generate_moving_line(image_size, image_size, count=image_count).float()\n","#print(\"Distinct images in animation: \", images.shape)\n","mu1__ = normal_distribution_table.to_pdf(images)\n","#print(\"mu1__: PDFs for images in animation: \", mu1__.shape)\n","\n","unit_stack = UnitStack(resolution=resolution)\n","unit_stack.append_unit()\n","unit_stack.append_unit()\n","unit_stack.append_unit()\n","mu1_bar__ = unit_stack.process(mu1__)\n","\n","sampled_images = sample_from_images__(mu1__)\n","for i in range(sampled_images.shape[0]):\n","  show_image(images[i].detach(), title=f\"images {i}\", vmin=0, vmax=1)\n","  show_image(mu1_bar__[i].detach(), title=f\"mu1_bar__ {i}\", vmin=0, vmax=1)\n","  show_image(sampled_images[i].detach(), title=f\"sampled_images {i}\", vmin=0, vmax=1)"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-5b9d2ba3f766>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimage_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]}]},{"metadata":{"id":"KuP7C7pUOZPO","colab_type":"code","colab":{}},"cell_type":"code","source":["# import os\n","# import glob\n","\n","# files = glob.glob('./*.pt')\n","# for f in files:\n","#     os.remove(f)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"y5uBUuzZIWoH","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}