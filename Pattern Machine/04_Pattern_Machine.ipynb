{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pattern Machine.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNljesUGFXiqk4ogP1jqsjF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "48a02fc2840544169c9087fd11c53c63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bd3efcf4e7f3454883ed7ec8b6001834",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5d4a2dfc7d8749b99f0335532b99f258",
              "IPY_MODEL_a457d294885048be86cbee8faa12a4bb"
            ]
          }
        },
        "bd3efcf4e7f3454883ed7ec8b6001834": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5d4a2dfc7d8749b99f0335532b99f258": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_dabad3eda46b44f8813d1c503aee866c",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 30,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 30,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b34e890c4f234267bab52eafba543dd4"
          }
        },
        "a457d294885048be86cbee8faa12a4bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f6f0c2c8d81049caa0f8e184b95f8272",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 30/30 [00:01&lt;00:00, 15.71it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_159ef09e922a4a459921c0e1b35e4743"
          }
        },
        "dabad3eda46b44f8813d1c503aee866c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b34e890c4f234267bab52eafba543dd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f6f0c2c8d81049caa0f8e184b95f8272": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "159ef09e922a4a459921c0e1b35e4743": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amolk/AGI-experiments/blob/master/Pattern%20Machine/04_Pattern_Machine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH0h1CpkBIIc"
      },
      "source": [
        "Pattern machine\n",
        "- continuous, i.e. activation changes over time as decaying history of instantaneous activation\n",
        "- modular, i.e. connect up more flexibly,\n",
        "- use multi-patterns, i.e. pattern contains more than 1 weights tensor. This is to represent input and output jointly.\n",
        "\n",
        "Architectural decisions\n",
        "- Signal, SignalGrid, CompositeSignalGrid\n",
        "- input patches may overlap when utility factor > 1, fixed number (a grid) of patterns per patch\n",
        "- output neighborhood is the patterns corresponding to a neighborhood of patches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4SBcILxqTNy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87a849e1-db5e-47ba-a285-da51db396c20"
      },
      "source": [
        "!pip install ipytest"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ipytest\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/d6/1797c114d57ec1c93b8078d81bd09b9f82d5f3a989c11fd1c575ff2846e7/ipytest-0.9.1-py3-none-any.whl\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from ipytest) (20.8)\n",
            "Collecting pytest>=5.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/15/5ef931cbd22585865aad0ea025162545b53af9319cf38542e0b7981d5b34/pytest-6.2.1-py3-none-any.whl (279kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from ipytest) (5.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->ipytest) (2.4.7)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.6/dist-packages (from pytest>=5.4->ipytest) (1.1.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from pytest>=5.4->ipytest) (0.10.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.4->ipytest) (20.3.0)\n",
            "Collecting pluggy<1.0.0a1,>=0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.4->ipytest) (3.3.0)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.4->ipytest) (1.10.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->ipytest) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->ipytest) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->ipytest) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->ipytest) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->ipytest) (51.0.0)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->ipytest) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->ipytest) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->ipytest) (4.3.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.4->ipytest) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.4->ipytest) (3.7.4.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipytest) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipytest) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ipytest) (0.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->ipytest) (0.2.0)\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pluggy, pytest, ipytest\n",
            "  Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "Successfully installed ipytest-0.9.1 pluggy-0.13.1 pytest-6.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQhsjAS1XKB5"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 1\n",
        "import ipytest\n",
        "ipytest.autoconfig()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WI0DQENlMSC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e4bf3dd-c3e4-4993-ce56-58329024d63e"
      },
      "source": [
        "%%writefile utils.py\n",
        "import pdb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
        "\n",
        "def pretty_s(name, clas, indent=0):\n",
        "  if type(clas).__name__ == 'Tensor':\n",
        "    return ' ' * indent + name + \":\" + type(clas).__name__ + \" size\" + str(tuple(clas.shape))\n",
        "\n",
        "  strs = []\n",
        "  strs.append(' ' * indent + name + \":\" + type(clas).__name__)\n",
        "\n",
        "  indent += 2\n",
        "  for k,v in clas.__dict__.items():\n",
        "    if '__dict__' in dir(v):\n",
        "      strs.append(pretty_s(k, v,indent))\n",
        "    elif '__iter__' in dir(v):\n",
        "      if type(v) is tuple:\n",
        "        strs.append(' ' * indent + k + ' = ' + str(v))\n",
        "      else:\n",
        "        strs.append(' ' * indent +  k + ' = [')\n",
        "        for index, item in enumerate(v):\n",
        "          if '__dict__' in dir(item):\n",
        "            strs.append(pretty_s(str(index), item, indent+2))\n",
        "          else:\n",
        "            strs.append(' ' * (indent+2) + str(item))\n",
        "        strs.append(' ' * indent +  ']')\n",
        "    else:\n",
        "      strs.append(' ' * indent +  k + ' = ' + str(v))\n",
        "\n",
        "  return \"\\n\".join(strs)\n",
        "          \n",
        "def pretty_print(name, clas, indent=0):\n",
        "  print(pretty_s(name, clas, indent))\n",
        "\n",
        "def soft_add(a, b, tau):\n",
        "  return a * (1 - tau) + b * tau\n",
        "\n",
        "def add_gaussian_noise(tensor, mean=0., std=1.):\n",
        "    t = tensor + torch.randn(tensor.size()).to(device) * std + mean\n",
        "    t.to(device)\n",
        "    return t\n",
        "\n",
        "def plot_patterns(patterns, pattern_lr, dataset, voronoi=False, annotate=False, figsize=(7,7), dpi=100):\n",
        "  patterns = patterns.cpu()\n",
        "  dataset = dataset.cpu()\n",
        "  assert len(patterns.shape) == 2 # (pattern count, 2)\n",
        "  assert patterns.shape[1] == 2 # 2D\n",
        "\n",
        "  rgba_colors = torch.zeros((patterns.shape[0], 4))\n",
        "\n",
        "  # for blue the last column needs to be one\n",
        "  rgba_colors[:,2] = 1.0\n",
        "  # the fourth column needs to be your alphas\n",
        "  if pattern_lr is not None:\n",
        "    alpha = (1.1 - pattern_lr.cpu()).clamp(0, 1) * 0.9\n",
        "    rgba_colors[:, 3] = alpha\n",
        "  else:\n",
        "    rgba_colors[:, 3] = 1.0\n",
        "\n",
        "  plt.figure(figsize=figsize, dpi=dpi)\n",
        "  ax = plt.gca()\n",
        "  ax.cla() # clear things for fresh plot\n",
        "\n",
        "  if annotate:\n",
        "    for i in range(patterns.shape[0]):\n",
        "      ax.annotate(str(i), (patterns[i][0], patterns[i][1]), xytext=(5,-3), textcoords='offset points')\n",
        "\n",
        "  ax.scatter(patterns[:, 0], patterns[:, 1], marker='.', c=rgba_colors, s=50)\n",
        "  ax.scatter(dataset[:, 0], dataset[:, 1], marker='.', c='r', s=10)\n",
        "\n",
        "  if voronoi:\n",
        "    vor = Voronoi(patterns)\n",
        "    vor_fig = voronoi_plot_2d(vor, ax=ax, show_vertices=False, line_colors='gray',\n",
        "                              line_width=1, line_alpha=0.2, point_size=0)\n",
        "\n",
        "  ax.set_xlim(0, 1)\n",
        "  ax.set_ylim(0, 1)\n",
        "  plt.show()\n",
        "\n",
        "\"\"\"\n",
        "Create a numpy array of given shape, each element initialized using supplied function\n",
        "Example: make_ndarray((2,3), lambda multi_index:multi_index)\n",
        "\"\"\"\n",
        "def make_ndarray(shape, fn):\n",
        "  a = np.empty(shape, dtype=object)\n",
        "  with np.nditer(a, flags=['refs_ok', 'multi_index'], op_flags=['readwrite']) as it:\n",
        "    for x in it:\n",
        "      a[it.multi_index] = fn(it.multi_index)\n",
        "\n",
        "  return a"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFuOT8nFZ9pl"
      },
      "source": [
        "# !pip install matplotlib scipy\n",
        "%aimport utils"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lbp81xkiBF_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93c50cf1-5251-4e29-b711-7f12ff76e4a1"
      },
      "source": [
        "%%writefile pattern.py\n",
        "\n",
        "import torch\n",
        "import numpy\n",
        "import pdb\n",
        "from typing import List, Tuple\n",
        "from utils import pretty_s\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class GridShapeMismatchError(Exception): pass\n",
        "class NoComponentsError(Exception): pass\n",
        "\n",
        "class SignalUtils:\n",
        "  @staticmethod\n",
        "  def compute_precision(variance):\n",
        "    return torch.exp(-variance)\n",
        "\n",
        "class SignalGridHP:\n",
        "  def __init__(self, grid_shape:Tuple, signal_shape:Tuple, init_pixel_scale:float=0.1, init_variance:float=10):\n",
        "    self.grid_shape = grid_shape\n",
        "    self.grid_size = np.prod(self.grid_shape)\n",
        "    if self.grid_size <= 0:\n",
        "      raise ValueError(\"Invalid grid size\")\n",
        "\n",
        "    self.signal_shape = tuple(signal_shape)\n",
        "    self.signal_size = np.prod(self.signal_shape)\n",
        "    if self.signal_size <= 0:\n",
        "      raise ValueError(\"Invalid signal size\")\n",
        "\n",
        "    self.init_pixel_scale = init_pixel_scale\n",
        "    self.init_variance = init_variance\n",
        "\n",
        "class SignalGrid:\n",
        "  def __init__(self, hp:SignalGridHP, alloc_pixels=True, pixels=None):\n",
        "    self.hp = hp\n",
        "    if pixels is not None:\n",
        "      self.pixels = pixels\n",
        "    elif alloc_pixels:\n",
        "      self.pixels = torch.rand((hp.grid_size, hp.signal_size)).to(device) * hp.init_pixel_scale\n",
        "    else:\n",
        "      self.pixels = None\n",
        "\n",
        "    self.variance = torch.ones((hp.grid_size, hp.signal_size)).to(device) * hp.init_variance\n",
        "    self.refresh_precision()\n",
        "\n",
        "  def refresh_precision(self):\n",
        "    self.precision = SignalUtils.compute_precision(self.variance)\n",
        "\n",
        "  @property\n",
        "  def signal_shape(self):\n",
        "    return self.hp.signal_shape\n",
        "\n",
        "  @property\n",
        "  def __dict__(self):\n",
        "    return {\n",
        "        'grid_shape': self.hp.grid_shape,\n",
        "        'signal_shape': self.hp.signal_shape,\n",
        "        'pixels': self.pixels,\n",
        "        'precision': self.precision\n",
        "    }\n",
        "\n",
        "  def __repr__(self):\n",
        "    return pretty_s(\"\", self)\n",
        "\n",
        "class CompositeSignalGridHP:\n",
        "  def __init__(self, hps:List[SignalGridHP]):\n",
        "    if len(hps) == 0:\n",
        "      raise NoComponentsError(\"Must specify at least one component\")\n",
        "\n",
        "    self.components = hps\n",
        "    self.grid_shape = hps[0].grid_shape\n",
        "\n",
        "    # all components must have same grid size\n",
        "    for component_hp in hps:\n",
        "      if component_hp.grid_shape != hps[0].grid_shape:\n",
        "        raise GridShapeMismatchError\n",
        "\n",
        "class CompositeSignalGrid:\n",
        "  @staticmethod\n",
        "  def from_pixels_list(pixels_list:List[torch.Tensor], variance:float=0.0):\n",
        "    signal_hps = [SignalGridHP(grid_shape=(1,1), signal_shape=pixels.shape, init_variance=variance) for pixels in pixels_list]\n",
        "    hp = CompositeSignalGridHP(hps=signal_hps)\n",
        "    result = CompositeSignalGrid(hp=hp, alloc=False)\n",
        "    for index, component in enumerate(result.components):\n",
        "      component.pixels = pixels_list[index]\n",
        "    return result\n",
        "\n",
        "  @staticmethod\n",
        "  def from_pixels_and_variances_list(pixels_list:List[torch.Tensor], variances_list:List[torch.Tensor], signal_shape:List[Tuple]):\n",
        "    signal_hps = [SignalGridHP(grid_shape=(1,1), signal_shape=signal_shape[index], init_variance=0.0) for index, pixels in enumerate(pixels_list)]\n",
        "    hp = CompositeSignalGridHP(hps=signal_hps)\n",
        "    result = CompositeSignalGrid(hp=hp, alloc=False)\n",
        "    for index, component in enumerate(result.components):\n",
        "      component.pixels = pixels_list[index]\n",
        "      component.variance = variances_list[index]\n",
        "      component.refresh_precision()\n",
        "    return result\n",
        "\n",
        "  def from_signal_grids(signal_grids:List[SignalGrid]):\n",
        "    result = CompositeSignalGrid(hp=None)\n",
        "    hp = CompositeSignalGridHP(hps=[sg.hp for sg in signal_grids])\n",
        "    result.hp = hp\n",
        "    result.components = signal_grids\n",
        "    return result\n",
        "\n",
        "  def __init__(self, hp:CompositeSignalGridHP=None, alloc=True):\n",
        "    self.hp = hp\n",
        "    if hp:\n",
        "      self.components = [SignalGrid(component_hp, alloc_pixels=alloc) for component_hp in hp.components]\n",
        "\n",
        "  # add a SignalGrid as a component  \n",
        "  def add_component(self, o:SignalGrid):\n",
        "    assert self.hp.grid_shape == o.hp.grid_shape, f\"{self.hp.grid_shape} != {o.hp.grid_shape}\"\n",
        "\n",
        "    self.hp.components.append(o.hp)\n",
        "    self.components.append(o)\n",
        "\n",
        "  @property\n",
        "  def signal_shape(self):\n",
        "    return [c.hp.signal_shape for c in self.components]\n",
        "\n",
        "  @property\n",
        "  def pixels(self):\n",
        "    return [component.pixels for component in self.components]\n",
        "\n",
        "  @property\n",
        "  def grid_shape(self):\n",
        "    return self.hp.grid_shape\n",
        "\n",
        "  @property\n",
        "  def component_count(self):\n",
        "    return len(self.components)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    pixels_list = [component.pixels[index] for component in self.components]\n",
        "    variances_list = [component.variance[index] for component in self.components]\n",
        "\n",
        "    return CompositeSignalGrid.from_pixels_and_variances_list(pixels_list, variances_list, self.signal_shape)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return pretty_s(\"\", self)\n",
        "\n",
        "class PatternGridHP:\n",
        "  def __init__(self, grid_shape, pattern_composite_signal_shape:Tuple):\n",
        "    self.grid_shape = grid_shape\n",
        "    self.grid_size = np.prod(self.grid_shape)\n",
        "\n",
        "    self.composite_signal_grid_hp =  CompositeSignalGridHP(hps=[SignalGridHP(grid_shape=grid_shape, signal_shape=signal_shape) for signal_shape in pattern_composite_signal_shape])\n",
        "\n",
        "class PatternGrid:\n",
        "  def __init__(self, hp:PatternGridHP):\n",
        "    self.hp = hp\n",
        "    self.composite_signal_grid_begin = CompositeSignalGrid(hp.composite_signal_grid_hp) # Trajectory begin\n",
        "    self.composite_signal_grid_end = CompositeSignalGrid(hp.composite_signal_grid_hp)   # Trajectory end\n",
        "    self.alpha = torch.ones((hp.grid_size,)).to(device)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return pretty_s(\"\", self)\n",
        "\n",
        "  @property\n",
        "  def pixels(self):\n",
        "    return [\n",
        "      [component.pixels for component in self.composite_signal_grid_begin.components],\n",
        "      [component.pixels for component in self.composite_signal_grid_end.components]\n",
        "    ]\n",
        "\n",
        "class PatternSimilarityHP:\n",
        "  def __init__(self, enable_precision_weighted_distance=True):\n",
        "    self.enable_precision_weighted_distance = enable_precision_weighted_distance\n",
        "\n",
        "class PatternSimilarity:\n",
        "  def __init__(self, signal:CompositeSignalGrid, patterns:CompositeSignalGrid, hp:PatternSimilarityHP=None):\n",
        "    \"\"\"\n",
        "    signal must be        grid_shape=(grid_shape)                               signal_shape=(composite_signal_shapes)\n",
        "    patterns must be      grid_shape=(grid_shape, per_item_pattern_grid_shape)  signal_shape=(composite_signal_shapes)\n",
        "\n",
        "    Each signal is compared with corresponding per_x_pattern_grid_shape patterns.\n",
        "    Each comparison is done across composite signal components\n",
        "    \"\"\"\n",
        "    if hp:\n",
        "      self.hp = hp\n",
        "    else:\n",
        "      self.hp = PatternSimilarityHP()\n",
        "\n",
        "    assert signal.component_count == patterns.component_count, f\"signal.component_count {signal.component_count} != patterns.component_count {patterns.component_count}\"\n",
        "    assert signal.signal_shape == patterns.signal_shape, f\"signal.signal_shape {signal.signal_shape} != patterns.signal_shape {patterns.signal_shape}\"\n",
        "    assert patterns.grid_shape[0:len(signal.grid_shape)] == signal.grid_shape, f\"patterns.grid_shape {patterns.grid_shape} must match 0-n dimensions with signal.grid_shape {signal.grid_shape}\"\n",
        "    per_item_pattern_grid_shape = patterns.grid_shape[len(signal.grid_shape):]\n",
        "    per_item_pattern_grid_size = np.prod(per_item_pattern_grid_shape)\n",
        "\n",
        "    self.dist_1 = []\n",
        "    self.dist_d = []\n",
        "    self.dist = []\n",
        "    self.sim_components = []\n",
        "\n",
        "    # find similarity based on each signal component\n",
        "    for component_index in range(signal.component_count):\n",
        "      x_component = signal.components[component_index]\n",
        "      y_component = patterns.components[component_index]\n",
        "\n",
        "      xs = x_component.pixels.shape\n",
        "      x_component_pixels_expanded = x_component.pixels \\\n",
        "                                    .unsqueeze(dim=1) \\\n",
        "                                    .expand((xs[0], per_item_pattern_grid_size, xs[1])) \\\n",
        "                                    .reshape(-1, xs[1]) # expensive to reshape. How to vectorize better?\n",
        "      assert x_component_pixels_expanded.shape == y_component.pixels.shape\n",
        "      x_component_precision_expanded = x_component.precision \\\n",
        "                                    .unsqueeze(dim=1) \\\n",
        "                                    .expand((xs[0], per_item_pattern_grid_size, xs[1])) \\\n",
        "                                    .reshape(-1, xs[1]) # expensive to reshape. How to vectorize better?\n",
        "      assert x_component_precision_expanded.shape == y_component.precision.shape\n",
        "\n",
        "      dist_1_component, dist_d_component, dist_component = self.l2_distance(\n",
        "          x=x_component_pixels_expanded,\n",
        "          x_precision=x_component_precision_expanded,\n",
        "          y=y_component.pixels,\n",
        "          y_precision=y_component.precision)\n",
        "      \n",
        "      sim_component = torch.exp(-dist_component)\n",
        "\n",
        "      self.dist_1.append(dist_1_component)\n",
        "      self.dist_d.append(dist_d_component)\n",
        "      self.dist.append(dist_component)\n",
        "      self.sim_components.append(sim_component)\n",
        "\n",
        "    # final similarity is mean of signal component similarities\n",
        "    # this equalizes class weights for all components (e.g. modalities)\n",
        "    self.sim = torch.stack(self.sim_components).mean(dim=0)\n",
        "\n",
        "    # HACKHACK?! contrast enhancement\n",
        "    # self.sim = self.sim - self.sim.min(dim=0).values + 0.01\n",
        "\n",
        "  def l2_cross_distance(self, x, x_precision, y, y_precision):\n",
        "    xs = x.shape\n",
        "    assert len(xs) == 2\n",
        "    assert (x_precision is None) or (x_precision.shape == xs), \"Precision, if specified, must be same shape as patterns\"\n",
        "\n",
        "    ys = y.shape\n",
        "    assert len(ys) == 2\n",
        "    assert (y_precision is None) or (y_precision.shape == ys), \"Precision, if specified, must be same shape as patterns\"\n",
        "\n",
        "    assert xs[1] == ys[1], \"Patch size, i.e. dim 1, must match\"\n",
        "\n",
        "    n = xs[0]\n",
        "    m = ys[0]\n",
        "    d = xs[1]\n",
        "\n",
        "    x = x.unsqueeze(1).expand(n, m, d)\n",
        "    x_precision = x_precision.unsqueeze(1).expand(n, m, d)\n",
        "\n",
        "    y = y.unsqueeze(0).expand(n, m, d)\n",
        "    y_precision = y_precision.unsqueeze(0).expand(n, m, d)\n",
        "\n",
        "    return self.l2_distance(x, x_precision, y, y_precision)\n",
        "\n",
        "  def l2_distance(self, x, x_precision, y, y_precision):\n",
        "    dist_1 = (x - y).abs()\n",
        "    dist_d = torch.pow(dist_1, 2)\n",
        "\n",
        "    if self.hp.enable_precision_weighted_distance:\n",
        "      if x_precision is not None:\n",
        "        dist_d = dist_d * x_precision\n",
        "\n",
        "      if y_precision is not None:\n",
        "        dist_d = dist_d * y_precision\n",
        "\n",
        "    dist = dist_d.sum(-1).sqrt()\n",
        "    return dist_1, dist_d, dist\n",
        "\n",
        "  def __repr__(self):\n",
        "    s = []\n",
        "    s.append(f\"self.dist_1: {self.dist_1}\")\n",
        "    return \"\".join(s)\n"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting pattern.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ7AetD9sXXx"
      },
      "source": [
        "# !pip install torch\n",
        "%aimport pattern"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFBqB6E_8Xrm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "903f1aba-64cc-4a65-97b5-44190278a346"
      },
      "source": [
        "%%writefile convoluation_utils.py\n",
        "import math\n",
        "from pattern import *\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "import pdb\n",
        "\n",
        "class ConvolutionUtils:\n",
        "  @staticmethod\n",
        "  def make_afferent_patches(signal:CompositeSignalGrid, grid_shape:Tuple, coverage_factor:float=1.0):\n",
        "    # print(\"make_afferent_patches\")\n",
        "    # print(\"  signal\", signal.signal_shape)\n",
        "    # print(\"  grid_shape\", grid_shape)\n",
        "    # print(\"  coverage_factor\", coverage_factor)\n",
        "\n",
        "    assert signal.grid_shape == (1,1)\n",
        "\n",
        "    if grid_shape == (1,1):\n",
        "      # no convoluation\n",
        "      return signal\n",
        "\n",
        "    for grid_shape_i in grid_shape:\n",
        "      assert grid_shape_i > 1\n",
        "\n",
        "    patch_signal_grids = []\n",
        "\n",
        "    for component_index in range(len(signal.components)):\n",
        "      # print(\"  component\", component_index)\n",
        "      component = signal.components[component_index]\n",
        "\n",
        "      patch_shape = tuple([int(coverage_factor *  component.signal_shape[i] / grid_shape[i]) for i in range(len(grid_shape))])\n",
        "      # print(\"    patch_shape\", patch_shape)\n",
        "      stride = tuple([math.floor((component.signal_shape[i]-patch_shape[i])/(grid_shape[i]-1)) for i in range(len(grid_shape))])\n",
        "      # print(\"    stride\", stride)\n",
        "\n",
        "      patches = ConvolutionUtils.conv_slice(component.pixels.view((1,) + component.hp.signal_shape), patch_shape, stride=stride).squeeze(dim=0)\n",
        "      sghp = SignalGridHP(grid_shape=grid_shape, signal_shape=patch_shape)\n",
        "      patch_signal_grid = SignalGrid(hp=sghp, alloc_pixels=False, pixels = patches)\n",
        "               \n",
        "      # print(\"  patch_signal_grid\", patch_signal_grid)\n",
        "      patch_signal_grids.append(patch_signal_grid)\n",
        "\n",
        "    patches = CompositeSignalGrid.from_signal_grids(patch_signal_grids)\n",
        "    return patches\n",
        "\n",
        "  @staticmethod\n",
        "  def conv_slice(images, kernel_shape, stride, padding=0):\n",
        "    assert len(images.shape) == 3, \"Must be (image count, image height, image width)\"\n",
        "    images = images.unsqueeze(1)\n",
        "\n",
        "    fold_params = dict(kernel_size=kernel_shape, dilation=1, padding=padding, stride=stride)\n",
        "    unfold = torch.nn.Unfold(**fold_params)\n",
        "    # print(\"images\", images.shape)\n",
        "    unfolded = unfold(images)\n",
        "    unfolded = unfolded.view(images.shape[0], -1, unfolded.shape[-1])\n",
        "    unfolded = unfolded.transpose(1, 2)\n",
        "    return unfolded"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting convoluation_utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1ptRk9z86rb"
      },
      "source": [
        "%aimport convoluation_utils"
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnMru_ABTjKx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e23a619b-c9f6-43f4-9268-72a04646c4c7"
      },
      "source": [
        "%%run_pytest[clean]\n",
        "\"\"\"\n",
        "Test SignalGrid\n",
        "\"\"\"\n",
        "\n",
        "import pytest\n",
        "import pdb\n",
        "from pattern import *\n",
        "\n",
        "def test_signal_grid_invalid_grid_size1():\n",
        "  with pytest.raises(ValueError):\n",
        "    SignalGridHP(\n",
        "      grid_shape=(0,4),     # <-- zero\n",
        "      signal_shape=(5,6,2)\n",
        "    )\n",
        "\n",
        "def test_signal_grid_invalid_grid_size2():\n",
        "  with pytest.raises(ValueError):\n",
        "    SignalGridHP(\n",
        "      grid_shape=(1,4),\n",
        "      signal_shape=(5,-1,2) # <-- negative\n",
        "    )\n",
        "\n",
        "@pytest.fixture\n",
        "def signal_grid_hp1():\n",
        "  return SignalGridHP(\n",
        "      grid_shape=(3,4),\n",
        "      signal_shape=(5,6,2))\n",
        "\n",
        "@pytest.fixture\n",
        "def signal_grid_hp2():\n",
        "  return SignalGridHP(\n",
        "      grid_shape=(3,4),\n",
        "      signal_shape=(2,2))\n",
        "\n",
        "@pytest.fixture\n",
        "def signal_grid1(signal_grid_hp1):\n",
        "  return SignalGrid(hp=signal_grid_hp1)\n",
        "\n",
        "def test_create_signal_grid(signal_grid_hp1):\n",
        "  signal_grid = SignalGrid(hp=signal_grid_hp1)\n",
        "  assert signal_grid.pixels.shape == (3*4, 5*6*2)\n",
        "  assert signal_grid.variance.shape == signal_grid.pixels.shape\n",
        "  assert signal_grid.precision.shape == signal_grid.pixels.shape\n",
        "\n",
        "@pytest.fixture\n",
        "def signal_grid_hp_degenerate():\n",
        "  return SignalGridHP(\n",
        "      grid_shape=(1,1),\n",
        "      signal_shape=(1,1))\n",
        "\n",
        "def test_create_signal_grid_degenerate(signal_grid_hp_degenerate):\n",
        "  signal_grid = SignalGrid(hp=signal_grid_hp_degenerate)\n",
        "  assert signal_grid.pixels.shape == (1, 1)\n",
        "  assert signal_grid.variance.shape == signal_grid.pixels.shape\n",
        "  assert signal_grid.precision.shape == signal_grid.pixels.shape\n",
        "\n",
        "def test_composite_signal_grid_from_pixels_list():\n",
        "  csg = CompositeSignalGrid.from_pixels_list([torch.ones((10,10)), torch.ones((5,5))])\n",
        "  assert csg.grid_shape == (1,1)\n",
        "  assert len(csg.components) == 2\n",
        "\n",
        "  c0 = csg.components[0]\n",
        "  assert c0.hp.grid_shape == (1,1)\n",
        "  assert c0.signal_shape == (10,10)\n",
        "\n",
        "  c1 = csg.components[1]\n",
        "  assert c1.hp.grid_shape == (1,1)\n",
        "  assert c1.signal_shape == (5,5)\n",
        "\n",
        "def test_composite_signal_grid_from_signal_grids_error1():\n",
        "  grid_shape0 = (3,4)\n",
        "  signal_shape0 = (5,3,2)\n",
        "  grid_shape1 = (1,2)\n",
        "  signal_shape1 = (12,)\n",
        "\n",
        "  sgs = [\n",
        "         SignalGrid(hp=SignalGridHP(grid_shape=grid_shape0, signal_shape=signal_shape0)), \n",
        "         SignalGrid(hp=SignalGridHP(grid_shape=grid_shape1, signal_shape=signal_shape1))\n",
        "  ]\n",
        "  with pytest.raises(GridShapeMismatchError):\n",
        "    CompositeSignalGrid.from_signal_grids(sgs)\n",
        "\n",
        "def test_composite_signal_grid_from_signal_grids():\n",
        "  grid_shape = (3,4)\n",
        "  signal_shape0 = (5,3,2)\n",
        "  signal_shape1 = (12,)\n",
        "\n",
        "  sgs = [\n",
        "         SignalGrid(hp=SignalGridHP(grid_shape=grid_shape, signal_shape=signal_shape0)), \n",
        "         SignalGrid(hp=SignalGridHP(grid_shape=grid_shape, signal_shape=signal_shape1))\n",
        "  ]\n",
        "  csg = CompositeSignalGrid.from_signal_grids(sgs)\n",
        "  \n",
        "  assert csg.hp.grid_shape == grid_shape\n",
        "  assert len(csg.components) == 2\n",
        "\n",
        "  c0 = csg.components[0]\n",
        "  assert c0.hp.grid_shape == grid_shape\n",
        "  assert c0.signal_shape == signal_shape0\n",
        "\n",
        "  c1 = csg.components[1]\n",
        "  assert c1.hp.grid_shape == grid_shape\n",
        "  assert c1.signal_shape == signal_shape1\n",
        "\n",
        "def test_composite_signal_grid_from_pixels_and_variances_list():\n",
        "  pixels_list = [torch.ones((10,10)), torch.ones((5,5))]\n",
        "  variances_list = [torch.ones((10,10)), torch.ones((5,5))]\n",
        "\n",
        "  csg = CompositeSignalGrid.from_pixels_and_variances_list(pixels_list=pixels_list, variances_list=variances_list, signal_shape=[(10,10),(5,5)])\n",
        "  assert csg.grid_shape == (1,1)\n",
        "  assert len(csg.components) == 2\n",
        "\n",
        "  c0 = csg.components[0]\n",
        "  assert c0.hp.grid_shape == (1,1)\n",
        "  assert c0.signal_shape == (10,10)\n",
        "  assert c0.pixels is pixels_list[0]\n",
        "  assert c0.variance is variances_list[0]\n",
        "\n",
        "  c1 = csg.components[1]\n",
        "  assert c1.hp.grid_shape == (1,1)\n",
        "  assert c1.signal_shape == (5,5)\n",
        "  assert c1.pixels is pixels_list[1]\n",
        "  assert c1.variance is variances_list[1]\n",
        "\n",
        "def test_composite_signal_grid_indexing(signal_grid_hp1, signal_grid_hp2):\n",
        "  csg_hp = CompositeSignalGridHP(hps=[signal_grid_hp1, signal_grid_hp2])\n",
        "  csg = CompositeSignalGrid(hp=csg_hp)\n",
        "\n",
        "  for index in range(5, 10):\n",
        "    item = csg[index]\n",
        "    assert item.hp.grid_shape == (1,1)\n",
        "    assert len(item.hp.components) == len(csg.hp.components)\n",
        "    assert item.signal_shape == csg.signal_shape\n",
        "\n"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".........                                                                [100%]\n",
            "9 passed in 0.05s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBqW9IrAqxHv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07bc121e-0d2d-48a9-c10f-e7cc110338e8"
      },
      "source": [
        "%%run_pytest[clean]\n",
        "\"\"\"\n",
        "Test CompositeSignalGrid\n",
        "\"\"\"\n",
        "\n",
        "import pytest\n",
        "from pattern import *\n",
        "\n",
        "def test_csg_zero_components():\n",
        "  \"\"\"\n",
        "  Must have at least 1 component\n",
        "  \"\"\"\n",
        "  with pytest.raises(NoComponentsError):\n",
        "    CompositeSignalGridHP([])\n",
        "\n",
        "def test_csg_different_grid_shapes():\n",
        "  \"\"\"\n",
        "  Components may not have different grid shapes\n",
        "  \"\"\"\n",
        "  with pytest.raises(GridShapeMismatchError):\n",
        "    CompositeSignalGridHP([\n",
        "      SignalGridHP(grid_shape=(1,2), signal_shape=(3,4)),\n",
        "      SignalGridHP(grid_shape=(2,2), signal_shape=(3,4))\n",
        "    ])\n",
        "\n",
        "@pytest.fixture\n",
        "def composite_signal_grid1():\n",
        "  return CompositeSignalGrid(CompositeSignalGridHP([\n",
        "    SignalGridHP(grid_shape=(1,2), signal_shape=(3,4)),\n",
        "    SignalGridHP(grid_shape=(1,2), signal_shape=(3,2,1))\n",
        "  ]))\n",
        "\n",
        "def test_csg_different_signal_shapes(composite_signal_grid1):\n",
        "  \"\"\"\n",
        "  Components may have different signal shapes\n",
        "  \"\"\"\n",
        "  assert composite_signal_grid1.component_count == 2\n",
        "  assert composite_signal_grid1.components[0].signal_shape == (3,4)\n",
        "  assert composite_signal_grid1.components[1].signal_shape == (3,2,1)\n",
        "  assert composite_signal_grid1.signal_shape == [(3,4), (3,2,1)]"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "...                                                                      [100%]\n",
            "3 passed in 0.03s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4lPXN94vHcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "214e82d7-6d21-4a38-97a9-09385ebdd530"
      },
      "source": [
        "%%run_pytest[clean]\n",
        "\"\"\"\n",
        "Test PatternGrid\n",
        "\"\"\"\n",
        "\n",
        "import pytest\n",
        "from pattern import *\n",
        "\n",
        "@pytest.fixture\n",
        "def pg1():\n",
        "  pg_hp = PatternGridHP(grid_shape=(1,2), pattern_composite_signal_shape=[(3,4),(3,2,1)])\n",
        "\n",
        "  return PatternGrid(hp=pg_hp)\n",
        "\n",
        "def test_pg_create(pg1):\n",
        "  pg1.alpha.shape == (1,2)\n",
        "\n",
        "@pytest.fixture\n",
        "def pg2():\n",
        "  pg_hp = PatternGridHP(grid_shape=(1,), pattern_composite_signal_shape=[(1,),(1,)])\n",
        "\n",
        "  return PatternGrid(hp=pg_hp)\n",
        "\n",
        "def test_pg_create_2(pg2):\n",
        "  pg2.alpha.shape == (1,)\n"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "..                                                                       [100%]\n",
            "2 passed in 0.02s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcQ4WOe3WpLi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16182d58-ea32-4e95-c70d-ec6379e68970"
      },
      "source": [
        "%%run_pytest[clean] -s\n",
        "\"\"\"\n",
        "Test Similarity\n",
        "\"\"\"\n",
        "\n",
        "import pytest\n",
        "from pattern import *\n",
        "from utils import *\n",
        "\n",
        "def make_signal():\n",
        "  pg_hp = PatternGridHP(grid_shape=(2,4), pattern_composite_signal_shape=[(3,4),(3,2,1)])\n",
        "  return PatternGrid(hp=pg_hp)\n",
        "\n",
        "def make_patterns():\n",
        "  pg_hp = PatternGridHP(grid_shape=(2,4,3,2), pattern_composite_signal_shape=[(3,4),(3,2,1)])\n",
        "  return PatternGrid(hp=pg_hp)\n",
        "\n",
        "def test_sim_low_precision():\n",
        "  # By default, very low precision, so equal even when pixels are random\n",
        "  pgs = [make_signal(), make_patterns()]\n",
        "  sim = PatternSimilarity(pgs[0].composite_signal_grid_begin, pgs[1].composite_signal_grid_begin)\n",
        "  assert sim.sim.shape == pgs[1].hp.grid_size\n",
        "  assert torch.allclose(sim.sim, torch.ones_like(sim.sim))\n",
        "\n",
        "def test_sim_disable_precision_weighting():\n",
        "  # If disabled precision weighting, then unequal because pixels are random\n",
        "  pgs = [make_signal(), make_patterns()]\n",
        "  hp = PatternSimilarityHP(enable_precision_weighted_distance=False)\n",
        "  sim = PatternSimilarity(pgs[0].composite_signal_grid_begin, pgs[1].composite_signal_grid_begin, hp=hp)\n",
        "  assert sim.sim.shape == pgs[1].hp.grid_size\n",
        "  assert not torch.allclose(sim.sim, torch.ones_like(sim.sim))\n",
        "\n",
        "def test_sim_high_precision():\n",
        "  # If high precision, then unequal given pixels are random\n",
        "  pgs = [make_signal(), make_patterns()]\n",
        "\n",
        "  # force high precisions\n",
        "  for pg in pgs:\n",
        "    for component in pg.composite_signal_grid_begin.components:\n",
        "      component.precision = torch.ones_like(component.precision)\n",
        "\n",
        "  sim = PatternSimilarity(pgs[0].composite_signal_grid_begin, pgs[1].composite_signal_grid_begin)\n",
        "  assert sim.sim.shape == pgs[1].hp.grid_size\n",
        "  assert not torch.allclose(sim.sim, torch.ones_like(sim.sim))"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "...\n",
            "3 passed in 0.02s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvisc3G4cvFB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "757eaa82-aad1-45b7-d655-01fd8858d81e"
      },
      "source": [
        "%%writefile layer.py\n",
        "\n",
        "import torch\n",
        "import numpy\n",
        "import pdb\n",
        "from typing import List, Tuple\n",
        "from pattern import *\n",
        "import math\n",
        "from utils import *\n",
        "from convoluation_utils import ConvolutionUtils\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class CompositeSignalPatchGrid:\n",
        "  def __init__(self, signal:CompositeSignalGrid, grid_shape:Tuple, coverage_factor=1.0):\n",
        "    # signal, i.e. composite image, must be a single composite signal, i.e. its grid shape must be (1,)\n",
        "    assert signal.hp.grid_shape == (1,1), f\"{signal.hp.grid_shape} != (1,1)\"\n",
        "    self.patches:CompositeSignalGrid = ConvolutionUtils.make_afferent_patches(signal=signal, grid_shape=grid_shape, coverage_factor=coverage_factor)\n",
        "\n",
        "class NeighborhoodPatchGrid:\n",
        "  def __init__(self, signal:SignalGrid, patch_grid_shape:Tuple, per_patch_grid_shape:Tuple, patch_neighborhood_shape:Tuple):\n",
        "    # signal must be a single signal, i.e. its grid shape must be (1,)\n",
        "    assert signal.hp.grid_shape == (1,1), f\"{signal.hp.grid_shape} != (1,1)\"\n",
        "\n",
        "    # signal shape must be flattend combination of patch_grid_shape and per_patch_grid_shape\n",
        "    assert tuple(signal.hp.signal_shape) == tuple(np.multiply(patch_grid_shape, per_patch_grid_shape))\n",
        "\n",
        "    stride = per_patch_grid_shape\n",
        "    # print(\"  stride\", stride)\n",
        "\n",
        "    padding_patches = tuple([int((i-1)/2) for i in patch_neighborhood_shape])\n",
        "    padding = tuple(np.multiply(padding_patches, per_patch_grid_shape))\n",
        "    # print(\"  padding\", padding)\n",
        "\n",
        "    patch_shape = tuple(np.multiply(patch_neighborhood_shape, per_patch_grid_shape))\n",
        "    patches = ConvolutionUtils.conv_slice(signal.pixels.view(signal.signal_shape).unsqueeze(dim=0), patch_shape, stride=stride, padding=padding).squeeze(dim=0)\n",
        "    sghp = SignalGridHP(grid_shape=patch_grid_shape, signal_shape=patch_shape)\n",
        "    self.patches:SignalGrid = SignalGrid(hp=sghp, alloc_pixels=False, pixels = patches)\n",
        "\n",
        "class InputOutputPatchGrid:\n",
        "  def __init__(self, patch_grid_shape:Tuple, input:CompositeSignalGrid, output:SignalGrid, output_patch_neighborhood_shape:Tuple, per_patch_pattern_grid_shape:Tuple, input_coverage_factor=1.0):\n",
        "    input_grid_shape = output.signal_shape\n",
        "\n",
        "    self.input_patches = CompositeSignalPatchGrid(signal=input, grid_shape=patch_grid_shape, coverage_factor=input_coverage_factor)\n",
        "    self.output_patches = NeighborhoodPatchGrid(signal=output, patch_grid_shape=patch_grid_shape, per_patch_grid_shape=per_patch_pattern_grid_shape, patch_neighborhood_shape=output_patch_neighborhood_shape)\n",
        "    \n",
        "    self.patches = self.input_patches.patches\n",
        "    self.patches.add_component(self.output_patches.patches)\n",
        "\n",
        "class LayerHP:\n",
        "  def __init__(self, input_signal_shapes:List[Tuple], input_coverage_factor:float, patch_grid_shape:Tuple, per_patch_pattern_grid_shape:Tuple, output_patch_neighborhood_shape:Tuple, output_tau=0.5):\n",
        "    assert len(patch_grid_shape) == len(per_patch_pattern_grid_shape), \"This is so that output can be flattened\"\n",
        "    assert len(patch_grid_shape) == len(output_patch_neighborhood_shape), \"Output patch neighborhood is a subset of patch grid, so number of dimensions must match\"\n",
        "    # assert len(patch_grid_shape) == 2, \"Currently support for 2D\"\n",
        "\n",
        "    self.input_signal_shapes:List[Tuple] = input_signal_shapes\n",
        "    self.input_coverage_factor:float = input_coverage_factor\n",
        "    self.patch_grid_shape:Tuple = patch_grid_shape\n",
        "    self.per_patch_pattern_grid_shape:Tuple = per_patch_pattern_grid_shape\n",
        "    self.output_patch_neighborhood_shape = output_patch_neighborhood_shape\n",
        "    self.output_neighborhood_shape:Tuple = np.multiply(output_patch_neighborhood_shape, per_patch_pattern_grid_shape)\n",
        "    self.output_tau:float = output_tau\n",
        "\n",
        "    for size in self.output_patch_neighborhood_shape:\n",
        "      assert size % 2 == 1, \"Output patch neighborhood shape must be odd, so can be centered around specific output activation\"\n",
        "\n",
        "    # Get input patch shapes\n",
        "    sample_input_signal = [torch.ones(shape) for shape in input_signal_shapes]\n",
        "    sample_input = CompositeSignalGrid.from_pixels_list(sample_input_signal)\n",
        "    patches = ConvolutionUtils.make_afferent_patches(signal=sample_input, grid_shape=patch_grid_shape, coverage_factor=input_coverage_factor)\n",
        "\n",
        "    # Patterns -\n",
        "    # each patch in the patch grid will get per_patch_pattern_grid_shape patterns, so patterns will be of shape -\n",
        "    # (patch_grid_shape,) + (per_patch_pattern_grid_shape shape,) + (pattern_composite_signal_shape,)\n",
        "    pattern_composite_signal_shape = patches.signal_shape + [self.output_neighborhood_shape] # input signals + output signal\n",
        "    patch_grid_shape = patch_grid_shape\n",
        "    pattern_grid_shape = patch_grid_shape + per_patch_pattern_grid_shape # 2D, 2D = 4D\n",
        "    self.pattern_grid_hp:PatternGridHP = PatternGridHP(grid_shape=pattern_grid_shape, pattern_composite_signal_shape=pattern_composite_signal_shape)\n",
        "\n",
        "    # Output is flattened 2D version of the 4D pattern_grid_shape\n",
        "    output_shape = np.multiply(patch_grid_shape, per_patch_pattern_grid_shape) # height1*height2, width1*width2\n",
        "    self.output_hp:SignalGridHP = SignalGridHP(grid_shape=(1,1), signal_shape=output_shape, init_pixel_scale=0.0)\n",
        "\n",
        "    # print(\"Output shape\", output_shape)\n",
        "\n",
        "    # # input grid HP\n",
        "    # hps = [\n",
        "    #   SignalGridHP(\n",
        "    #       grid_shape=output_shape, # Each grid element in input grid produces 1 pixel of output\n",
        "    #       signal_shape=input_component.signal_shape)\n",
        "    #   for input_component in patches.components # each component of input\n",
        "    # ]\n",
        "    # self.input_grid_hp = CompositeSignalGridHP(hps=hps.copy())\n",
        "\n",
        "    # # pattern HP\n",
        "    # hps.append(SignalGridHP(\n",
        "    #     grid_shape=output_shape,\n",
        "    #     signal_shape=output_neighborhood_shape))\n",
        "\n",
        "    # self.pattern_grid_hp = PatternGridHP(\n",
        "    #     grid_shape=pattern_grid_shape,\n",
        "    #     composite_signal_grid_hp=CompositeSignalGridHP(hps=[\n",
        "    #       SignalGridHP(grid_shape=pattern_grid_shape, signal_shape=component.signal_shape)\n",
        "    #      for component in hps]))\n",
        "\n",
        "class DirectFitLearning:\n",
        "  def __init__(self, input:CompositeSignalGrid, output:SignalGrid, patch_grid_shape:Tuple, per_patch_pattern_grid_shape:Tuple, patterns:PatternGrid):\n",
        "    # Output shape example (1, 2x3x4x5), where patch_grid_shape is (2x4) and per_patch_pattern_grid_shape is (3x5)\n",
        "    assert len(patch_grid_shape) == len(per_patch_pattern_grid_shape)\n",
        "    assert output.pixels.shape[0] == 1\n",
        "    assert output.pixels.shape[2] == np.dot(patch_grid_shape, per_patch_pattern_grid_shape)\n",
        "\n",
        "    activation_shape = torch.stack((torch.tensor(patch_grid_shape), torch.tensor(per_patch_pattern_grid_shape)), dim=1).view(-1) # e.g. (2, 3, 4, 5)\n",
        "    activation = output.pixels.view(activation_shape)\n",
        "\n",
        "\n",
        "  \n",
        "class Layer:\n",
        "  def __init__(self, hp:LayerHP):\n",
        "    self.hp = hp\n",
        "    self.patterns = PatternGrid(hp=self.hp.pattern_grid_hp)\n",
        "    self.output = SignalGrid(hp.output_hp)\n",
        "    self.debug = False\n",
        "\n",
        "  def forward(self, input:CompositeSignalGrid):\n",
        "    assert input.grid_shape == (1,1) # single signal input\n",
        "\n",
        "    input_output_patch_grid = InputOutputPatchGrid(patch_grid_shape=self.hp.patch_grid_shape,\n",
        "                                                   input=input,\n",
        "                                                   output=self.output,\n",
        "                                                   output_patch_neighborhood_shape=self.hp.output_patch_neighborhood_shape,\n",
        "                                                   per_patch_pattern_grid_shape=self.hp.per_patch_pattern_grid_shape,\n",
        "                                                   input_coverage_factor=1.0)\n",
        "    \n",
        "    # BOTTOM UP\n",
        "    # Compare with pattern_end \n",
        "    pattern_similarity_end = PatternSimilarity(signal=input_output_patch_grid.patches, patterns=self.patterns.composite_signal_grid_end)\n",
        "    if self.debug:\n",
        "      print(\"pattern_similarity_end\", pattern_similarity_end)\n",
        "    \n",
        "    activation_end = pattern_similarity_end.sim.unsqueeze(dim=0)\n",
        "    # print(\"output shape\", self.output.pixels.shape)\n",
        "    # print(\"self.hp.patch_grid_shape\", self.hp.patch_grid_shape)\n",
        "    # print(\"self.hp.per_patch_pattern_grid_shape\", self.hp.per_patch_pattern_grid_shape)\n",
        "    assert self.output.pixels.shape == activation_end.shape \n",
        "\n",
        "    self.output.pixels = soft_add(self.output.pixels, activation_end, tau=self.hp.output_tau)\n",
        "\n",
        "    # TOP DOWN\n",
        "    # Compare with pattern_begin and send down winning pattern's _end as prediction\n",
        "    pattern_similarity_begin = PatternSimilarity(signal=input_output_patch_grid.patches, patterns=self.patterns.composite_signal_grid_begin)\n",
        "    if self.debug:\n",
        "      print(\"pattern_similarity_begin\", pattern_similarity_begin)\n",
        "\n",
        "    assert len(pattern_similarity_begin.sim.shape) == 1\n",
        "    print(\"pattern_similarity_begin.sim\", pattern_similarity_begin.sim)\n",
        "    activation_begin = torch.softmax(pattern_similarity_begin.sim * 100, 0)\n",
        "    print(\"activation_begin\", activation_begin)\n",
        "    winner_index = torch.multinomial(activation_begin, 1)\n",
        "    print(\"winner_index\", winner_index)\n",
        "    self.top_down_prediction = self.patterns.composite_signal_grid_end[winner_index]\n",
        "\n",
        "\n"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting layer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozYdg-Q-q_aU"
      },
      "source": [
        "%aimport layer"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzMn4q-FrA9K"
      },
      "source": [
        "\n",
        "%%run_pytest[clean] -s\n",
        "\"\"\"\n",
        "Test Layer\n",
        "\"\"\"\n",
        "\n",
        "import pytest\n",
        "from utils import *\n",
        "from pattern import *\n",
        "from layer import *\n",
        "import pdb\n",
        "\n",
        "def test_layer_create():\n",
        "  hp = LayerHP(\n",
        "      input_signal_shapes=[(10,15),(20,10)],\n",
        "      input_coverage_factor=1.0, \n",
        "      patch_grid_shape=(5,5),\n",
        "      per_patch_pattern_grid_shape=(4,4),\n",
        "      output_patch_neighborhood_shape=(3,3),\n",
        "      output_tau=0.5)\n",
        "  layer = Layer(hp=hp)\n",
        "  # pretty_print(\"Layer\", layer)\n",
        "  assert(True)\n",
        "\n",
        "def test_layer_forward():\n",
        "  input_signal_shapes = [(10,10), (20,20)]\n",
        "  input_pixels_list = [torch.ones(shape) for shape in input_signal_shapes]\n",
        "  input = CompositeSignalGrid.from_pixels_list(input_pixels_list)\n",
        "\n",
        "  hp = LayerHP(\n",
        "      input_signal_shapes=input_signal_shapes,\n",
        "      input_coverage_factor=1.0, \n",
        "      patch_grid_shape=(5,5),\n",
        "      per_patch_pattern_grid_shape=(2,3),\n",
        "      output_patch_neighborhood_shape=(3,3),\n",
        "      output_tau=0.5)\n",
        "\n",
        "  layer = Layer(hp=hp)\n",
        "  #pretty_print(\"Layer\", layer)\n",
        "\n",
        "  for _ in range(100):\n",
        "    layer.forward(input)\n",
        "\n",
        "  assert torch.allclose(layer.output.pixels, torch.ones_like(layer.output.pixels), atol=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGs1sDJ4BaU9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "48a02fc2840544169c9087fd11c53c63",
            "bd3efcf4e7f3454883ed7ec8b6001834",
            "5d4a2dfc7d8749b99f0335532b99f258",
            "a457d294885048be86cbee8faa12a4bb",
            "dabad3eda46b44f8813d1c503aee866c",
            "b34e890c4f234267bab52eafba543dd4",
            "f6f0c2c8d81049caa0f8e184b95f8272",
            "159ef09e922a4a459921c0e1b35e4743"
          ]
        },
        "outputId": "1f91e13f-8fc3-43f0-876e-b107876e3780"
      },
      "source": [
        "# !pip install pandas\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.datasets import load_boston\n",
        "from utils import *\n",
        "from layer import *\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def normalize(df):\n",
        "  df1 = (df - df.mean())/df.std()\n",
        "  return df1\n",
        "\n",
        "def scale(df):\n",
        "  min = df.min()\n",
        "  max = df.max()\n",
        "\n",
        "  df1 = (df - min) / (max - min)\n",
        "  return df1\n",
        "\n",
        "dataset = load_boston()\n",
        "dataset = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "dataset = pd.DataFrame(np.c_[scale(normalize(dataset['LSTAT'])), scale(normalize(dataset['RM']))], columns = ['LSTAT','RM'])\n",
        "dataset = torch.tensor(dataset.to_numpy()).float().to(device)\n",
        "dataset\n",
        "X = dataset[:,0]\n",
        "Y = dataset[:,1]\n",
        "\n",
        "class SingleLayerRegression:\n",
        "  def __init__(self, X:torch.Tensor, Y:torch.Tensor, pattern_grid_shape:Tuple=(1,10)):\n",
        "    assert X.shape[0] == Y.shape[0], \"Batch size must be same for X and Y\"\n",
        "    assert len(X.shape) >= 1, \"X must be at least 1D batch of scalars\"\n",
        "    assert len(Y.shape) >= 1, \"Y must be at least 1D batch of scalars\"\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    self.pattern_grid_shape = pattern_grid_shape\n",
        "\n",
        "    self.batch_size = 30 #X.shape[0]\n",
        "\n",
        "    # X\n",
        "    self.X_signal_shape = X.shape[1:]\n",
        "    if len(self.X_signal_shape) == 0:\n",
        "      self.X_signal_shape = (1,1)\n",
        "    elif len(self.X_signal_shape) == 1:\n",
        "      self.X_signal_shape = [1] + self.X_signal_shape\n",
        "    self.X_signal_shape = tuple(self.X_signal_shape)\n",
        "\n",
        "    # Y\n",
        "    self.Y_signal_shape = Y.shape[1:]\n",
        "    if len(self.Y_signal_shape) == 0:\n",
        "      self.Y_signal_shape = (1,1)\n",
        "    elif len(self.Y_signal_shape) == 1:\n",
        "      self.Y_signal_shape = [1] + self.Y_signal_shape\n",
        "    self.Y_signal_shape = tuple(self.Y_signal_shape)\n",
        "\n",
        "    # Layer\n",
        "    self.layer_hp = self.create_layer_hp(per_patch_pattern_grid_shape=pattern_grid_shape)\n",
        "    self.layer = Layer(hp=self.layer_hp)\n",
        "\n",
        "\n",
        "  def create_layer_hp(self, input_coverage_factor=1.0, patch_grid_shape=(1,1), per_patch_pattern_grid_shape=(1, 10), output_patch_neighborhood_shape=(1,1), output_tau=1.0):\n",
        "    hp = LayerHP(\n",
        "          input_signal_shapes=[self.X_signal_shape, self.Y_signal_shape],\n",
        "          input_coverage_factor=input_coverage_factor, \n",
        "          patch_grid_shape=patch_grid_shape,\n",
        "          per_patch_pattern_grid_shape=per_patch_pattern_grid_shape,\n",
        "          output_patch_neighborhood_shape=output_patch_neighborhood_shape,\n",
        "          output_tau=output_tau) # set output_tau=1.0 for IID data\n",
        "    return hp\n",
        "\n",
        "  def epoch(self):\n",
        "    for i in tqdm(range(self.batch_size)):\n",
        "      input = CompositeSignalGrid.from_pixels_list([self.X[i].view(self.X_signal_shape), self.Y[i].view(self.Y_signal_shape)])\n",
        "      for component in input.components:\n",
        "        component.variance *= 0\n",
        "        component.refresh_precision()\n",
        "\n",
        "      print(\"composite input,output\", input.pixels)\n",
        "\n",
        "      self.layer.forward(input)\n",
        "\n",
        "      print(\"prediction\", self.layer.top_down_prediction.components[1].pixels)\n",
        "\n",
        "      if 1: #i == self.batch_size - 1:\n",
        "        # print(\"layer output\", self.layer.output.pixels.view(self.layer.hp.per_patch_pattern_grid_shape))\n",
        "        plt.imshow(self.layer.output.pixels.view(self.layer.hp.per_patch_pattern_grid_shape), vmin=0, vmax=1, cmap=plt.cm.viridis)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "      # print(\"patterns\", self.layer.patterns.pixels)\n",
        "\n",
        "pattern_machine = SingleLayerRegression(X, Y)\n",
        "pattern_machine.layer.debug = False\n",
        "\n",
        "# preset patterns with x and y as (0.0, 0.0) to (1.0, 1.0), i.e. y=x\n",
        "for csgb in [pattern_machine.layer.patterns.composite_signal_grid_begin, pattern_machine.layer.patterns.composite_signal_grid_end]:\n",
        "  csgb.components[0].pixels = torch.linspace(0, 1, 10).unsqueeze(1)\n",
        "  csgb.components[1].pixels = torch.linspace(0, 1, 10).unsqueeze(1)\n",
        "  csgb.components[0].variance = torch.zeros_like(csgb.components[0].variance)\n",
        "  csgb.components[1].variance = torch.zeros_like(csgb.components[0].variance)\n",
        "  csgb.components[0].refresh_precision()\n",
        "  csgb.components[1].refresh_precision()\n",
        "\n",
        "print(\"pattern_machine.layer.patterns.composite_signal_grid_begin\", pattern_machine.layer.patterns.composite_signal_grid_begin.pixels)\n",
        "pattern_machine.epoch()\n"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pattern_machine.layer.patterns.composite_signal_grid_begin [tensor([[0.0000],\n",
            "        [0.1111],\n",
            "        [0.2222],\n",
            "        [0.3333],\n",
            "        [0.4444],\n",
            "        [0.5556],\n",
            "        [0.6667],\n",
            "        [0.7778],\n",
            "        [0.8889],\n",
            "        [1.0000]]), tensor([[0.0000],\n",
            "        [0.1111],\n",
            "        [0.2222],\n",
            "        [0.3333],\n",
            "        [0.4444],\n",
            "        [0.5556],\n",
            "        [0.6667],\n",
            "        [0.7778],\n",
            "        [0.8889],\n",
            "        [1.0000]]), tensor([[0.0737, 0.0679, 0.0835, 0.0953, 0.0897, 0.0874, 0.0357, 0.0237, 0.0964,\n",
            "         0.0845],\n",
            "        [0.0149, 0.0404, 0.0056, 0.0102, 0.0120, 0.0565, 0.0895, 0.0648, 0.0069,\n",
            "         0.0569],\n",
            "        [0.0519, 0.0380, 0.0267, 0.0240, 0.0413, 0.0837, 0.0787, 0.0207, 0.0091,\n",
            "         0.0926],\n",
            "        [0.0694, 0.0250, 0.0964, 0.0440, 0.0080, 0.0044, 0.0260, 0.0284, 0.0561,\n",
            "         0.0108],\n",
            "        [0.0137, 0.0371, 0.0542, 0.0739, 0.0486, 0.0147, 0.0656, 0.0655, 0.0703,\n",
            "         0.0535],\n",
            "        [0.0977, 0.0439, 0.0080, 0.0574, 0.0144, 0.0428, 0.0755, 0.0121, 0.0523,\n",
            "         0.0627],\n",
            "        [0.0622, 0.0002, 0.0474, 0.0582, 0.0938, 0.0991, 0.0890, 0.0858, 0.0746,\n",
            "         0.0807],\n",
            "        [0.0291, 0.0137, 0.0429, 0.0048, 0.0970, 0.0420, 0.0764, 0.0019, 0.0863,\n",
            "         0.0699],\n",
            "        [0.0818, 0.0275, 0.0613, 0.0093, 0.0576, 0.0149, 0.0880, 0.0663, 0.0952,\n",
            "         0.0644],\n",
            "        [0.0515, 0.0251, 0.0431, 0.0347, 0.0039, 0.0952, 0.0427, 0.0543, 0.0155,\n",
            "         0.0573]])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48a02fc2840544169c9087fd11c53c63",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.0897]]), tensor([[0.5775]])]\n",
            "pattern_similarity_begin.sim tensor([0.8252, 0.8687, 0.8589, 0.8557, 0.8589, 0.8686, 0.8254, 0.7737, 0.7274,\n",
            "        0.6859])\n",
            "activation_begin tensor([4.2299e-03, 3.2817e-01, 1.2392e-01, 8.9601e-02, 1.2355e-01, 3.2617e-01,\n",
            "        4.3393e-03, 2.4548e-05, 2.3928e-07, 3.7961e-09])\n",
            "winner_index tensor([5])\n",
            "prediction tensor([[0.5556]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABF0lEQVR4nO3aIU6DQRRG0dcGCIIEgWU5JCwOVsUi0AiCAt0GEn4sBuQd0XPsmE/dPDG7bdsGgMZ+9QCAUyK6ACHRBQiJLkBIdAFCZ/89Pj7fL//acHvxsXrCzMwcv89XT+CXy/3X6gkzM/P6ebN6wrwc12+YmXk7XK+eMO+Hq9UTZmbm6e5h99ebSxcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgtNu2bfUGgJPh0gUIiS5ASHQBQqILEBJdgJDoAoR+AHDMFlkdrLweAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.2045]]), tensor([[0.5480]])]\n",
            "pattern_similarity_begin.sim tensor([0.7977, 0.8523, 0.9014, 0.8953, 0.8961, 0.8988, 0.8393, 0.7861, 0.7385,\n",
            "        0.6959])\n",
            "activation_begin tensor([1.0780e-05, 2.5276e-03, 3.4471e-01, 1.8626e-01, 2.0158e-01, 2.6422e-01,\n",
            "        6.9103e-04, 3.3768e-06, 2.8881e-08, 4.0750e-10])\n",
            "winner_index tensor([2])\n",
            "prediction tensor([[0.2222]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABI0lEQVR4nO3arU1EQRiF4XP5SRCIFTgMgkZogXIogo5oYhM8BkHAswgyVADyfCT7PHbMEfe+GTHbWisAdJxMDwA4JqILUCS6AEWiC1AkugBFZ38dPuzvx5823F68TU9Iktycf0xPyHe26Qn/xmnGP80kyfPX9fSEvByupickSV4/d9MT8n64nJ6QJHm6e/z1Z3XTBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUo2tZa0xsAjoabLkCR6AIUiS5AkegCFIkuQJHoAhT9AAxEFlm5ZQGkAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.0635]]), tensor([[0.6944]])]\n",
            "pattern_similarity_begin.sim tensor([0.8126, 0.8371, 0.8256, 0.8201, 0.8206, 0.8272, 0.8399, 0.8031, 0.7537,\n",
            "        0.7095])\n",
            "activation_begin tensor([2.4591e-02, 2.8637e-01, 9.0124e-02, 5.2112e-02, 5.4999e-02, 1.0600e-01,\n",
            "        3.7619e-01, 9.5510e-03, 6.8278e-05, 8.2056e-07])\n",
            "winner_index tensor([5])\n",
            "prediction tensor([[0.5556]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABI0lEQVR4nO3asU1DQRRE0fm2SZAQiIZogLJogG5cCBWQIBrAsix5qQDCeYHPSTeZYHW1wW5rrQDQsZseAHBLRBegSHQBikQXoEh0AYoO/x2+fbyOf214PPxMT0iSnK930xNyWfvpCUmS3XadnpB9xq9mkuR+d56ekK/L0/SEJMnn6Xl6Qr5PD9MTkiTHl/ftrzMvXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2Aom2tNb0B4GZ46QIUiS5AkegCFIkuQJHoAhSJLkDRL6OjGFl1vhXvAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.0334]]), tensor([[0.6586]])]\n",
            "pattern_similarity_begin.sim tensor([0.8282, 0.8345, 0.8247, 0.8210, 0.8234, 0.8318, 0.8409, 0.7875, 0.7397,\n",
            "        0.6970])\n",
            "activation_begin tensor([1.0334e-01, 1.9391e-01, 7.2973e-02, 5.0402e-02, 6.3615e-02, 1.4712e-01,\n",
            "        3.6686e-01, 1.7630e-03, 1.4855e-05, 2.0681e-07])\n",
            "winner_index tensor([6])\n",
            "prediction tensor([[0.6667]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABGUlEQVR4nO3asU0DMBiE0TMBiYKCin0oWC8bMQdVSioGIFIUmQmgvB8p77VurvEnF1577wDQcTc9AOCWiC5AkegCFIkuQJHoAhTd/3V4/Hgb/9pwyPiEJMllH6Yn5Jo1PeHfeFjX6QlJksd1mZ6Q0/llekKS5PP7eXpCvs5P0xOSJO+vx18vq5cuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkDR2ntPbwC4GV66AEWiC1AkugBFogtQJLoARaILUPQD708WWYpReIsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.0993]]), tensor([[0.6871]])]\n",
            "pattern_similarity_begin.sim tensor([0.8028, 0.8501, 0.8375, 0.8311, 0.8309, 0.8368, 0.8489, 0.8069, 0.7571,\n",
            "        0.7125])\n",
            "activation_begin tensor([3.2025e-03, 3.6366e-01, 1.0293e-01, 5.4319e-02, 5.3038e-02, 9.5771e-02,\n",
            "        3.2222e-01, 4.8180e-03, 3.3116e-05, 3.8425e-07])\n",
            "winner_index tensor([5])\n",
            "prediction tensor([[0.5556]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABHklEQVR4nO3asU1DQRRE0fmAEQkJBVAKNdAQTVCMG6EDywkBMRYWWiqAcB6Sz0k3mejqBbuttQJAx9X0AIBLIroARaILUCS6AEWiC1B089fjy9vz+NeGx9uP6QlJktPaTU/IeV1PT/g3dtv39IQkyd12np6Q49fD9IQkyeFzfsf76X56QpJk//S6/fbm0gUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKNrWWtMbAC6GSxegSHQBikQXoEh0AYpEF6BIdAGKfgAcRBdZ/1KrIQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.0960]]), tensor([[0.5497]])]\n",
            "pattern_similarity_begin.sim tensor([0.8285, 0.8766, 0.8674, 0.8647, 0.8686, 0.8752, 0.8182, 0.7672, 0.7216,\n",
            "        0.6808])\n",
            "activation_begin tensor([2.6815e-03, 3.3025e-01, 1.3077e-01, 1.0017e-01, 1.4800e-01, 2.8716e-01,\n",
            "        9.6201e-04, 5.8669e-06, 6.1180e-08, 1.0308e-09])\n",
            "winner_index tensor([1])\n",
            "prediction tensor([[0.1111]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABJElEQVR4nO3aIU4EQQBE0eqFrMAgUDgus5dDcCruQJAYFEGSCQnbnABk9Sbznm1TZn63mDHnDAAdh9UDAPZEdAGKRBegSHQBikQXoOj6v8Onl9PyXxsejh+rJyRJvufV6gk5z8u4Iw/jvHpCjuNn9YQkyet2v3pC3ra71ROSJO9ft6sn5HO7WT0hSfJ8ehx/nV3GVwywE6ILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEVjzrl6A8BueOkCFIkuQJHoAhSJLkCR6AIUiS5A0S9DphlZJvhfZgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.2953]]), tensor([[0.4696]])]\n",
            "pattern_similarity_begin.sim tensor([0.7898, 0.8435, 0.9034, 0.9450, 0.9455, 0.8961, 0.8369, 0.7840, 0.7366,\n",
            "        0.6942])\n",
            "activation_begin tensor([8.7830e-08, 1.8774e-05, 7.5408e-03, 4.8356e-01, 5.0524e-01, 3.6326e-03,\n",
            "        9.7676e-06, 4.8931e-08, 4.2789e-10, 6.1586e-12])\n",
            "winner_index tensor([4])\n",
            "prediction tensor([[0.4444]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABKklEQVR4nO3aIU4DQQCF4beUBARJMRhQWI7RO3AUjsGZOAMOVVdVS1JISIYTgHzTZL/Pjnlm/xmxyxgjAHRczB4AsCaiC1AkugBFogtQJLoARZf/Hb68P0//teHx6jh7QpLk6fowe0IeNp+zJyQ5j5t6/7OdPSFJ8vF9P3tC9qe72ROSJIev29kTcjzdzJ6QJHnbvS5/nZ3D9wOwGqILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEXLGGP2BoDV8NIFKBJdgCLRBSgSXYAi0QUoEl2Aol8r1xdZMEmqTgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.4807]]), tensor([[0.5003]])]\n",
            "pattern_similarity_begin.sim tensor([0.7415, 0.7895, 0.8431, 0.9030, 0.9700, 0.9580, 0.8923, 0.8335, 0.7809,\n",
            "        0.7338])\n",
            "activation_begin tensor([9.1890e-11, 1.1137e-08, 2.3725e-06, 9.4894e-04, 7.6756e-01, 2.3116e-01,\n",
            "        3.2429e-04, 9.0754e-07, 4.7136e-09, 4.2563e-11])\n",
            "winner_index tensor([4])\n",
            "prediction tensor([[0.4444]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABK0lEQVR4nO3aITIGUBiF4fMbhlGMoJgRVDM2YCVWYE0WYR8WIAkiRdD4Ca4VEM8XPE+95aR3vnA3a60A0LEzPQDgPxFdgCLRBSgSXYAi0QUo2v3r8fr+Zvxrw9nh2/SEJMn5/uv0hFwcPE9PSJJc7r1PT8h2fU9PSJI8fB1PT8jj5+n0hCTJ0/ZkekJePo6mJyRJ7q5uN7+9uXQBikQXoEh0AYpEF6BIdAGKRBegSHQBikQXoEh0AYpEF6BIdAGKRBegSHQBikQXoEh0AYpEF6BIdAGKRBegSHQBikQXoEh0AYpEF6BIdAGKRBegSHQBikQXoEh0AYpEF6BIdAGKRBegSHQBikQXoEh0AYo2a63pDQD/hksXoEh0AYpEF6BIdAGKRBegSHQBin4A5O0ZWaRUZ8wAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.7781]]), tensor([[0.3966]])]\n",
            "pattern_similarity_begin.sim tensor([0.7106, 0.7549, 0.8045, 0.8598, 0.8898, 0.8845, 0.8859, 0.8942, 0.8354,\n",
            "        0.7826])\n",
            "activation_begin tensor([4.2495e-09, 3.5795e-07, 5.0780e-05, 1.2894e-02, 2.5870e-01, 1.5138e-01,\n",
            "        1.7508e-01, 4.0076e-01, 1.1240e-03, 5.7205e-06])\n",
            "winner_index tensor([4])\n",
            "prediction tensor([[0.4444]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABIUlEQVR4nO3arU2EQRiF0fvxk7B6FQSNpwu62VKogzIoA4fEkRAEiM1QAcj7kuw5dswVkycjZltrBYCOs+kBAKdEdAGKRBegSHQBikQXoOjir8OH58P414br3cf0hCTJ7e59ekLurt6mJyRJ9uef0xNyzDY94d+4zHF6QpLk5ftmekJev/bTE5Ikj/dPv15QL12AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgKJtrTW9AeBkeOkCFIkuQJHoAhSJLkCR6AIUiS5A0Q+HNBRZrgcUhAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.4241]]), tensor([[0.4681]])]\n",
            "pattern_similarity_begin.sim tensor([0.7601, 0.8103, 0.8664, 0.9290, 0.9855, 0.9310, 0.8681, 0.8119, 0.7616,\n",
            "        0.7165])\n",
            "activation_begin tensor([1.6259e-10, 2.4521e-08, 6.6699e-06, 3.5056e-03, 9.9220e-01, 4.2752e-03,\n",
            "        7.9668e-06, 2.8746e-08, 1.8741e-10, 2.0741e-12])\n",
            "winner_index tensor([4])\n",
            "prediction tensor([[0.4444]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABLElEQVR4nO3arU1EURSF0f34yQSBwoDCkJDQA72QUBKt0AUaQzAoMgLNBEgeFYDcR8xa9pqtvhxxl3VdA0DHwfQAgH0iugBFogtQJLoARaILUHT03+P9093414bLk4/pCUmSq812ekJuNu/TE5Ik18eH0xPy+v0zPSFJ8vx1Pj0hL7uL6QlJkrfPs+kJ2e5OpyckSR5vH5a/3ly6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFy7qu0xsA9oZLF6BIdAGKRBegSHQBikQXoEh0AYp+AVRWGFnN1ra9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.5166]]), tensor([[0.5396]])]\n",
            "pattern_similarity_begin.sim tensor([0.7265, 0.7727, 0.8243, 0.8820, 0.9465, 0.9819, 0.9137, 0.8527, 0.7981,\n",
            "        0.7492])\n",
            "activation_begin tensor([7.8266e-12, 7.9479e-10, 1.3896e-07, 4.4565e-05, 2.8163e-02, 9.7073e-01,\n",
            "        1.0588e-03, 2.3656e-06, 1.0044e-08, 7.5731e-11])\n",
            "winner_index tensor([5])\n",
            "prediction tensor([[0.5556]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABKUlEQVR4nO3aIU5DURRF0fMpKalENSAJFs00GAdzwjEURoBCoFAEHAZS8xkByHNF17LPHLVzxVvWdQ0AHSfTAwCOiegCFIkuQJHoAhSJLkDR6X+Pd0/3418bLndf0xOSJFe7z+kJuT57n56QJLnZfkxPyH6znZ6QJHk+bKYn5OVwMT0hSfL6s5+ekLfv8+kJSZLH24flrzeXLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5A0bKu6/QGgKPh0gUoEl2AItEFKBJdgCLRBSgSXYCiX7bsFlnP2NvLAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.3184]]), tensor([[0.4691]])]\n",
            "pattern_similarity_begin.sim tensor([0.7843, 0.8373, 0.8965, 0.9527, 0.9524, 0.9020, 0.8422, 0.7887, 0.7408,\n",
            "        0.6979])\n",
            "activation_begin tensor([2.4402e-08, 4.8860e-06, 1.8242e-03, 5.0578e-01, 4.8922e-01, 3.1602e-03,\n",
            "        7.9908e-06, 3.7886e-08, 3.1537e-10, 4.3434e-12])\n",
            "winner_index tensor([4])\n",
            "prediction tensor([[0.4444]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABJElEQVR4nO3aMS4EUBSF4TNCRCKikmglEtuwCuuwETuyBq2GjmYqlRHFswLKcyXzfe1rTvXnFm+z1goAHQfTAwD2iegCFIkuQJHoAhSJLkDR4V+P9093418brk620xOSJDfH79MTcn30MT3h33j9PpuekCR5/rqcnpCX3cX0hCTJ2+f59IRsd6fTE5Ikj7cPm9/eXLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWbtdb0BoC94dIFKBJdgCLRBSgSXYAi0QUoEl2Aoh8xKxdZyJXqIQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.3858]]), tensor([[0.4461]])]\n",
            "pattern_similarity_begin.sim tensor([0.7733, 0.8250, 0.8828, 0.9474, 0.9804, 0.9133, 0.8523, 0.7978, 0.7489,\n",
            "        0.7052])\n",
            "activation_begin tensor([9.7589e-10, 1.7184e-07, 5.5572e-05, 3.5441e-02, 9.6333e-01, 1.1754e-03,\n",
            "        2.6372e-06, 1.1235e-08, 8.4992e-11, 1.0745e-12])\n",
            "winner_index tensor([4])\n",
            "prediction tensor([[0.4444]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABLUlEQVR4nO3aoU1EQRiF0fuATUAgEIRgMSRUQSFUQDXUQw84BAKFQAB+yZpHBay8v9hz7JgrJl9GzLKuawDoOJoeAHBIRBegSHQBikQXoEh0AYpO9h0+vjyMf224OfuenpAkuT39nJ6Qu83P9IQkyeXx3mtT8brbTE9Ikrztrqcn5P33anpCkuRjezE9IV/b8+kJSZLn+6flvzMvXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AomVd1+kNAAfDSxegSHQBikQXoEh0AYpEF6BIdAGK/gA/IxdZ9RAOxQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.1802]]), tensor([[0.4576]])]\n",
            "pattern_similarity_begin.sim tensor([0.8226, 0.8801, 0.9163, 0.9137, 0.9182, 0.8645, 0.8087, 0.7587, 0.7139,\n",
            "        0.6739])\n",
            "activation_begin tensor([2.8276e-05, 8.8857e-03, 3.3279e-01, 2.5525e-01, 4.0117e-01, 1.8696e-03,\n",
            "        7.0087e-06, 4.7268e-08, 5.3934e-10, 9.8496e-12])\n",
            "winner_index tensor([4])\n",
            "prediction tensor([[0.4444]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABJklEQVR4nO3aIU4DQQCF4bekhGBqEHADbkK4Wk/GDTDFoCqbqgIJDCcA+abJfp8d88z+M2KXMUYA6LiaPQBgTUQXoEh0AYpEF6BIdAGKNv8d7l6fp//a8HhzmD0hSfKwOc2ekJ9xGXfk9fI9e0L2X/ezJyRJ3j7n73g/382ekCQ5nLezJ+T4cTt7QpLk5Wm3/HV2GV8xwEqILkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIULWOM2RsAVsNLF6BIdAGKRBegSHQBikQXoEh0AYp+AcQcGVkHpEPoAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.2354]]), tensor([[0.4857]])]\n",
            "pattern_similarity_begin.sim tensor([0.8018, 0.8569, 0.9184, 0.9217, 0.9236, 0.8862, 0.8280, 0.7760, 0.7294,\n",
            "        0.6878])\n",
            "activation_begin tensor([2.0967e-06, 5.1598e-04, 2.4257e-01, 3.3908e-01, 4.0815e-01, 9.6536e-03,\n",
            "        2.8827e-05, 1.5861e-07, 1.5085e-09, 2.3406e-11])\n",
            "winner_index tensor([4])\n",
            "prediction tensor([[0.4444]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABIklEQVR4nO3aoU0FQRiF0btA8gQCgcMiKYMGaIgOaIoaUDgML0GgWUOGCkDen+SdY8fcZDdfRsy21goAHWfTAwBOiegCFIkuQJHoAhSJLkDRxV+Hjy8P408bbg8f0xOSJHeH4/SEf+M8479F3r+vpickSV73m+kJeduvpyckSY5f89/kc7+cnpAkeb5/2n47c9MFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSja1lrTGwBOhpsuQJHoAhSJLkCR6AIUiS5AkegCFP0AlqQXWb2uul4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.1860]]), tensor([[0.4355]])]\n",
            "pattern_similarity_begin.sim tensor([0.8257, 0.8836, 0.9241, 0.9219, 0.9211, 0.8593, 0.8040, 0.7545, 0.7102,\n",
            "        0.6705])\n",
            "activation_begin tensor([2.0825e-05, 6.7861e-03, 3.8962e-01, 3.1399e-01, 2.8899e-01, 5.9760e-04,\n",
            "        2.3676e-06, 1.6777e-08, 2.0009e-10, 3.8017e-12])\n",
            "winner_index tensor([4])\n",
            "prediction tensor([[0.4444]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABIElEQVR4nO3aMS6FQRiF4fOjoJGoJCzAUu7WFDZmAXQ6oRTVdQsZK6A8n+Q+TzvNaebNFLOttQJAx8n0AIBjIroARaILUCS6AEWiC1B09tfhw/Nu/GvD3fnb9IQkye3p5/SEf+M72/SEPB1upickSV4O19MT8rq/mp6QJHnfX05PyMfXxfSEJMnj7v7XS+KlC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQtK21pjcAHA0vXYAi0QUoEl2AItEFKBJdgCLRBSj6AY0UF1nmiYgwAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.1338]]), tensor([[0.4549]])]\n",
            "pattern_similarity_begin.sim tensor([0.8364, 0.8955, 0.9026, 0.9015, 0.9075, 0.8533, 0.7987, 0.7497, 0.7059,\n",
            "        0.6667])\n",
            "activation_begin tensor([3.3051e-04, 1.2211e-01, 2.4754e-01, 2.2294e-01, 4.0527e-01, 1.8013e-03,\n",
            "        7.5952e-06, 5.6908e-08, 7.1346e-10, 1.4175e-11])\n",
            "winner_index tensor([3])\n",
            "prediction tensor([[0.3333]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABKElEQVR4nO3aoU1EQQBF0TeEDUEgUFACdaDojALoigpIQCEQKNxuCDBUAPLNJv8cO+aZf2fEH3POANBxsnoAwJaILkCR6AIUiS5AkegCFJ3+d/jwdLv814abs7fVE5Iku/G9ekJ+5nHckbvxtXpCnj+vV09IkrwcrlZPyOv+cvWEJMn7/mL1hHwczldPSJI83t2Pv86O4ysG2AjRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYCiMedcvQFgM7x0AYpEF6BIdAGKRBegSHQBikQXoOgXuBQZWQI/6tAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.3571]]), tensor([[0.4654]])]\n",
            "pattern_similarity_begin.sim tensor([0.7758, 0.8278, 0.8860, 0.9509, 0.9652, 0.9112, 0.8504, 0.7961, 0.7474,\n",
            "        0.7038])\n",
            "activation_begin tensor([4.8139e-09, 8.7300e-07, 2.9179e-04, 1.9306e-01, 8.0299e-01, 3.6501e-03,\n",
            "        8.3739e-06, 3.6394e-08, 2.8027e-10, 3.6001e-12])\n",
            "winner_index tensor([4])\n",
            "prediction tensor([[0.4444]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABK0lEQVR4nO3aoU1EQRiF0fsIIS8kJCgMBoeiB+rYGiiGeigChwNHEAQ0SxCPCkDeX+w5dsw182XELNu2BYCOo+kBAIdEdAGKRBegSHQBikQXoOj4v8O7x93414ar9XN6QpLken2bnpCbk4/pCUmS02WZnpDnn3V6QpLk6ftyekJe9hfTE5Ikr/vz6Ql5/zqbnpAkebi9//OSeOkCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhQt27ZNbwA4GF66AEWiC1AkugBFogtQJLoARaILUPQLOX8XWSUPYTsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.2748]]), tensor([[0.3631]])]\n",
            "pattern_similarity_begin.sim tensor([0.8184, 0.8754, 0.9391, 0.9712, 0.9219, 0.8600, 0.8046, 0.7551, 0.7107,\n",
            "        0.6710])\n",
            "activation_begin tensor([2.1904e-07, 6.5485e-05, 3.8266e-02, 9.5477e-01, 6.8798e-03, 1.4102e-05,\n",
            "        5.5432e-08, 3.9004e-10, 4.6226e-12, 8.7335e-14])\n",
            "winner_index tensor([3])\n",
            "prediction tensor([[0.3333]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABKUlEQVR4nO3aMUpDQRiF0fuiiDaChSBuQHAjATemW3MBaewEIVhZJojwXIGW9y9yTjvNbeZjilnWdQ0AHZvpAQCnRHQBikQXoEh0AYpEF6Do/L/D593T+NeGh8v99IQkyePF5/SE3G6W6QlJkvefs+kJ2X3fT09Ikrwd76Yn5ONwMz0hSbI/XE9PyNfxanpCkuR1+/LnZfXSBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoWtZ1nd4AcDK8dAGKRBegSHQBikQXoEh0AYpEF6DoFzY6GFmODUrWAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.2635]]), tensor([[0.4150]])]\n",
            "pattern_similarity_begin.sim tensor([0.8095, 0.8655, 0.9280, 0.9513, 0.9351, 0.8718, 0.8152, 0.7645, 0.7192,\n",
            "        0.6786])\n",
            "activation_begin tensor([5.3515e-07, 1.4419e-04, 7.5012e-02, 7.7191e-01, 1.5266e-01, 2.7232e-04,\n",
            "        9.4527e-07, 5.9509e-09, 6.3845e-11, 1.1034e-12])\n",
            "winner_index tensor([3])\n",
            "prediction tensor([[0.3333]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABK0lEQVR4nO3aMUoDURiF0ZsgpBBErLS3ciF2biqdG3MFIlhaScBWEkQcV6Dl/Ys5p33NbeZ7r5jNsiwBoGM7PQBgTUQXoEh0AYpEF6BIdAGKzv473D8/jP/acLs7TE9Iktzt3qcn5HL7PT0hSfL5M39Xv3xdT09IkryebqYn5O10NT0hSXI4XkxPyMfxfHpCkuTp/nHz19n81wOwIqILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWbZVmmNwCshpcuQJHoAhSJLkCR6AIUiS5AkegCFP0CTfgZWYkICNkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.5323]]), tensor([[0.3849]])]\n",
            "pattern_similarity_begin.sim tensor([0.7559, 0.8055, 0.8610, 0.9231, 0.9527, 0.9400, 0.8762, 0.8191, 0.7680,\n",
            "        0.7223])\n",
            "activation_begin tensor([2.1289e-09, 3.0539e-07, 7.8546e-05, 3.8780e-02, 7.4955e-01, 2.1123e-01,\n",
            "        3.5802e-04, 1.1867e-06, 7.1711e-09, 7.4151e-11])\n",
            "winner_index tensor([4])\n",
            "prediction tensor([[0.4444]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABKklEQVR4nO3aoU0EQBiE0TkgBBQJCIJB4a4GirmeqIQyKIAEibkEgyQXEEsFIOcX955dM2LzZcVu1loBoONkegDAMRFdgCLRBSgSXYAi0QUoOvvvcPeyG//acH/5OT0hSfJw8TE9Idvz/fSEJMn16c/0hBzWZnpCkuT1+3Z6Qt4Od9MTkiTvh5vpCdl/XU1PSJI8Pz79eUG9dAGKRBegSHQBikQXoEh0AYpEF6BIdAGKRBegSHQBikQXoEh0AYpEF6BIdAGKRBegSHQBikQXoEh0AYpEF6BIdAGKRBegSHQBikQXoEh0AYpEF6BIdAGKRBegSHQBikQXoEh0AYpEF6BIdAGKRBegSHQBijZrrekNAEfDSxegSHQBikQXoEh0AYpEF6BIdAGKfgHMBhhZam5uoQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.3339]]), tensor([[0.4606]])]\n",
            "pattern_similarity_begin.sim tensor([0.7823, 0.8351, 0.8940, 0.9599, 0.9597, 0.9035, 0.8435, 0.7899, 0.7419,\n",
            "        0.6989])\n",
            "activation_begin tensor([9.7288e-09, 1.9038e-06, 6.9279e-04, 5.0408e-01, 4.9344e-01, 1.7844e-03,\n",
            "        4.4400e-06, 2.0750e-08, 1.7053e-10, 2.3217e-12])\n",
            "winner_index tensor([4])\n",
            "prediction tensor([[0.4444]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABKElEQVR4nO3aIU6dQRiG0e+SBhA1CFQNDsUiWAXrYCPdEWuow1RUkGAIwZFL0uRnBSCfuck9x455xeTJiNlt2zYANE5WDwA4JqILEBJdgJDoAoREFyD047vD+z93y782XJ2/rp4wMzPXZ8+rJ8zN6dvqCQfj7/+fqyfMzMzj/tfqCfPv43L1hJmZeXq/WD1hXvaHcS8ebn/vvjrz0gUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUI7bZtW70B4Gh46QKERBcgJLoAIdEFCIkuQEh0AUKfNm8XWYU2iv0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.4688]]), tensor([[0.4945]])]\n",
            "pattern_similarity_begin.sim tensor([0.7452, 0.7936, 0.8477, 0.9081, 0.9756, 0.9525, 0.8874, 0.8291, 0.7770,\n",
            "        0.7303])\n",
            "activation_begin tensor([8.8811e-11, 1.1232e-08, 2.5095e-06, 1.0586e-03, 9.0871e-01, 9.0098e-02,\n",
            "        1.3386e-04, 3.9435e-07, 2.1444e-09, 2.0177e-11])\n",
            "winner_index tensor([4])\n",
            "prediction tensor([[0.4444]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABLElEQVR4nO3aPS5FURiF4XVE/FQKUWhEzwzMRG9MZmEcRiAqlahEorq5Eo4RUK6vuM/T7mZVb75iL+u6BoCOvekBALtEdAGKRBegSHQBikQXoGj/v8fbx7vxrw0Xxx/TE5Ikl0fv0xNydfg6PSFJcn2wnZ6Qz5/v6QlJkqev0+kJed6eT09IkrxszqYn5G1zMj0hSfJwc7/89ebSBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoEl2AItEFKBJdgCLRBSgSXYAi0QUoWtZ1nd4AsDNcugBFogtQJLoARaILUCS6AEWiC1D0C+RNGVnSpiFcAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.5008]]), tensor([[0.4315]])]\n",
            "pattern_similarity_begin.sim tensor([0.7518, 0.8010, 0.8560, 0.9174, 0.9774, 0.9433, 0.8792, 0.8218, 0.7704,\n",
            "        0.7244])\n",
            "activation_begin tensor([1.5411e-10, 2.1076e-08, 5.1391e-06, 2.3903e-03, 9.6559e-01, 3.1965e-02,\n",
            "        5.2325e-05, 1.6812e-07, 9.8799e-10, 9.9644e-12])\n",
            "winner_index tensor([4])\n",
            "prediction tensor([[0.4444]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABK0lEQVR4nO3aIUqEYRSF4fOLMlhMIlgsFnEN7kVwUa7DddgNRosyxTooDr8r0HhumOepXznp5YZvWdc1AHQcTQ8AOCSiC1AkugBFogtQJLoARcf/Pd4/P4x/bbg6/ZyekCS53mynJ+Rm8zE9IUlye7KfnpDt/md6QpLk5ftiekJevy6nJyRJ3nbn0xPyvjubnpAkebp7XP56c+kCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhQt67pObwA4GC5dgCLRBSgSXYAi0QUoEl2AItEFKPoF5UUZWUrcTDwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.4020]]), tensor([[0.4528]])]\n",
            "pattern_similarity_begin.sim tensor([0.7682, 0.8193, 0.8765, 0.9403, 0.9834, 0.9200, 0.8583, 0.8031, 0.7537,\n",
            "        0.7095])\n",
            "activation_begin tensor([4.4739e-10, 7.4207e-08, 2.2448e-05, 1.3286e-02, 9.8495e-01, 1.7385e-03,\n",
            "        3.6381e-06, 1.4562e-08, 1.0418e-10, 1.2529e-12])\n",
            "winner_index tensor([4])\n",
            "prediction tensor([[0.4444]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABKklEQVR4nO3aK05EURRE0Xr80gaDISEkSBImwUjwTIfZMAkkQWEwpAW600E8BBpkHdFr2WtK7Rxxl3VdA0DH0fQAgEMiugBFogtQJLoARaILUHTy3+Pjy8P414abzdf0hCTJ7eZzekLuzrbTE5Ik18en0xPy9j294Nfr/mp6Qt73l9MTkiQfu4vpCdnuzqcnJEme75+Wv95cugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARcu6rtMbAA6GSxegSHQBikQXoEh0AYpEF6BIdAGKfgBBMxdZ+GBi/gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.4078]]), tensor([[0.3905]])]\n",
            "pattern_similarity_begin.sim tensor([0.7806, 0.8331, 0.8919, 0.9575, 0.9705, 0.9035, 0.8435, 0.7899, 0.7418,\n",
            "        0.6989])\n",
            "activation_begin tensor([4.4346e-09, 8.5024e-07, 3.0241e-04, 2.1449e-01, 7.8424e-01, 9.6472e-04,\n",
            "        2.4012e-06, 1.1226e-08, 9.2275e-11, 1.2566e-12])\n",
            "winner_index tensor([4])\n",
            "prediction tensor([[0.4444]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABLUlEQVR4nO3aoU0EURiF0TuEwIZgECgkAkURlEETlEJH1IAgwSJIMCtwZAliqADk/cWcY5+5Zr73xCzrugaAjqPpAQBbIroARaILUCS6AEWiC1B0/N/hw/P9+K8N17v99IQkyc3px/SE3J58Tk9IkuyW+bv65edsekKS5PVwNT0hb9+X0xOSJO9fF9MTsj+cT09IkjzdPS5/nc1/PQAbIroARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILULSs6zq9AWAzvHQBikQXoEh0AYpEF6BIdAGKRBeg6Bc7lxdZiTgbmAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.3609]]), tensor([[0.4315]])]\n",
            "pattern_similarity_begin.sim tensor([0.7822, 0.8349, 0.8938, 0.9597, 0.9690, 0.9021, 0.8423, 0.7888, 0.7409,\n",
            "        0.6980])\n",
            "activation_begin tensor([5.5084e-09, 1.0760e-06, 3.9075e-04, 2.8367e-01, 7.1504e-01, 8.9365e-04,\n",
            "        2.2561e-06, 1.0682e-08, 8.8809e-11, 1.2218e-12])\n",
            "winner_index tensor([3])\n",
            "prediction tensor([[0.3333]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABKklEQVR4nO3aMS4FYRiF4TMi3EKhkuh0KouwCuuwETuyBtFplDQiSpdmrIDyfJJ5nvZvTjHzzl/Msq5rAOg4mB4AsCWiC1AkugBFogtQJLoARYd/Hd4+3Iz/2nCxe5+ekCS5PH6dnpCro4/pCUmS3TL/rX78PpmekCR5+jqfnpDn/dn0hCTJy+fp9IS87f/Hc3F/fbf8djb/9gBsiOgCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQNGyruv0BoDNcNMFKBJdgCLRBSgSXYAi0QUoEl2Aoh86gxdZ2Ux+ngAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.4291]]), tensor([[0.4763]])]\n",
            "pattern_similarity_begin.sim tensor([0.7573, 0.8072, 0.8629, 0.9251, 0.9844, 0.9350, 0.8717, 0.8151, 0.7644,\n",
            "        0.7191])\n",
            "activation_begin tensor([1.3624e-10, 1.9885e-08, 5.2139e-06, 2.6303e-03, 9.9030e-01, 7.0541e-03,\n",
            "        1.2606e-05, 4.3812e-08, 2.7622e-10, 2.9667e-12])\n",
            "winner_index tensor([4])\n",
            "prediction tensor([[0.4444]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABLUlEQVR4nO3aIU4DURiF0VtCGjAIFBgUhrCGLqZ7YiUsgwWQYEgwJDVYGiqGFYC8v5hz7DPXzPeemM2yLAGg42x6AMCaiC5AkegCFIkuQJHoAhSd/3e4f9mP/9pwd/k1PSFJcn9xmJ6Qx+3n9IQkycN2/q5+P52mJyRJXn9upifk7Xg7PSFJ8nG8np6Qw/fV9IQkyfPuafPX2fzXA7AiogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARaILUCS6AEWiC1AkugBFogtQJLoARZtlWaY3AKyGly5AkegCFIkuQJHoAhSJLkCR6AIU/QJTzhhZ0xb8egAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.3055]]), tensor([[0.5622]])]\n",
            "pattern_similarity_begin.sim tensor([0.7689, 0.8201, 0.8773, 0.9226, 0.9197, 0.9240, 0.8658, 0.8098, 0.7597,\n",
            "        0.7149])\n",
            "activation_begin tensor([7.2357e-08, 1.2093e-05, 3.6892e-03, 3.4374e-01, 2.5646e-01, 3.9491e-01,\n",
            "        1.1757e-03, 4.3460e-06, 2.8954e-08, 3.2672e-10])\n",
            "winner_index tensor([4])\n",
            "prediction tensor([[0.4444]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABJ0lEQVR4nO3aK04EURiE0WoYMQaFAAeOVbAUPPthK2wDj8CgCALNIyGXFYCsn2TOsW0qN91fruhtrRUAOo6mBwAcEtEFKBJdgCLRBSgSXYCi3V8Pbx9uxn9tuNi/TU9IklztX6Yn5HL3P87iO9v0hBxn/NVMkjx+nU9PyNPn2fSEJMnz++n0hLx+nExPSJLcX9/9+pG46QIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFG1rrekNAAfDTRegSHQBikQXoEh0AYpEF6BIdAGKfgCwnxdZAcalaAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "composite input,output [tensor([[0.2828]]), tensor([[0.5965]])]\n",
            "pattern_similarity_begin.sim tensor([0.7681, 0.8192, 0.8763, 0.9064, 0.9032, 0.9070, 0.8711, 0.8146, 0.7639,\n",
            "        0.7187])\n",
            "activation_begin tensor([3.4281e-07, 5.6766e-05, 1.7140e-02, 3.4888e-01, 2.5291e-01, 3.7076e-01,\n",
            "        1.0216e-02, 3.5724e-05, 2.2647e-07, 2.4444e-09])\n",
            "winner_index tensor([5])\n",
            "prediction tensor([[0.5556]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABKUlEQVR4nO3aoVEEQRiE0V4OcQ6KKgQKSRSEgicfUiENAkCiDoPBcAJYIgDZP1X7nh3TZr8Zscu6rgGg42R6AMCWiC5AkegCFIkuQJHoAhSd/nV4/3Q3/mvD9f5tekKS5GZ/mJ6Qy9379IQkydc/uKt3+Z6ekCR5/TyfnpDn49X0hCTJy/FiekIOH2fTE5Ikj7cPy29n818PwIaILkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIUiS5AkegCFIkuQJHoAhSJLkCR6AIULeu6Tm8A2AwvXYAi0QUoEl2AItEFKBJdgCLRBSj6AaivF1kiiLApAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa4LHN3uHaoX"
      },
      "source": [
        "Sampled prediction now typically the mean of X and Y values. This indicates that the pipeline is working well."
      ]
    }
  ]
}