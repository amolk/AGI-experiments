{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pattern Machine.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMO+342Zj6dhnrNrkjk+hIC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7312f21ee3274f8b9dd27ecaa9d46574": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b02776f047a24cbb8c492e7b76ddb6a8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_739a3892a73e4536873eacd24443b630",
              "IPY_MODEL_7d626b1d7d784f578bd5cf4646bc9568"
            ]
          }
        },
        "b02776f047a24cbb8c492e7b76ddb6a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "739a3892a73e4536873eacd24443b630": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8cd276f9663f4cc89c3ed3707ba79f84",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 506,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 506,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_526ce7854b0a42d899c6544b5a25f07b"
          }
        },
        "7d626b1d7d784f578bd5cf4646bc9568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6370e6c475824833aac519bced346a10",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 506/506 [00:02&lt;00:00, 230.01it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f35273820bdd425bb31c9863f90d764c"
          }
        },
        "8cd276f9663f4cc89c3ed3707ba79f84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "526ce7854b0a42d899c6544b5a25f07b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6370e6c475824833aac519bced346a10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f35273820bdd425bb31c9863f90d764c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amolk/AGI-experiments/blob/master/Pattern%20Machine/04_Pattern_Machine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH0h1CpkBIIc"
      },
      "source": [
        "Pattern machine\n",
        "- continuous, i.e. activation changes over time as decaying history of instantaneous activation\n",
        "- modular, i.e. connect up more flexibly,\n",
        "- use multi-patterns, i.e. pattern contains more than 1 weights tensor. This is to represent input and output jointly.\n",
        "\n",
        "Architectural decisions\n",
        "- Signal, SignalGrid, CompositeSignalGrid\n",
        "- input patches may overlap when utility factor > 1, fixed number (a grid) of patterns per patch\n",
        "- output neighborhood is the patterns corresponding to a neighborhood of patches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4SBcILxqTNy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae34b115-350c-4983-b595-29e379139fe8"
      },
      "source": [
        "!pip install ipytest"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ipytest\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/d6/1797c114d57ec1c93b8078d81bd09b9f82d5f3a989c11fd1c575ff2846e7/ipytest-0.9.1-py3-none-any.whl\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from ipytest) (5.5.0)\n",
            "Collecting pytest>=5.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/15/5ef931cbd22585865aad0ea025162545b53af9319cf38542e0b7981d5b34/pytest-6.2.1-py3-none-any.whl (279kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 286kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from ipytest) (20.8)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->ipytest) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->ipytest) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->ipytest) (4.3.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->ipytest) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->ipytest) (51.0.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->ipytest) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->ipytest) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->ipytest) (2.6.1)\n",
            "Collecting pluggy<1.0.0a1,>=0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.4->ipytest) (1.10.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.6/dist-packages (from pytest>=5.4->ipytest) (1.1.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.4->ipytest) (20.3.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from pytest>=5.4->ipytest) (0.10.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.4->ipytest) (3.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->ipytest) (2.4.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ipytest) (0.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->ipytest) (1.15.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->ipytest) (0.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipytest) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.4->ipytest) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.4->ipytest) (3.4.0)\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pluggy, pytest, ipytest\n",
            "  Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "Successfully installed ipytest-0.9.1 pluggy-0.13.1 pytest-6.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQhsjAS1XKB5"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 1\n",
        "import ipytest\n",
        "ipytest.autoconfig()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WI0DQENlMSC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef27171-7fa9-4ef2-e8e4-a0d78ca63a57"
      },
      "source": [
        "%%writefile utils.py\n",
        "import pdb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
        "\n",
        "def pretty_s(name, clas, indent=0):\n",
        "  if type(clas).__name__ == 'Tensor':\n",
        "    return ' ' * indent + name + \":\" + type(clas).__name__ + \" size\" + str(tuple(clas.shape))\n",
        "\n",
        "  strs = []\n",
        "  strs.append(' ' * indent + name + \":\" + type(clas).__name__)\n",
        "\n",
        "  indent += 2\n",
        "  for k,v in clas.__dict__.items():\n",
        "    if '__dict__' in dir(v):\n",
        "      strs.append(pretty_s(k, v,indent))\n",
        "    elif '__iter__' in dir(v):\n",
        "      if type(v) is tuple:\n",
        "        strs.append(' ' * indent + k + ' = ' + str(v))\n",
        "      else:\n",
        "        strs.append(' ' * indent +  k + ' = [')\n",
        "        for index, item in enumerate(v):\n",
        "          if '__dict__' in dir(item):\n",
        "            strs.append(pretty_s(str(index), item, indent+2))\n",
        "          else:\n",
        "            strs.append(' ' * (indent+2) + str(item))\n",
        "        strs.append(' ' * indent +  ']')\n",
        "    else:\n",
        "      strs.append(' ' * indent +  k + ' = ' + str(v))\n",
        "\n",
        "  return \"\\n\".join(strs)\n",
        "          \n",
        "def pretty_print(name, clas, indent=0):\n",
        "  print(pretty_s(name, clas, indent))\n",
        "\n",
        "def soft_add(a, b, tau):\n",
        "  return a * (1 - tau) + b * tau\n",
        "\n",
        "def add_gaussian_noise(tensor, mean=0., std=1.):\n",
        "    t = tensor + torch.randn(tensor.size()).to(device) * std + mean\n",
        "    t.to(device)\n",
        "    return t\n",
        "\n",
        "def plot_patterns(patterns, pattern_lr, dataset, voronoi=False, annotate=False, figsize=(7,7), dpi=100):\n",
        "  patterns = patterns.cpu()\n",
        "  dataset = dataset.cpu()\n",
        "  assert len(patterns.shape) == 2 # (pattern count, 2)\n",
        "  assert patterns.shape[1] == 2 # 2D\n",
        "\n",
        "  rgba_colors = torch.zeros((patterns.shape[0], 4))\n",
        "\n",
        "  # for blue the last column needs to be one\n",
        "  rgba_colors[:,2] = 1.0\n",
        "  # the fourth column needs to be your alphas\n",
        "  if pattern_lr is not None:\n",
        "    alpha = (1.1 - pattern_lr.cpu()).clamp(0, 1) * 0.9\n",
        "    rgba_colors[:, 3] = alpha\n",
        "  else:\n",
        "    rgba_colors[:, 3] = 1.0\n",
        "\n",
        "  plt.figure(figsize=figsize, dpi=dpi)\n",
        "  ax = plt.gca()\n",
        "  ax.cla() # clear things for fresh plot\n",
        "\n",
        "  if annotate:\n",
        "    for i in range(patterns.shape[0]):\n",
        "      ax.annotate(str(i), (patterns[i][0], patterns[i][1]), xytext=(5,-3), textcoords='offset points')\n",
        "\n",
        "  ax.scatter(patterns[:, 0], patterns[:, 1], marker='.', c=rgba_colors, s=50)\n",
        "  ax.scatter(dataset[:, 0], dataset[:, 1], marker='.', c='r', s=10)\n",
        "\n",
        "  if voronoi:\n",
        "    vor = Voronoi(patterns)\n",
        "    vor_fig = voronoi_plot_2d(vor, ax=ax, show_vertices=False, line_colors='gray',\n",
        "                              line_width=1, line_alpha=0.2, point_size=0)\n",
        "\n",
        "  ax.set_xlim(0, 1)\n",
        "  ax.set_ylim(0, 1)\n",
        "  plt.show()\n",
        "\n",
        "\"\"\"\n",
        "Create a numpy array of given shape, each element initialized using supplied function\n",
        "Example: make_ndarray((2,3), lambda multi_index:multi_index)\n",
        "\"\"\"\n",
        "def make_ndarray(shape, fn):\n",
        "  a = np.empty(shape, dtype=object)\n",
        "  with np.nditer(a, flags=['refs_ok', 'multi_index'], op_flags=['readwrite']) as it:\n",
        "    for x in it:\n",
        "      a[it.multi_index] = fn(it.multi_index)\n",
        "\n",
        "  return a"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFuOT8nFZ9pl"
      },
      "source": [
        "# !pip install matplotlib scipy\n",
        "%aimport utils"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lbp81xkiBF_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac6615c4-536b-4dad-9bcf-1246dc906c38"
      },
      "source": [
        "%%writefile pattern.py\n",
        "\n",
        "import torch\n",
        "import numpy\n",
        "import pdb\n",
        "from typing import List, Tuple\n",
        "from utils import pretty_s\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class GridShapeMismatchError(Exception): pass\n",
        "class NoComponentsError(Exception): pass\n",
        "\n",
        "class SignalUtils:\n",
        "  @staticmethod\n",
        "  def compute_precision(variance):\n",
        "    return torch.exp(-variance)\n",
        "\n",
        "class SignalGridHP:\n",
        "  def __init__(self, grid_shape:Tuple, signal_shape:Tuple, init_pixel_scale:float=0.1, init_variance:float=10):\n",
        "    self.grid_shape = grid_shape\n",
        "    self.grid_size = np.prod(self.grid_shape)\n",
        "    if self.grid_size <= 0:\n",
        "      raise ValueError(\"Invalid grid size\")\n",
        "\n",
        "    self.signal_shape = tuple(signal_shape)\n",
        "    self.signal_size = np.prod(self.signal_shape)\n",
        "    if self.signal_size <= 0:\n",
        "      raise ValueError(\"Invalid signal size\")\n",
        "\n",
        "    self.init_pixel_scale = init_pixel_scale\n",
        "    self.init_variance = init_variance\n",
        "\n",
        "class SignalGrid:\n",
        "  def __init__(self, hp:SignalGridHP, alloc_pixels=True, pixels=None):\n",
        "    self.hp = hp\n",
        "    if pixels is not None:\n",
        "      self.pixels = pixels\n",
        "    elif alloc_pixels:\n",
        "      self.pixels = torch.rand((hp.grid_size, hp.signal_size)).to(device) * hp.init_pixel_scale\n",
        "    else:\n",
        "      self.pixels = None\n",
        "\n",
        "    self.variance = torch.ones((hp.grid_size, hp.signal_size)).to(device) * hp.init_variance\n",
        "    self.refresh_precision()\n",
        "\n",
        "  def refresh_precision(self):\n",
        "    self.precision = SignalUtils.compute_precision(self.variance)\n",
        "\n",
        "  @property\n",
        "  def signal_shape(self):\n",
        "    return self.hp.signal_shape\n",
        "\n",
        "  @property\n",
        "  def __dict__(self):\n",
        "    return {\n",
        "        'grid_shape': self.hp.grid_shape,\n",
        "        'signal_shape': self.hp.signal_shape,\n",
        "        'pixels': self.pixels,\n",
        "        'precision': self.precision\n",
        "    }\n",
        "\n",
        "  def __repr__(self):\n",
        "    return pretty_s(\"\", self)\n",
        "\n",
        "class CompositeSignalGridHP:\n",
        "  def __init__(self, hps:List[SignalGridHP]):\n",
        "    if len(hps) == 0:\n",
        "      raise NoComponentsError(\"Must specify at least one component\")\n",
        "\n",
        "    self.components = hps\n",
        "    self.grid_shape = hps[0].grid_shape\n",
        "\n",
        "    # all components must have same grid size\n",
        "    for component_hp in hps:\n",
        "      if component_hp.grid_shape != hps[0].grid_shape:\n",
        "        raise GridShapeMismatchError\n",
        "\n",
        "class CompositeSignalGrid:\n",
        "  @staticmethod\n",
        "  def from_pixels_list(pixels_list:List[torch.Tensor], variance:float=0.0):\n",
        "    signal_hps = [SignalGridHP(grid_shape=(1,1), signal_shape=pixels.shape, init_variance=variance) for pixels in pixels_list]\n",
        "    hp = CompositeSignalGridHP(hps=signal_hps)\n",
        "    result = CompositeSignalGrid(hp=hp, alloc=False)\n",
        "    for index, component in enumerate(result.components):\n",
        "      component.pixels = pixels_list[index]\n",
        "    return result\n",
        "\n",
        "  @staticmethod\n",
        "  def from_pixels_and_variances_list(pixels_list:List[torch.Tensor], variances_list:List[torch.Tensor], signal_shape:List[Tuple]):\n",
        "    signal_hps = [SignalGridHP(grid_shape=(1,1), signal_shape=signal_shape[index], init_variance=0.0) for index, pixels in enumerate(pixels_list)]\n",
        "    hp = CompositeSignalGridHP(hps=signal_hps)\n",
        "    result = CompositeSignalGrid(hp=hp, alloc=False)\n",
        "    for index, component in enumerate(result.components):\n",
        "      component.pixels = pixels_list[index]\n",
        "      component.variance = variances_list[index]\n",
        "      component.refresh_precision()\n",
        "    return result\n",
        "\n",
        "  def from_signal_grids(signal_grids:List[SignalGrid]):\n",
        "    result = CompositeSignalGrid(hp=None)\n",
        "    hp = CompositeSignalGridHP(hps=[sg.hp for sg in signal_grids])\n",
        "    result.hp = hp\n",
        "    result.components = signal_grids\n",
        "    return result\n",
        "\n",
        "  def __init__(self, hp:CompositeSignalGridHP=None, alloc=True):\n",
        "    self.hp = hp\n",
        "    if hp:\n",
        "      self.components = [SignalGrid(component_hp, alloc_pixels=alloc) for component_hp in hp.components]\n",
        "\n",
        "  # add a SignalGrid as a component  \n",
        "  def add_component(self, o:SignalGrid):\n",
        "    assert self.hp.grid_shape == o.hp.grid_shape, f\"{self.hp.grid_shape} != {o.hp.grid_shape}\"\n",
        "\n",
        "    self.hp.components.append(o.hp)\n",
        "    self.components.append(o)\n",
        "\n",
        "  @property\n",
        "  def signal_shape(self):\n",
        "    return [c.hp.signal_shape for c in self.components]\n",
        "\n",
        "  @property\n",
        "  def pixels(self):\n",
        "    return [component.pixels for component in self.components]\n",
        "\n",
        "  @property\n",
        "  def grid_shape(self):\n",
        "    return self.hp.grid_shape\n",
        "\n",
        "  @property\n",
        "  def component_count(self):\n",
        "    return len(self.components)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    pixels_list = [component.pixels[index] for component in self.components]\n",
        "    variances_list = [component.variance[index] for component in self.components]\n",
        "\n",
        "    return CompositeSignalGrid.from_pixels_and_variances_list(pixels_list, variances_list, self.signal_shape)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return pretty_s(\"\", self)\n",
        "\n",
        "class PatternGridHP:\n",
        "  def __init__(self, grid_shape, pattern_composite_signal_shape:Tuple):\n",
        "    self.grid_shape = grid_shape\n",
        "    self.grid_size = np.prod(self.grid_shape)\n",
        "\n",
        "    self.composite_signal_grid_hp =  CompositeSignalGridHP(hps=[SignalGridHP(grid_shape=grid_shape, signal_shape=signal_shape) for signal_shape in pattern_composite_signal_shape])\n",
        "\n",
        "class PatternGrid:\n",
        "  def __init__(self, hp:PatternGridHP):\n",
        "    self.hp = hp\n",
        "    self.composite_signal_grid_begin = CompositeSignalGrid(hp.composite_signal_grid_hp) # Trajectory begin\n",
        "    self.composite_signal_grid_end = CompositeSignalGrid(hp.composite_signal_grid_hp)   # Trajectory end\n",
        "    self.alpha = torch.ones((hp.grid_size,)).to(device)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return pretty_s(\"\", self)\n",
        "\n",
        "  @property\n",
        "  def pixels(self):\n",
        "    return [\n",
        "      [component.pixels for component in self.composite_signal_grid_begin.components],\n",
        "      [component.pixels for component in self.composite_signal_grid_end.components]\n",
        "    ]\n",
        "\n",
        "class PatternSimilarityHP:\n",
        "  def __init__(self, enable_precision_weighted_distance=True):\n",
        "    self.enable_precision_weighted_distance = enable_precision_weighted_distance\n",
        "\n",
        "class PatternSimilarity:\n",
        "  def __init__(self, signal:CompositeSignalGrid, patterns:CompositeSignalGrid, hp:PatternSimilarityHP=None):\n",
        "    \"\"\"\n",
        "    signal must be        grid_shape=(grid_shape)                               signal_shape=(composite_signal_shapes)\n",
        "    patterns must be      grid_shape=(grid_shape, per_item_pattern_grid_shape)  signal_shape=(composite_signal_shapes)\n",
        "\n",
        "    Each signal is compared with corresponding per_x_pattern_grid_shape patterns.\n",
        "    Each comparison is done across composite signal components\n",
        "    \"\"\"\n",
        "    if hp:\n",
        "      self.hp = hp\n",
        "    else:\n",
        "      self.hp = PatternSimilarityHP()\n",
        "\n",
        "    assert signal.component_count == patterns.component_count, f\"signal.component_count {signal.component_count} != patterns.component_count {patterns.component_count}\"\n",
        "    assert signal.signal_shape == patterns.signal_shape, f\"signal.signal_shape {signal.signal_shape} != patterns.signal_shape {patterns.signal_shape}\"\n",
        "    assert patterns.grid_shape[0:len(signal.grid_shape)] == signal.grid_shape, f\"patterns.grid_shape {patterns.grid_shape} must match 0-n dimensions with signal.grid_shape {signal.grid_shape}\"\n",
        "    per_item_pattern_grid_shape = patterns.grid_shape[len(signal.grid_shape):]\n",
        "    per_item_pattern_grid_size = np.prod(per_item_pattern_grid_shape)\n",
        "\n",
        "    self.dist_1 = []\n",
        "    self.dist_d = []\n",
        "    self.dist = []\n",
        "    self.sim_components = []\n",
        "\n",
        "    # find similarity based on each signal component\n",
        "    for component_index in range(signal.component_count):\n",
        "      x_component = signal.components[component_index]\n",
        "      y_component = patterns.components[component_index]\n",
        "\n",
        "      xs = x_component.pixels.shape\n",
        "      x_component_pixels_expanded = x_component.pixels \\\n",
        "                                    .unsqueeze(dim=1) \\\n",
        "                                    .expand((xs[0], per_item_pattern_grid_size, xs[1])) \\\n",
        "                                    .reshape(-1, xs[1]) # expensive to reshape. How to vectorize better?\n",
        "      assert x_component_pixels_expanded.shape == y_component.pixels.shape\n",
        "      x_component_precision_expanded = x_component.precision \\\n",
        "                                    .unsqueeze(dim=1) \\\n",
        "                                    .expand((xs[0], per_item_pattern_grid_size, xs[1])) \\\n",
        "                                    .reshape(-1, xs[1]) # expensive to reshape. How to vectorize better?\n",
        "      assert x_component_precision_expanded.shape == y_component.precision.shape\n",
        "\n",
        "      dist_1_component, dist_d_component, dist_component = self.l2_distance(\n",
        "          x=x_component_pixels_expanded,\n",
        "          x_precision=x_component_precision_expanded,\n",
        "          y=y_component.pixels,\n",
        "          y_precision=y_component.precision)\n",
        "      \n",
        "      sim_component = torch.exp(-dist_component)\n",
        "\n",
        "      self.dist_1.append(dist_1_component)\n",
        "      self.dist_d.append(dist_d_component)\n",
        "      self.dist.append(dist_component)\n",
        "      self.sim_components.append(sim_component)\n",
        "\n",
        "    # final similarity is mean of signal component similarities\n",
        "    # this equalizes class weights for all components (e.g. modalities)\n",
        "    self.sim = torch.stack(self.sim_components).mean(dim=0)\n",
        "\n",
        "    # HACKHACK?! contrast enhancement\n",
        "    # self.sim = self.sim - self.sim.min(dim=0).values + 0.01\n",
        "\n",
        "  def l2_cross_distance(self, x, x_precision, y, y_precision):\n",
        "    xs = x.shape\n",
        "    assert len(xs) == 2\n",
        "    assert (x_precision is None) or (x_precision.shape == xs), \"Precision, if specified, must be same shape as patterns\"\n",
        "\n",
        "    ys = y.shape\n",
        "    assert len(ys) == 2\n",
        "    assert (y_precision is None) or (y_precision.shape == ys), \"Precision, if specified, must be same shape as patterns\"\n",
        "\n",
        "    assert xs[1] == ys[1], \"Patch size, i.e. dim 1, must match\"\n",
        "\n",
        "    n = xs[0]\n",
        "    m = ys[0]\n",
        "    d = xs[1]\n",
        "\n",
        "    x = x.unsqueeze(1).expand(n, m, d)\n",
        "    x_precision = x_precision.unsqueeze(1).expand(n, m, d)\n",
        "\n",
        "    y = y.unsqueeze(0).expand(n, m, d)\n",
        "    y_precision = y_precision.unsqueeze(0).expand(n, m, d)\n",
        "\n",
        "    return self.l2_distance(x, x_precision, y, y_precision)\n",
        "\n",
        "  def l2_distance(self, x, x_precision, y, y_precision):\n",
        "    dist_1 = (x - y).abs()\n",
        "    dist_d = torch.pow(dist_1, 2)\n",
        "\n",
        "    if self.hp.enable_precision_weighted_distance:\n",
        "      if x_precision is not None:\n",
        "        dist_d = dist_d * x_precision\n",
        "\n",
        "      if y_precision is not None:\n",
        "        dist_d = dist_d * y_precision\n",
        "\n",
        "    dist = dist_d.sum(-1).sqrt()\n",
        "    return dist_1, dist_d, dist\n",
        "\n",
        "  def __repr__(self):\n",
        "    s = []\n",
        "    s.append(f\"self.dist_1: {self.dist_1}\")\n",
        "    return \"\".join(s)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing pattern.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ7AetD9sXXx"
      },
      "source": [
        "# !pip install torch\n",
        "%aimport pattern"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFBqB6E_8Xrm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b335147-c021-4878-cb19-c6a0252d7518"
      },
      "source": [
        "%%writefile convoluation_utils.py\n",
        "import math\n",
        "from pattern import *\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "import pdb\n",
        "\n",
        "class ConvolutionUtils:\n",
        "  @staticmethod\n",
        "  def make_afferent_patches(signal:CompositeSignalGrid, grid_shape:Tuple, coverage_factor:float=1.0):\n",
        "    # print(\"make_afferent_patches\")\n",
        "    # print(\"  signal\", signal.signal_shape)\n",
        "    # print(\"  grid_shape\", grid_shape)\n",
        "    # print(\"  coverage_factor\", coverage_factor)\n",
        "\n",
        "    assert signal.grid_shape == (1,1)\n",
        "\n",
        "    if grid_shape == (1,1):\n",
        "      # no convoluation\n",
        "      return signal\n",
        "\n",
        "    for grid_shape_i in grid_shape:\n",
        "      assert grid_shape_i > 1\n",
        "\n",
        "    patch_signal_grids = []\n",
        "\n",
        "    for component_index in range(len(signal.components)):\n",
        "      # print(\"  component\", component_index)\n",
        "      component = signal.components[component_index]\n",
        "\n",
        "      patch_shape = tuple([int(coverage_factor *  component.signal_shape[i] / grid_shape[i]) for i in range(len(grid_shape))])\n",
        "      # print(\"    patch_shape\", patch_shape)\n",
        "      stride = tuple([math.floor((component.signal_shape[i]-patch_shape[i])/(grid_shape[i]-1)) for i in range(len(grid_shape))])\n",
        "      # print(\"    stride\", stride)\n",
        "\n",
        "      patches = ConvolutionUtils.conv_slice(component.pixels.view((1,) + component.hp.signal_shape), patch_shape, stride=stride).squeeze(dim=0)\n",
        "      sghp = SignalGridHP(grid_shape=grid_shape, signal_shape=patch_shape)\n",
        "      patch_signal_grid = SignalGrid(hp=sghp, alloc_pixels=False, pixels = patches)\n",
        "               \n",
        "      # print(\"  patch_signal_grid\", patch_signal_grid)\n",
        "      patch_signal_grids.append(patch_signal_grid)\n",
        "\n",
        "    patches = CompositeSignalGrid.from_signal_grids(patch_signal_grids)\n",
        "    return patches\n",
        "\n",
        "  @staticmethod\n",
        "  def conv_slice(images, kernel_shape, stride, padding=0):\n",
        "    assert len(images.shape) == 3, \"Must be (image count, image height, image width)\"\n",
        "    images = images.unsqueeze(1)\n",
        "\n",
        "    fold_params = dict(kernel_size=kernel_shape, dilation=1, padding=padding, stride=stride)\n",
        "    unfold = torch.nn.Unfold(**fold_params)\n",
        "    # print(\"images\", images.shape)\n",
        "    unfolded = unfold(images)\n",
        "    unfolded = unfolded.view(images.shape[0], -1, unfolded.shape[-1])\n",
        "    unfolded = unfolded.transpose(1, 2)\n",
        "    return unfolded"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing convoluation_utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1ptRk9z86rb"
      },
      "source": [
        "%aimport convoluation_utils"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnMru_ABTjKx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f200db09-248a-4f17-c818-61951dcdca73"
      },
      "source": [
        "%%run_pytest[clean]\n",
        "\"\"\"\n",
        "Test SignalGrid\n",
        "\"\"\"\n",
        "\n",
        "import pytest\n",
        "import pdb\n",
        "from pattern import *\n",
        "\n",
        "def test_signal_grid_invalid_grid_size1():\n",
        "  with pytest.raises(ValueError):\n",
        "    SignalGridHP(\n",
        "      grid_shape=(0,4),     # <-- zero\n",
        "      signal_shape=(5,6,2)\n",
        "    )\n",
        "\n",
        "def test_signal_grid_invalid_grid_size2():\n",
        "  with pytest.raises(ValueError):\n",
        "    SignalGridHP(\n",
        "      grid_shape=(1,4),\n",
        "      signal_shape=(5,-1,2) # <-- negative\n",
        "    )\n",
        "\n",
        "@pytest.fixture\n",
        "def signal_grid_hp1():\n",
        "  return SignalGridHP(\n",
        "      grid_shape=(3,4),\n",
        "      signal_shape=(5,6,2))\n",
        "\n",
        "@pytest.fixture\n",
        "def signal_grid_hp2():\n",
        "  return SignalGridHP(\n",
        "      grid_shape=(3,4),\n",
        "      signal_shape=(2,2))\n",
        "\n",
        "@pytest.fixture\n",
        "def signal_grid1(signal_grid_hp1):\n",
        "  return SignalGrid(hp=signal_grid_hp1)\n",
        "\n",
        "def test_create_signal_grid(signal_grid_hp1):\n",
        "  signal_grid = SignalGrid(hp=signal_grid_hp1)\n",
        "  assert signal_grid.pixels.shape == (3*4, 5*6*2)\n",
        "  assert signal_grid.variance.shape == signal_grid.pixels.shape\n",
        "  assert signal_grid.precision.shape == signal_grid.pixels.shape\n",
        "\n",
        "@pytest.fixture\n",
        "def signal_grid_hp_degenerate():\n",
        "  return SignalGridHP(\n",
        "      grid_shape=(1,1),\n",
        "      signal_shape=(1,1))\n",
        "\n",
        "def test_create_signal_grid_degenerate(signal_grid_hp_degenerate):\n",
        "  signal_grid = SignalGrid(hp=signal_grid_hp_degenerate)\n",
        "  assert signal_grid.pixels.shape == (1, 1)\n",
        "  assert signal_grid.variance.shape == signal_grid.pixels.shape\n",
        "  assert signal_grid.precision.shape == signal_grid.pixels.shape\n",
        "\n",
        "def test_composite_signal_grid_from_pixels_list():\n",
        "  csg = CompositeSignalGrid.from_pixels_list([torch.ones((10,10)), torch.ones((5,5))])\n",
        "  assert csg.grid_shape == (1,1)\n",
        "  assert len(csg.components) == 2\n",
        "\n",
        "  c0 = csg.components[0]\n",
        "  assert c0.hp.grid_shape == (1,1)\n",
        "  assert c0.signal_shape == (10,10)\n",
        "\n",
        "  c1 = csg.components[1]\n",
        "  assert c1.hp.grid_shape == (1,1)\n",
        "  assert c1.signal_shape == (5,5)\n",
        "\n",
        "def test_composite_signal_grid_from_signal_grids_error1():\n",
        "  grid_shape0 = (3,4)\n",
        "  signal_shape0 = (5,3,2)\n",
        "  grid_shape1 = (1,2)\n",
        "  signal_shape1 = (12,)\n",
        "\n",
        "  sgs = [\n",
        "         SignalGrid(hp=SignalGridHP(grid_shape=grid_shape0, signal_shape=signal_shape0)), \n",
        "         SignalGrid(hp=SignalGridHP(grid_shape=grid_shape1, signal_shape=signal_shape1))\n",
        "  ]\n",
        "  with pytest.raises(GridShapeMismatchError):\n",
        "    CompositeSignalGrid.from_signal_grids(sgs)\n",
        "\n",
        "def test_composite_signal_grid_from_signal_grids():\n",
        "  grid_shape = (3,4)\n",
        "  signal_shape0 = (5,3,2)\n",
        "  signal_shape1 = (12,)\n",
        "\n",
        "  sgs = [\n",
        "         SignalGrid(hp=SignalGridHP(grid_shape=grid_shape, signal_shape=signal_shape0)), \n",
        "         SignalGrid(hp=SignalGridHP(grid_shape=grid_shape, signal_shape=signal_shape1))\n",
        "  ]\n",
        "  csg = CompositeSignalGrid.from_signal_grids(sgs)\n",
        "  \n",
        "  assert csg.hp.grid_shape == grid_shape\n",
        "  assert len(csg.components) == 2\n",
        "\n",
        "  c0 = csg.components[0]\n",
        "  assert c0.hp.grid_shape == grid_shape\n",
        "  assert c0.signal_shape == signal_shape0\n",
        "\n",
        "  c1 = csg.components[1]\n",
        "  assert c1.hp.grid_shape == grid_shape\n",
        "  assert c1.signal_shape == signal_shape1\n",
        "\n",
        "def test_composite_signal_grid_from_pixels_and_variances_list():\n",
        "  pixels_list = [torch.ones((10,10)), torch.ones((5,5))]\n",
        "  variances_list = [torch.ones((10,10)), torch.ones((5,5))]\n",
        "\n",
        "  csg = CompositeSignalGrid.from_pixels_and_variances_list(pixels_list=pixels_list, variances_list=variances_list, signal_shape=[(10,10),(5,5)])\n",
        "  assert csg.grid_shape == (1,1)\n",
        "  assert len(csg.components) == 2\n",
        "\n",
        "  c0 = csg.components[0]\n",
        "  assert c0.hp.grid_shape == (1,1)\n",
        "  assert c0.signal_shape == (10,10)\n",
        "  assert c0.pixels is pixels_list[0]\n",
        "  assert c0.variance is variances_list[0]\n",
        "\n",
        "  c1 = csg.components[1]\n",
        "  assert c1.hp.grid_shape == (1,1)\n",
        "  assert c1.signal_shape == (5,5)\n",
        "  assert c1.pixels is pixels_list[1]\n",
        "  assert c1.variance is variances_list[1]\n",
        "\n",
        "def test_composite_signal_grid_indexing(signal_grid_hp1, signal_grid_hp2):\n",
        "  csg_hp = CompositeSignalGridHP(hps=[signal_grid_hp1, signal_grid_hp2])\n",
        "  csg = CompositeSignalGrid(hp=csg_hp)\n",
        "\n",
        "  for index in range(5, 10):\n",
        "    item = csg[index]\n",
        "    assert item.hp.grid_shape == (1,1)\n",
        "    assert len(item.hp.components) == len(csg.hp.components)\n",
        "    assert item.signal_shape == csg.signal_shape\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".........                                                                [100%]\n",
            "9 passed in 0.17s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBqW9IrAqxHv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e39134e8-4672-415f-c410-058813e3c6c4"
      },
      "source": [
        "%%run_pytest[clean]\n",
        "\"\"\"\n",
        "Test CompositeSignalGrid\n",
        "\"\"\"\n",
        "\n",
        "import pytest\n",
        "from pattern import *\n",
        "\n",
        "def test_csg_zero_components():\n",
        "  \"\"\"\n",
        "  Must have at least 1 component\n",
        "  \"\"\"\n",
        "  with pytest.raises(NoComponentsError):\n",
        "    CompositeSignalGridHP([])\n",
        "\n",
        "def test_csg_different_grid_shapes():\n",
        "  \"\"\"\n",
        "  Components may not have different grid shapes\n",
        "  \"\"\"\n",
        "  with pytest.raises(GridShapeMismatchError):\n",
        "    CompositeSignalGridHP([\n",
        "      SignalGridHP(grid_shape=(1,2), signal_shape=(3,4)),\n",
        "      SignalGridHP(grid_shape=(2,2), signal_shape=(3,4))\n",
        "    ])\n",
        "\n",
        "@pytest.fixture\n",
        "def composite_signal_grid1():\n",
        "  return CompositeSignalGrid(CompositeSignalGridHP([\n",
        "    SignalGridHP(grid_shape=(1,2), signal_shape=(3,4)),\n",
        "    SignalGridHP(grid_shape=(1,2), signal_shape=(3,2,1))\n",
        "  ]))\n",
        "\n",
        "def test_csg_different_signal_shapes(composite_signal_grid1):\n",
        "  \"\"\"\n",
        "  Components may have different signal shapes\n",
        "  \"\"\"\n",
        "  assert composite_signal_grid1.component_count == 2\n",
        "  assert composite_signal_grid1.components[0].signal_shape == (3,4)\n",
        "  assert composite_signal_grid1.components[1].signal_shape == (3,2,1)\n",
        "  assert composite_signal_grid1.signal_shape == [(3,4), (3,2,1)]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "...                                                                      [100%]\n",
            "3 passed in 0.02s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4lPXN94vHcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6434e496-3deb-482b-8603-c49312a2b21d"
      },
      "source": [
        "%%run_pytest[clean]\n",
        "\"\"\"\n",
        "Test PatternGrid\n",
        "\"\"\"\n",
        "\n",
        "import pytest\n",
        "from pattern import *\n",
        "\n",
        "@pytest.fixture\n",
        "def pg1():\n",
        "  pg_hp = PatternGridHP(grid_shape=(1,2), pattern_composite_signal_shape=[(3,4),(3,2,1)])\n",
        "\n",
        "  return PatternGrid(hp=pg_hp)\n",
        "\n",
        "def test_pg_create(pg1):\n",
        "  pg1.alpha.shape == (1,2)\n",
        "\n",
        "@pytest.fixture\n",
        "def pg2():\n",
        "  pg_hp = PatternGridHP(grid_shape=(1,), pattern_composite_signal_shape=[(1,),(1,)])\n",
        "\n",
        "  return PatternGrid(hp=pg_hp)\n",
        "\n",
        "def test_pg_create_2(pg2):\n",
        "  pg2.alpha.shape == (1,)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "..                                                                       [100%]\n",
            "2 passed in 0.01s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcQ4WOe3WpLi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3085a91-4f75-4855-883e-8a9d5b0c7273"
      },
      "source": [
        "%%run_pytest[clean] -s\n",
        "\"\"\"\n",
        "Test Similarity\n",
        "\"\"\"\n",
        "\n",
        "import pytest\n",
        "from pattern import *\n",
        "from utils import *\n",
        "\n",
        "def make_signal():\n",
        "  pg_hp = PatternGridHP(grid_shape=(2,4), pattern_composite_signal_shape=[(3,4),(3,2,1)])\n",
        "  return PatternGrid(hp=pg_hp)\n",
        "\n",
        "def make_patterns():\n",
        "  pg_hp = PatternGridHP(grid_shape=(2,4,3,2), pattern_composite_signal_shape=[(3,4),(3,2,1)])\n",
        "  return PatternGrid(hp=pg_hp)\n",
        "\n",
        "def test_sim_low_precision():\n",
        "  # By default, very low precision, so equal even when pixels are random\n",
        "  pgs = [make_signal(), make_patterns()]\n",
        "  sim = PatternSimilarity(pgs[0].composite_signal_grid_begin, pgs[1].composite_signal_grid_begin)\n",
        "  assert sim.sim.shape == pgs[1].hp.grid_size\n",
        "  assert torch.allclose(sim.sim, torch.ones_like(sim.sim))\n",
        "\n",
        "def test_sim_disable_precision_weighting():\n",
        "  # If disabled precision weighting, then unequal because pixels are random\n",
        "  pgs = [make_signal(), make_patterns()]\n",
        "  hp = PatternSimilarityHP(enable_precision_weighted_distance=False)\n",
        "  sim = PatternSimilarity(pgs[0].composite_signal_grid_begin, pgs[1].composite_signal_grid_begin, hp=hp)\n",
        "  assert sim.sim.shape == pgs[1].hp.grid_size\n",
        "  assert not torch.allclose(sim.sim, torch.ones_like(sim.sim))\n",
        "\n",
        "def test_sim_high_precision():\n",
        "  # If high precision, then unequal given pixels are random\n",
        "  pgs = [make_signal(), make_patterns()]\n",
        "\n",
        "  # force high precisions\n",
        "  for pg in pgs:\n",
        "    for component in pg.composite_signal_grid_begin.components:\n",
        "      component.precision = torch.ones_like(component.precision)\n",
        "\n",
        "  sim = PatternSimilarity(pgs[0].composite_signal_grid_begin, pgs[1].composite_signal_grid_begin)\n",
        "  assert sim.sim.shape == pgs[1].hp.grid_size\n",
        "  assert not torch.allclose(sim.sim, torch.ones_like(sim.sim))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "...\n",
            "3 passed in 0.05s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvisc3G4cvFB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "121660e4-c74e-4be1-c5bd-065934b5cb3d"
      },
      "source": [
        "%%writefile layer.py\n",
        "\n",
        "import torch\n",
        "import numpy\n",
        "import pdb\n",
        "from typing import List, Tuple\n",
        "from pattern import *\n",
        "import math\n",
        "from utils import *\n",
        "from convoluation_utils import ConvolutionUtils\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class CompositeSignalPatchGrid:\n",
        "  def __init__(self, signal:CompositeSignalGrid, grid_shape:Tuple, coverage_factor=1.0):\n",
        "    # signal, i.e. composite image, must be a single composite signal, i.e. its grid shape must be (1,)\n",
        "    assert signal.hp.grid_shape == (1,1), f\"{signal.hp.grid_shape} != (1,1)\"\n",
        "    self.patches:CompositeSignalGrid = ConvolutionUtils.make_afferent_patches(signal=signal, grid_shape=grid_shape, coverage_factor=coverage_factor)\n",
        "\n",
        "class NeighborhoodPatchGrid:\n",
        "  def __init__(self, signal:SignalGrid, patch_grid_shape:Tuple, per_patch_grid_shape:Tuple, patch_neighborhood_shape:Tuple):\n",
        "    # signal must be a single signal, i.e. its grid shape must be (1,)\n",
        "    assert signal.hp.grid_shape == (1,1), f\"{signal.hp.grid_shape} != (1,1)\"\n",
        "\n",
        "    # signal shape must be flattend combination of patch_grid_shape and per_patch_grid_shape\n",
        "    assert tuple(signal.hp.signal_shape) == tuple(np.multiply(patch_grid_shape, per_patch_grid_shape))\n",
        "\n",
        "    stride = per_patch_grid_shape\n",
        "    # print(\"  stride\", stride)\n",
        "\n",
        "    padding_patches = tuple([int((i-1)/2) for i in patch_neighborhood_shape])\n",
        "    padding = tuple(np.multiply(padding_patches, per_patch_grid_shape))\n",
        "    # print(\"  padding\", padding)\n",
        "\n",
        "    patch_shape = tuple(np.multiply(patch_neighborhood_shape, per_patch_grid_shape))\n",
        "    patches = ConvolutionUtils.conv_slice(signal.pixels.view(signal.signal_shape).unsqueeze(dim=0), patch_shape, stride=stride, padding=padding).squeeze(dim=0)\n",
        "    sghp = SignalGridHP(grid_shape=patch_grid_shape, signal_shape=patch_shape)\n",
        "    self.patches:SignalGrid = SignalGrid(hp=sghp, alloc_pixels=False, pixels = patches)\n",
        "\n",
        "class InputOutputPatchGrid:\n",
        "  def __init__(self, patch_grid_shape:Tuple, input:CompositeSignalGrid, output:SignalGrid, output_patch_neighborhood_shape:Tuple, per_patch_pattern_grid_shape:Tuple, input_coverage_factor=1.0):\n",
        "    input_grid_shape = output.signal_shape\n",
        "\n",
        "    self.input_patches = CompositeSignalPatchGrid(signal=input, grid_shape=patch_grid_shape, coverage_factor=input_coverage_factor)\n",
        "    self.output_patches = NeighborhoodPatchGrid(signal=output, patch_grid_shape=patch_grid_shape, per_patch_grid_shape=per_patch_pattern_grid_shape, patch_neighborhood_shape=output_patch_neighborhood_shape)\n",
        "    \n",
        "    self.patches = self.input_patches.patches\n",
        "    self.patches.add_component(self.output_patches.patches)\n",
        "\n",
        "class LayerHP:\n",
        "  def __init__(self, input_signal_shapes:List[Tuple], input_coverage_factor:float, patch_grid_shape:Tuple, per_patch_pattern_grid_shape:Tuple, output_patch_neighborhood_shape:Tuple, output_tau=0.5):\n",
        "    assert len(patch_grid_shape) == len(per_patch_pattern_grid_shape), \"This is so that output can be flattened\"\n",
        "    assert len(patch_grid_shape) == len(output_patch_neighborhood_shape), \"Output patch neighborhood is a subset of patch grid, so number of dimensions must match\"\n",
        "    # assert len(patch_grid_shape) == 2, \"Currently support for 2D\"\n",
        "\n",
        "    self.input_signal_shapes:List[Tuple] = input_signal_shapes\n",
        "    self.input_coverage_factor:float = input_coverage_factor\n",
        "    self.patch_grid_shape:Tuple = patch_grid_shape\n",
        "    self.per_patch_pattern_grid_shape:Tuple = per_patch_pattern_grid_shape\n",
        "    self.output_patch_neighborhood_shape = output_patch_neighborhood_shape\n",
        "    self.output_neighborhood_shape:Tuple = np.multiply(output_patch_neighborhood_shape, per_patch_pattern_grid_shape)\n",
        "    self.output_tau:float = output_tau\n",
        "\n",
        "    for size in self.output_patch_neighborhood_shape:\n",
        "      assert size % 2 == 1, \"Output patch neighborhood shape must be odd, so can be centered around specific output activation\"\n",
        "\n",
        "    # Get input patch shapes\n",
        "    sample_input_signal = [torch.ones(shape) for shape in input_signal_shapes]\n",
        "    sample_input = CompositeSignalGrid.from_pixels_list(sample_input_signal)\n",
        "    patches = ConvolutionUtils.make_afferent_patches(signal=sample_input, grid_shape=patch_grid_shape, coverage_factor=input_coverage_factor)\n",
        "\n",
        "    # Patterns -\n",
        "    # each patch in the patch grid will get per_patch_pattern_grid_shape patterns, so patterns will be of shape -\n",
        "    # (patch_grid_shape,) + (per_patch_pattern_grid_shape shape,) + (pattern_composite_signal_shape,)\n",
        "    pattern_composite_signal_shape = patches.signal_shape + [self.output_neighborhood_shape] # input signals + output signal\n",
        "    patch_grid_shape = patch_grid_shape\n",
        "    pattern_grid_shape = patch_grid_shape + per_patch_pattern_grid_shape # 2D, 2D = 4D\n",
        "    self.pattern_grid_hp:PatternGridHP = PatternGridHP(grid_shape=pattern_grid_shape, pattern_composite_signal_shape=pattern_composite_signal_shape)\n",
        "\n",
        "    # Output is flattened 2D version of the 4D pattern_grid_shape\n",
        "    output_shape = np.multiply(patch_grid_shape, per_patch_pattern_grid_shape) # height1*height2, width1*width2\n",
        "    self.output_hp:SignalGridHP = SignalGridHP(grid_shape=(1,1), signal_shape=output_shape, init_pixel_scale=0.0)\n",
        "\n",
        "    # print(\"Output shape\", output_shape)\n",
        "\n",
        "    # # input grid HP\n",
        "    # hps = [\n",
        "    #   SignalGridHP(\n",
        "    #       grid_shape=output_shape, # Each grid element in input grid produces 1 pixel of output\n",
        "    #       signal_shape=input_component.signal_shape)\n",
        "    #   for input_component in patches.components # each component of input\n",
        "    # ]\n",
        "    # self.input_grid_hp = CompositeSignalGridHP(hps=hps.copy())\n",
        "\n",
        "    # # pattern HP\n",
        "    # hps.append(SignalGridHP(\n",
        "    #     grid_shape=output_shape,\n",
        "    #     signal_shape=output_neighborhood_shape))\n",
        "\n",
        "    # self.pattern_grid_hp = PatternGridHP(\n",
        "    #     grid_shape=pattern_grid_shape,\n",
        "    #     composite_signal_grid_hp=CompositeSignalGridHP(hps=[\n",
        "    #       SignalGridHP(grid_shape=pattern_grid_shape, signal_shape=component.signal_shape)\n",
        "    #      for component in hps]))\n",
        "\n",
        "class LocalLearning:\n",
        "  def __init__(self, input:CompositeSignalGrid, output:SignalGrid, patch_grid_shape:Tuple, per_patch_pattern_grid_shape:Tuple, patterns:PatternGrid):\n",
        "    # Output shape example (1, 2x3x4x5), where patch_grid_shape is (2x4) and per_patch_pattern_grid_shape is (3x5)\n",
        "    assert len(patch_grid_shape) == len(per_patch_pattern_grid_shape)\n",
        "    assert output.pixels.shape[0] == 1\n",
        "    assert output.pixels.shape[2] == np.dot(patch_grid_shape, per_patch_pattern_grid_shape)\n",
        "\n",
        "    activation_shape = torch.stack((torch.tensor(patch_grid_shape), torch.tensor(per_patch_pattern_grid_shape)), dim=1).view(-1) # e.g. (2, 3, 4, 5)\n",
        "    activation = output.pixels.view(activation_shape)\n",
        "\n",
        "\n",
        "  \n",
        "class Layer:\n",
        "  def __init__(self, hp:LayerHP):\n",
        "    self.hp = hp\n",
        "    self.patterns = PatternGrid(hp=self.hp.pattern_grid_hp)\n",
        "    self.output = SignalGrid(hp.output_hp)\n",
        "    self.debug = False\n",
        "\n",
        "  def forward(self, input:CompositeSignalGrid):\n",
        "    assert input.grid_shape == (1,1) # single signal input\n",
        "\n",
        "    input_output_patch_grid = InputOutputPatchGrid(patch_grid_shape=self.hp.patch_grid_shape,\n",
        "                                                   input=input,\n",
        "                                                   output=self.output,\n",
        "                                                   output_patch_neighborhood_shape=self.hp.output_patch_neighborhood_shape,\n",
        "                                                   per_patch_pattern_grid_shape=self.hp.per_patch_pattern_grid_shape,\n",
        "                                                   input_coverage_factor=1.0)\n",
        "    \n",
        "    # BOTTOM UP\n",
        "    # Compare with pattern_end \n",
        "    self.pattern_similarity_end = PatternSimilarity(signal=input_output_patch_grid.patches, patterns=self.patterns.composite_signal_grid_end)\n",
        "    if self.debug:\n",
        "      print(\"self.pattern_similarity_end\", self.pattern_similarity_end)\n",
        "    \n",
        "    self.activation_end = self.pattern_similarity_end.sim.unsqueeze(dim=0)\n",
        "    # print(\"output shape\", self.output.pixels.shape)\n",
        "    # print(\"self.hp.patch_grid_shape\", self.hp.patch_grid_shape)\n",
        "    # print(\"self.hp.per_patch_pattern_grid_shape\", self.hp.per_patch_pattern_grid_shape)\n",
        "    assert self.output.pixels.shape == self.activation_end.shape \n",
        "\n",
        "    self.output.pixels = soft_add(self.output.pixels, self.activation_end, tau=self.hp.output_tau)\n",
        "\n",
        "    # TOP DOWN\n",
        "    # Compare with pattern_begin and send down winning pattern's _end as prediction\n",
        "    self.pattern_similarity_begin = PatternSimilarity(signal=input_output_patch_grid.patches, patterns=self.patterns.composite_signal_grid_begin)\n",
        "    if self.debug:\n",
        "      print(\"pattern_similarity_begin\", self.pattern_similarity_begin)\n",
        "\n",
        "    assert len(self.pattern_similarity_begin.sim.shape) == 1\n",
        "    # print(\"pattern_similarity_begin.sim\", self.pattern_similarity_begin.sim)\n",
        "    self.activation_begin = torch.softmax(self.pattern_similarity_begin.sim * 100, 0)\n",
        "    # print(\"activation_begin\", self.activation_begin)\n",
        "    winner_index = torch.multinomial(self.activation_begin, 1)\n",
        "    # print(\"winner_index\", winner_index)\n",
        "    self.top_down_prediction = self.patterns.composite_signal_grid_end[winner_index]\n",
        "\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting layer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozYdg-Q-q_aU"
      },
      "source": [
        "%aimport layer"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzMn4q-FrA9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "308e8208-feb6-4011-c5c6-2d89141c4f83"
      },
      "source": [
        "\n",
        "%%run_pytest[clean] -s\n",
        "\"\"\"\n",
        "Test Layer\n",
        "\"\"\"\n",
        "\n",
        "import pytest\n",
        "from utils import *\n",
        "from pattern import *\n",
        "from layer import *\n",
        "import pdb\n",
        "\n",
        "def test_layer_create():\n",
        "  hp = LayerHP(\n",
        "      input_signal_shapes=[(10,15),(20,10)],\n",
        "      input_coverage_factor=1.0, \n",
        "      patch_grid_shape=(5,5),\n",
        "      per_patch_pattern_grid_shape=(4,4),\n",
        "      output_patch_neighborhood_shape=(3,3),\n",
        "      output_tau=0.5)\n",
        "  layer = Layer(hp=hp)\n",
        "  # pretty_print(\"Layer\", layer)\n",
        "  assert(True)\n",
        "\n",
        "def test_layer_forward():\n",
        "  input_signal_shapes = [(10,10), (20,20)]\n",
        "  input_pixels_list = [torch.ones(shape) for shape in input_signal_shapes]\n",
        "  input = CompositeSignalGrid.from_pixels_list(input_pixels_list)\n",
        "\n",
        "  hp = LayerHP(\n",
        "      input_signal_shapes=input_signal_shapes,\n",
        "      input_coverage_factor=1.0, \n",
        "      patch_grid_shape=(5,5),\n",
        "      per_patch_pattern_grid_shape=(2,3),\n",
        "      output_patch_neighborhood_shape=(3,3),\n",
        "      output_tau=0.5)\n",
        "\n",
        "  layer = Layer(hp=hp)\n",
        "  #pretty_print(\"Layer\", layer)\n",
        "\n",
        "  for _ in range(100):\n",
        "    layer.forward(input)\n",
        "\n",
        "  assert torch.allclose(layer.output.pixels, torch.ones_like(layer.output.pixels), atol=0.01)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "..\n",
            "2 passed in 0.23s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGs1sDJ4BaU9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 893,
          "referenced_widgets": [
            "7312f21ee3274f8b9dd27ecaa9d46574",
            "b02776f047a24cbb8c492e7b76ddb6a8",
            "739a3892a73e4536873eacd24443b630",
            "7d626b1d7d784f578bd5cf4646bc9568",
            "8cd276f9663f4cc89c3ed3707ba79f84",
            "526ce7854b0a42d899c6544b5a25f07b",
            "6370e6c475824833aac519bced346a10",
            "f35273820bdd425bb31c9863f90d764c"
          ]
        },
        "outputId": "e444c9f0-743c-4885-f54a-85baf108c4e0"
      },
      "source": [
        "# !pip install pandas\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.datasets import load_boston\n",
        "from utils import *\n",
        "from layer import *\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def normalize(df):\n",
        "  df1 = (df - df.mean())/df.std()\n",
        "  return df1\n",
        "\n",
        "def scale(df):\n",
        "  min = df.min()\n",
        "  max = df.max()\n",
        "\n",
        "  df1 = (df - min) / (max - min)\n",
        "  return df1\n",
        "\n",
        "dataset = load_boston()\n",
        "dataset = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "dataset = pd.DataFrame(np.c_[scale(normalize(dataset['LSTAT'])), scale(normalize(dataset['RM']))], columns = ['LSTAT','RM'])\n",
        "dataset = torch.tensor(dataset.to_numpy()).float().to(device)\n",
        "dataset\n",
        "X = dataset[:,0]\n",
        "Y = dataset[:,1]\n",
        "\n",
        "class SingleLayerRegression:\n",
        "  def __init__(self, X:torch.Tensor, Y:torch.Tensor, pattern_grid_shape:Tuple=(1,10)):\n",
        "    assert X.shape[0] == Y.shape[0], \"Batch size must be same for X and Y\"\n",
        "    assert len(X.shape) >= 1, \"X must be at least 1D batch of scalars\"\n",
        "    assert len(Y.shape) >= 1, \"Y must be at least 1D batch of scalars\"\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    self.pattern_grid_shape = pattern_grid_shape\n",
        "\n",
        "    self.batch_size = X.shape[0]\n",
        "\n",
        "    # X\n",
        "    self.X_signal_shape = X.shape[1:]\n",
        "    if len(self.X_signal_shape) == 0:\n",
        "      self.X_signal_shape = (1,1)\n",
        "    elif len(self.X_signal_shape) == 1:\n",
        "      self.X_signal_shape = [1] + self.X_signal_shape\n",
        "    self.X_signal_shape = tuple(self.X_signal_shape)\n",
        "\n",
        "    # Y\n",
        "    self.Y_signal_shape = Y.shape[1:]\n",
        "    if len(self.Y_signal_shape) == 0:\n",
        "      self.Y_signal_shape = (1,1)\n",
        "    elif len(self.Y_signal_shape) == 1:\n",
        "      self.Y_signal_shape = [1] + self.Y_signal_shape\n",
        "    self.Y_signal_shape = tuple(self.Y_signal_shape)\n",
        "\n",
        "    # Layer\n",
        "    self.layer_hp = self.create_layer_hp(per_patch_pattern_grid_shape=pattern_grid_shape)\n",
        "    self.layer = Layer(hp=self.layer_hp)\n",
        "\n",
        "\n",
        "  def create_layer_hp(self, input_coverage_factor=1.0, patch_grid_shape=(1,1), per_patch_pattern_grid_shape=(1, 10), output_patch_neighborhood_shape=(1,1), output_tau=1.0):\n",
        "    hp = LayerHP(\n",
        "          input_signal_shapes=[self.X_signal_shape, self.Y_signal_shape],\n",
        "          input_coverage_factor=input_coverage_factor, \n",
        "          patch_grid_shape=patch_grid_shape,\n",
        "          per_patch_pattern_grid_shape=per_patch_pattern_grid_shape,\n",
        "          output_patch_neighborhood_shape=output_patch_neighborhood_shape,\n",
        "          output_tau=output_tau) # set output_tau=1.0 for IID data\n",
        "    return hp\n",
        "\n",
        "  def epoch(self):\n",
        "    error = 0\n",
        "    prec = 0\n",
        "\n",
        "    for i in tqdm(range(self.batch_size)):\n",
        "      input = CompositeSignalGrid.from_pixels_list([self.X[i].view(self.X_signal_shape), self.Y[i].view(self.Y_signal_shape)])\n",
        "      for component in input.components:\n",
        "        component.variance *= 0\n",
        "        component.refresh_precision()\n",
        "\n",
        "      self.layer.forward(input)\n",
        "\n",
        "      predictions = []\n",
        "      sample_count = 10\n",
        "      winner_indices = torch.multinomial(self.layer.activation_begin, sample_count, replacement=True)\n",
        "      for winner_index in winner_indices:\n",
        "        # print(\"winner_index\", winner_index)\n",
        "        top_down_prediction = self.layer.patterns.composite_signal_grid_end[winner_index]\n",
        "        # print(\"prediction\", top_down_prediction.components[1].pixels)\n",
        "        predictions.append(top_down_prediction.components[1].pixels)\n",
        "\n",
        "      expected = (self.X[i] + self.Y[i]).item()/2.0\n",
        "      predictions = torch.stack(predictions)\n",
        "      variance = torch.var(predictions)\n",
        "      precision = SignalUtils.compute_precision(variance * 100)\n",
        "      prediction = predictions.mean(dim=0)\n",
        "      error = error + (prediction.item() - expected) ** 2\n",
        "      prec = prec + precision\n",
        "\n",
        "      if i < 1:\n",
        "        print(\"Sample --\")\n",
        "        print(\"x, y\", input.pixels[0].item(), input.pixels[1].item())\n",
        "        print(\"Expected prediction\", expected)\n",
        "        print(f\"prediction ({sample_count} samples)\", prediction.item())\n",
        "        print(\"prediction precision\", precision.item())\n",
        "\n",
        "        # print(\"layer output\", self.layer.output.pixels.view(self.layer.hp.per_patch_pattern_grid_shape))\n",
        "        plt.imshow(self.layer.output.pixels.view(self.layer.hp.per_patch_pattern_grid_shape), vmin=0, vmax=1, cmap=plt.cm.viridis)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "      # print(\"patterns\", self.layer.patterns.pixels)\n",
        "    print(\"RMS Error\", (error / self.batch_size) ** 0.5)\n",
        "    print(\"Mean precision\", prec / self.batch_size)\n",
        "\n",
        "pattern_machine = SingleLayerRegression(X, Y)\n",
        "pattern_machine.layer.debug = False\n",
        "\n",
        "# preset patterns with x and y as (0.0, 0.0) to (1.0, 1.0), i.e. y=x\n",
        "for csgb in [pattern_machine.layer.patterns.composite_signal_grid_begin, pattern_machine.layer.patterns.composite_signal_grid_end]:\n",
        "  csgb.components[0].pixels = torch.linspace(0, 1, 10).unsqueeze(1)\n",
        "  csgb.components[1].pixels = torch.linspace(0, 1, 10).unsqueeze(1)\n",
        "  csgb.components[0].variance = torch.zeros_like(csgb.components[0].variance)\n",
        "  csgb.components[1].variance = torch.zeros_like(csgb.components[0].variance)\n",
        "  csgb.components[0].refresh_precision()\n",
        "  csgb.components[1].refresh_precision()\n",
        "\n",
        "print(\"pattern_machine.layer.patterns.composite_signal_grid_begin\", pattern_machine.layer.patterns.composite_signal_grid_begin.pixels)\n",
        "pattern_machine.epoch()\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pattern_machine.layer.patterns.composite_signal_grid_begin [tensor([[0.0000],\n",
            "        [0.1111],\n",
            "        [0.2222],\n",
            "        [0.3333],\n",
            "        [0.4444],\n",
            "        [0.5556],\n",
            "        [0.6667],\n",
            "        [0.7778],\n",
            "        [0.8889],\n",
            "        [1.0000]]), tensor([[0.0000],\n",
            "        [0.1111],\n",
            "        [0.2222],\n",
            "        [0.3333],\n",
            "        [0.4444],\n",
            "        [0.5556],\n",
            "        [0.6667],\n",
            "        [0.7778],\n",
            "        [0.8889],\n",
            "        [1.0000]]), tensor([[0.0290, 0.0234, 0.0375, 0.0407, 0.0653, 0.0111, 0.0630, 0.0182, 0.0119,\n",
            "         0.0779],\n",
            "        [0.0353, 0.0911, 0.0508, 0.0614, 0.0151, 0.0111, 0.0523, 0.0741, 0.0915,\n",
            "         0.0518],\n",
            "        [0.0710, 0.0121, 0.0996, 0.0245, 0.0014, 0.0412, 0.0759, 0.0946, 0.0992,\n",
            "         0.0856],\n",
            "        [0.0530, 0.0993, 0.0133, 0.0055, 0.0917, 0.0521, 0.0895, 0.0133, 0.0703,\n",
            "         0.0149],\n",
            "        [0.0392, 0.0992, 0.0298, 0.0052, 0.0686, 0.0592, 0.0007, 0.0323, 0.0337,\n",
            "         0.0774],\n",
            "        [0.0448, 0.0098, 0.0620, 0.0113, 0.0047, 0.0445, 0.0749, 0.0173, 0.0358,\n",
            "         0.0364],\n",
            "        [0.0865, 0.0509, 0.0763, 0.0964, 0.0440, 0.0895, 0.0407, 0.0312, 0.0653,\n",
            "         0.0354],\n",
            "        [0.0073, 0.0744, 0.0139, 0.0987, 0.0736, 0.0689, 0.0390, 0.0810, 0.0937,\n",
            "         0.0795],\n",
            "        [0.0273, 0.0745, 0.0825, 0.0697, 0.0127, 0.0393, 0.0481, 0.0767, 0.0837,\n",
            "         0.0417],\n",
            "        [0.0059, 0.0374, 0.0602, 0.0275, 0.0693, 0.0789, 0.0122, 0.0463, 0.0191,\n",
            "         0.0359]])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7312f21ee3274f8b9dd27ecaa9d46574",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=506.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Sample --\n",
            "x, y 0.08967991173267365 0.5775052905082703\n",
            "Expected prediction 0.33359259366989136\n",
            "prediction (10 samples) 0.46666663885116577\n",
            "prediction precision 0.11766450852155685\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAAvCAYAAABAFRnJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABF0lEQVR4nO3aIU6DQRRG0dcGCIIEgWU5JCwOVsUi0AiCAt0GEn4sBuQd0XPsmE/dPDG7bdsGgMZ+9QCAUyK6ACHRBQiJLkBIdAFCZ/89Pj7fL//acHvxsXrCzMwcv89XT+CXy/3X6gkzM/P6ebN6wrwc12+YmXk7XK+eMO+Hq9UTZmbm6e5h99ebSxcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgtNu2bfUGgJPh0gUIiS5ASHQBQqILEBJdgJDoAoR+AHDMFlkdrLweAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "RMS Error 0.056254230594937706\n",
            "Mean precision tensor(0.4477)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa4LHN3uHaoX"
      },
      "source": [
        "# Discussion\n",
        "\n",
        "\n",
        "*   Trajectory beginnings of patterns are hardcoded as (0,0), (0.11, 0.11), (0.22, 0.22), ..., where the two numbers are values of X and Y modality. Trajectory endings are set to the same values as well.\n",
        "*   When X and Y are same, e.g. 0.33, then only that pattern is consistently the winner over multiple samples and the Y component of the end of trajectory is consistently 0.33, giving prediction 0.33 with 1.0 precision.\n",
        "*   When X and Y are different, e.g. 0.1 and 0.6, then we get both the (0.11, 0.11) pattern and (0.66, 0.66) pattern competing to be winner. As a result, sampling for winners and averaging Y component of the end of trajectory of those winners produces approximately the mean of 0.1 and 0.6, and precision is lower than 1.0.\n",
        "\n",
        "This shows that the Pattern Machine can act as a regression model.\n",
        "\n",
        "Next up, set up a PM that embodies a 1D bouncing ball (1 tall, few wide, left to right to left ball movement). Implement hippocalmpal theta-like cyclic top down future trajectory predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtGASzWWludc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}