{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR,\n",
    "    format='%(asctime)s.%(msecs)03d %(name)s:%(funcName)s %(levelname)s:%(message)s',\n",
    "    datefmt=\"%M:%S\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from skimage.draw import line_aa\n",
    "%matplotlib inline\n",
    "plt.style.use('classic')\n",
    "\n",
    "def show_image(image, vmin=None, vmax=None, title=None, print_values=False, figsize=(4, 4)):\n",
    "  #print(\"image \", image.shape)\n",
    "  image = image.cpu().numpy()\n",
    "  fig, ax1 = plt.subplots(figsize=figsize)\n",
    "  if title:\n",
    "    plt.title(title)\n",
    "  #i = image.reshape((height, width))\n",
    "  #print(\"i \", i.shape)\n",
    "  ax1.imshow(image, vmin=vmin, vmax=vmax, interpolation='none', cmap=plt.cm.plasma)\n",
    "  plt.show()\n",
    "  if print_values:\n",
    "    print(image)\n",
    "\n",
    "def generate_bouncing_pixel(width, height, count=100):\n",
    "  ball_width = 2\n",
    "  x = 3 #random.randint(0, width)\n",
    "  y = 2 #random.randint(0, height)\n",
    "  dx = -1 #random.randint(0, 2) - 1\n",
    "  dy = 1 #random.randint(0, 2) - 1\n",
    "  \n",
    "  images = []\n",
    "  for _ in range(count):\n",
    "    image = np.zeros((width, height))\n",
    "    image[x, y] = 1.0\n",
    "    image[x+1, y] = 1.0\n",
    "    image[x, y+1] = 1.0\n",
    "    image[x+1, y+1] = 1.0\n",
    "\n",
    "    #image=gaussian_filter(image, 0.5)\n",
    "    images.append(image)\n",
    "    x += dx\n",
    "    y += dy\n",
    "    if (x < 0 or x > width - 1 - (ball_width - 1)):\n",
    "      dx *= -1\n",
    "      x += dx\n",
    "    if (y < 0 or y > height - 1 - (ball_width - 1)):\n",
    "      dy *= -1\n",
    "      y += dy\n",
    "\n",
    "  return torch.as_tensor(images)\n",
    "\n",
    "def load_mnist(train=True, batch_size=64):\n",
    "  kwargs = {'num_workers': 1, 'pin_memory': True} if device==\"cuda\" else {}\n",
    "  loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=train, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "  \n",
    "  return loader\n",
    "\n",
    "def convolve(image_width, image_height, kernel_width, kernel_height, stride):\n",
    "  convolutions = []\n",
    "  for x in range(0, image_width - kernel_width + 1, stride):\n",
    "    for y in range(0, image_height - kernel_height + 1, stride):\n",
    "      convolutions.append([y, x, y + kernel_height, x + kernel_width])\n",
    "\n",
    "  return convolutions\n",
    "\n",
    "def conv_slice(images, image_width, image_height, kernel_width, kernel_height, stride):\n",
    "  convolutions = convolve(image_width, image_height, kernel_width, kernel_height, stride)\n",
    "  slices = [images[i, c[0]:c[2], c[1]:c[3]] for i in range(image_count) for c in convolutions]\n",
    "\n",
    "  slices = torch.stack(slices).float()\n",
    "  slices = slices.view(slices.shape[0], -1).to(device)\n",
    "  return slices\n",
    "\n",
    "\n",
    "def conv_join(slices, image_count, image_width, image_height, kernel_width, kernel_height, stride):\n",
    "  print(\"slices.shape\", slices.shape)\n",
    "  if len(slices.shape) == 2:\n",
    "    slices = slices.view(image_count, int(slices.shape[0] / image_count), kernel_width, kernel_height)\n",
    "\n",
    "  # slices is now (image count, convolutions, kernel size, kernel size)\n",
    "  #print(\"slices.shape\", slices.shape)\n",
    "\n",
    "  convolutions = convolve(image_width, image_height, kernel_width, kernel_height, stride)\n",
    "  assert len(convolutions) == slices.shape[1]\n",
    "\n",
    "  buffer = torch.zeros((image_count, image_width, image_height)).to(device)\n",
    "  #print(\"buffer\", buffer.shape)\n",
    "\n",
    "  for i in range(image_count):\n",
    "    #if i == 0:\n",
    "    #  print(\"slices[i]\", slices[i])\n",
    "\n",
    "    for x in range(image_width):\n",
    "      for y in range(image_height):\n",
    "        values = []\n",
    "        for c in range(len(convolutions)):\n",
    "          convolution = convolutions[c]\n",
    "          if x >= convolution[0] and x < convolution[2] and y >= convolution[1] and y < convolution[3]:\n",
    "            value = slices[i, c, x - convolution[0], y - convolution[1]]\n",
    "            values.append(value.item())\n",
    "            #if i == 0 and x == 3 and y == 3:\n",
    "            #  print(f\"x {x}, y {y}, convolution {convolution}\")\n",
    "            #  print(f\"[{x - convolution[0]}, {y - convolution[1]}]\", value)\n",
    "\n",
    "        #if i == 0 and x == 3 and y == 3:\n",
    "        #  print(f\"i {i}, x {x}, y {y}\")\n",
    "        #  print(\"values\", values)\n",
    "        if len(values) > 0:\n",
    "          value = np.average(values)\n",
    "          #print(\"value\", value)\n",
    "          buffer[i, x, y] = value\n",
    "\n",
    "  return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "  def __init__(self, input_width, input_height, feature_count):\n",
    "    super(VAE, self).__init__()\n",
    "    self.logger = logging.getLogger(self.__class__.__name__)\n",
    "    self.logger.setLevel(logging.WARN)\n",
    "\n",
    "    self.feature_count = feature_count\n",
    "\n",
    "    self.encoder = nn.Sequential(\n",
    "      nn.Linear(input_width * input_height    , input_width * input_height * 2),\n",
    "      #nn.BatchNorm1d(1),\n",
    "      nn.LeakyReLU(0.2, inplace=True),\n",
    "      nn.Linear(input_width * input_height * 2, input_width * input_height * 4),\n",
    "      #nn.BatchNorm1d(1),\n",
    "      nn.LeakyReLU(0.2, inplace=True),\n",
    "    )\n",
    "\n",
    "#     self.e_l1 = nn.Linear(input_width * input_height    , input_width * input_height * 2)\n",
    "#     self.e_b1 = nn.BatchNorm1d(1)\n",
    "#     self.e_r1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "#     self.e_l2 = nn.Linear(input_width * input_height * 2, input_width * input_height * 4)\n",
    "#     self.e_b2 = nn.BatchNorm1d(1)\n",
    "#     self.e_r2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "      \n",
    "    self.decoder = nn.Sequential(\n",
    "      nn.Linear(feature_count  , input_width * input_height * 2),\n",
    "      #nn.BatchNorm1d(1),\n",
    "      nn.LeakyReLU(0.2, inplace=True),\n",
    "      nn.Linear(input_width * input_height * 2, input_width * input_height),\n",
    "      #nn.BatchNorm2d(1),\n",
    "      nn.Sigmoid(),\n",
    "    )\n",
    "\n",
    "    self.linear_mu = nn.Linear(input_width * input_height * 4, feature_count)\n",
    "    self.linear_sigma = nn.Linear(input_width * input_height * 4, feature_count)\n",
    "\n",
    "    self.lrelu = nn.LeakyReLU()\n",
    "    self.relu = nn.ReLU()\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "    self.tanh = nn.Tanh()\n",
    "\n",
    "  def encode(self, x):\n",
    "    self.logger.debug(f\"x {x.shape}\")\n",
    "    x = self.encoder(x)\n",
    "#     x = self.e_l1(x)\n",
    "#     #x = self.e_b1(x)\n",
    "#     x = self.e_r1(x)\n",
    "#     x = self.e_l2(x)\n",
    "#     #x = self.e_b2(x)\n",
    "#     x = self.e_r2(x)\n",
    "    \n",
    "    #return self.tanh(self.linear_mu(x)), self.tanh(self.linear_sigma(x))\n",
    "    return self.sigmoid(self.linear_mu(x)), self.linear_sigma(x)\n",
    "\n",
    "  def decode(self, z):\n",
    "    #z = z.view(-1, 1, 1, self.feature_count)\n",
    "    self.logger.debug(f\"z {z.shape}\")\n",
    "    return self.decoder(z)\n",
    "\n",
    "  def reparametrize(self, mu, logvar):\n",
    "    std = logvar.mul(0.5).exp_()\n",
    "    eps = torch.FloatTensor(std.size()).normal_().to(device)\n",
    "    eps = eps.mul(std).add_(mu)\n",
    "    eps = torch.sigmoid(eps)\n",
    "    self.logger.debug(f\"eps {eps.shape}\")\n",
    "\n",
    "    return eps\n",
    "\n",
    "  def decode_features(self, mu, logvar):\n",
    "    z = self.reparametrize(mu, logvar)\n",
    "    self.logger.debug(f\"z {z.shape}\")\n",
    "    decoded = self.decode(z)\n",
    "    self.logger.debug(f\"decoded {decoded.shape}\")\n",
    "    return decoded, z\n",
    "\n",
    "    \n",
    "  def forward(self, x):\n",
    "    self.logger.debug(f\"x {x.shape}\")\n",
    "    mu, logvar = self.encode(x)\n",
    "    self.logger.debug(f\"mu {mu.shape}\")\n",
    "    self.logger.debug(f\"logvar {logvar.shape}\")\n",
    "    \n",
    "    decoded, z = self.decode_features(mu, logvar)\n",
    "    return decoded, mu, logvar, z\n",
    "  \n",
    "  \n",
    "class Network(nn.Module):\n",
    "  def __init__(self, image_width, image_height, kernel_width, kernel_height, stride, feature_count):\n",
    "    super(Network, self).__init__()\n",
    "    self.image_width = image_width\n",
    "    self.image_height = image_height\n",
    "    self.kernel_width = kernel_width\n",
    "    self.kernel_height = kernel_height\n",
    "    self.feature_count = feature_count\n",
    "    self.stride = stride\n",
    "\n",
    "    self.vae = VAE(kernel_width, kernel_height, feature_count)\n",
    "    self.trained = False\n",
    "    \n",
    "    if os.path.exists(self.save_path()):\n",
    "      self.load_state_dict(torch.load(self.save_path()))\n",
    "      self.eval()\n",
    "      self.trained = True\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.vae(x)\n",
    "\n",
    "  def loss_function(self, recon_x, x, mu, logvar):\n",
    "    # print(recon_x.size(), x.size())\n",
    "    BCE = F.binary_cross_entropy(recon_x.view(-1, self.kernel_width * self.kernel_height), x.view(-1, self.kernel_width * self.kernel_height), size_average=False)\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    # return BCE + KLD\n",
    "    BCE /= 0.00001\n",
    "    #print(BCE, KLD)\n",
    "    return BCE + KLD\n",
    "\n",
    "  def train(self, images, num_epochs=3000):\n",
    "    if self.trained:\n",
    "      return\n",
    "    \n",
    "    if isinstance(images, bool):\n",
    "      return\n",
    "\n",
    "    print(\"images\", images.shape)\n",
    "\n",
    "    input = conv_slice(images, self.image_width, self.image_height, self.kernel_width, self.kernel_height, self.stride)\n",
    "    learning_rate = 1e-3\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "    done = False\n",
    "    dataset = torch.utils.data.TensorDataset(input)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    epoch = 0\n",
    "    while not done:\n",
    "      for i, batch in enumerate(data_loader):\n",
    "        batch = batch[0].to(device)\n",
    "        output, mu, logvar, z = self(batch)\n",
    "        loss = self.loss_function(output, batch, mu, logvar)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "      if epoch % int(num_epochs / 10) == 0:\n",
    "        output, mu, logvar, z = self(input[0].unsqueeze(dim=0))\n",
    "        print('epoch [{}/{}], loss:{:.4f}'\n",
    "             .format(epoch+1, num_epochs, loss.item()))\n",
    "        show_image(output[0].view(self.kernel_height, self.kernel_width).detach(), title=f\"output {0}\", vmin=0, vmax=1)\n",
    "\n",
    "      if (loss.item() < 0.001 and epoch > 1500) or epoch > num_epochs:\n",
    "        done = True\n",
    "\n",
    "      epoch += 1\n",
    "\n",
    "    torch.save(self.state_dict(), self.save_path())\n",
    "\n",
    "  def eval_data(self, images):\n",
    "    input = conv_slice(images, self.image_width, self.image_height, self.kernel_width, self.kernel_height, self.stride)\n",
    "\n",
    "    reconstructed_slices, mu, logvar, z = self(input)\n",
    "    #reconstructed_slices = output[0]\n",
    "    reconstructed_images = conv_join(reconstructed_slices, images.shape[0], self.image_width, self.image_height, self.kernel_width, self.kernel_height, self.stride)\n",
    "    return reconstructed_images, reconstructed_slices, mu, logvar, z\n",
    "    \n",
    "  def save_path(self):\n",
    "    return f\"network.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n",
      "images torch.Size([64, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amolk/work/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/4000], loss:94614072.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-201128277551>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mreconstructed_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstructed_slices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f08b2522034e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, images, num_epochs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# ===================backward====================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/miniconda3/lib/python3.6/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if True:\n",
    "  import os\n",
    "  import glob\n",
    "\n",
    "  files = glob.glob('./*.pt')\n",
    "  for f in files:\n",
    "      os.remove(f)\n",
    "\n",
    "image_height = image_width = 16\n",
    "image_count = 20\n",
    "kernel_size = 5\n",
    "stride = 2\n",
    "feature_count = 4\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "#images = generate_bouncing_pixel(image_height, image_width, count=image_count)\n",
    "images = next(iter(load_mnist()))[0].squeeze(dim=1)\n",
    "image_count, image_height, image_width = images.shape\n",
    "\n",
    "network = Network(image_width=image_width, image_height=image_height, kernel_width=kernel_size, kernel_height=kernel_size, stride=stride, feature_count=feature_count).to(device)\n",
    "network.train(images, num_epochs=4000)\n",
    "reconstructed_images, reconstructed_slices, mu, logvar, z = network.eval_data(images)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "  print(f\"------------------------- IMAGE {i} --------------------------\")\n",
    "  show_image(images[i].detach(), vmin=0, vmax=1, title=f\"images image {i}\")\n",
    "  show_image(reconstructed_images[i].detach(), vmin=0, vmax=1, title=f\"reconstructed_images {i}\")\n",
    "  show_image(images[i].float()-reconstructed_images[i].detach().cpu(), title=f\"diff {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
