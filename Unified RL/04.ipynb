{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"04.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"sfftoja2WDCb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"6475b8a7-22ed-4194-d92d-50de6e2c5062","executionInfo":{"status":"error","timestamp":1561411213398,"user_tz":420,"elapsed":77582,"user":{"displayName":"Amol Kelkar","photoUrl":"","userId":"07278259258766517376"}}},"source":["import sys\n","import torch  \n","import gym\n","import numpy as np  \n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import matplotlib.pyplot as plt\n","\n","# Constants\n","GAMMA = 0.99\n","\n","class PolicyNetwork(nn.Module):\n","\n","    def __init__(self, num_inputs, num_actions, hidden_size=256):\n","        super(PolicyNetwork, self).__init__()\n","        self.num_actions = num_actions\n","        self.linear1 = nn.Linear(num_inputs, hidden_size)\n","        self.linear2 = nn.Linear(hidden_size, num_actions)\n","\n","    def forward(self, state):\n","        x = F.relu(self.linear1(state))\n","        x = F.softmax(self.linear2(x), dim=1)\n","        return x \n","    \n","class Agent:\n","\n","    def __init__(self, env, learning_rate=3e-4):\n","        self.env = env\n","        self.num_actions = self.env.action_space.n\n","        self.policy_network = PolicyNetwork(self.env.observation_space.shape[0], self.env.action_space.n)\n","        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=learning_rate)\n","\n","    def get_action(self, state):\n","        state = torch.from_numpy(state).float().unsqueeze(0)\n","        probs = self.policy_network.forward(Variable(state))\n","        highest_prob_action = np.random.choice(self.num_actions, p=np.squeeze(probs.detach().numpy()))\n","        log_prob = torch.log(probs.squeeze(0)[highest_prob_action])\n","        return highest_prob_action, log_prob\n","\n","    def update_policy(self, rewards, log_probs):\n","        discounted_rewards = []\n","\n","        for t in range(len(rewards)):\n","            Gt = 0 \n","            pw = 0\n","            for r in rewards[t:]:\n","                Gt = Gt + GAMMA**pw * r\n","                pw = pw + 1\n","            discounted_rewards.append(Gt)\n","            \n","        discounted_rewards = torch.FloatTensor(discounted_rewards)\n","        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9) # normalize discounted rewards\n","\n","        policy_gradient = []\n","        for log_prob, Gt in zip(log_probs, discounted_rewards):\n","            policy_gradient.append(-log_prob * Gt)\n","        \n","        self.optimizer.zero_grad()\n","        policy_gradient = torch.stack(policy_gradient).sum()\n","        policy_gradient.backward()\n","        self.optimizer.step()\n","\n","    def train(self, max_episode=3000, max_step=200):\n","        for episode in range(max_episode):\n","            state = env.reset()\n","            log_probs = []\n","            rewards = []\n","            episode_reward = 0\n","\n","            for steps in range(max_step):\n","                action, log_prob = self.get_action(state)\n","                new_state, reward, done, _ = self.env.step(action)\n","                \n","                log_probs.append(log_prob)\n","                rewards.append(reward)\n","                episode_reward += reward\n","\n","                if done:\n","                    self.update_policy(rewards, log_probs)\n","                    if episode % 10 == 0:\n","                        print(\"episode \" + str(episode) + \": \" + str(episode_reward))\n","\n","                    break\n","                \n","                state = new_state     \n","# def main():\n","#     env = gym.make('obstacle-v0')\n","#     policy_net = PolicyNetwork(env.observation_space.shape[0], env.action_space.n, 128)\n","    \n","#     max_episode_num = 5000\n","#     max_steps = 10000\n","#     numsteps = []\n","#     avg_numsteps = []\n","#     all_rewards = []\n","\n","#     for episode in range(max_episode_num):\n","#         state = env.reset()\n","#         log_probs = []\n","#         rewards = []\n","\n","#         for steps in range(max_steps):\n","#             env.render()\n","#             action, log_prob = policy_net.get_action(state)\n","#             new_state, reward, done, _ = env.step(action)\n","#             log_probs.append(log_prob)\n","#             rewards.append(reward)\n","\n","#             if done:\n","#                 update_policy(policy_net, rewards, log_probs)\n","#                 numsteps.append(steps)\n","#                 avg_numsteps.append(np.mean(numsteps[-10:]))\n","#                 all_rewards.append(np.sum(rewards))\n","#                 if episode % 1 == 0:\n","#                     sys.stdout.write(\"episode: {}, total reward: {}, average_reward: {}, length: {}\\n\".format(episode, np.round(np.sum(rewards), decimals = 3),  np.round(np.mean(all_rewards[-10:]), decimals = 3), steps))\n","\n","#                 break\n","            \n","#             state = new_state\n","        \n","#     plt.plot(numsteps)\n","#     plt.plot(avg_numsteps)\n","#     plt.xlabel('Episode')\n","#     plt.show()\n","        \n","if __name__ == '__main__':\n","    env = gym.make('CartPole-v0')\n","    agent = Agent(env)\n","    agent.train(3000,200)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["episode 0: 17.0\n","episode 10: 30.0\n","episode 20: 21.0\n","episode 30: 24.0\n","episode 40: 23.0\n","episode 50: 50.0\n","episode 60: 19.0\n","episode 70: 18.0\n","episode 80: 15.0\n","episode 90: 12.0\n","episode 100: 26.0\n","episode 110: 21.0\n","episode 120: 14.0\n","episode 130: 12.0\n","episode 140: 29.0\n","episode 150: 57.0\n","episode 160: 28.0\n","episode 170: 24.0\n","episode 180: 17.0\n","episode 190: 29.0\n","episode 200: 33.0\n","episode 210: 34.0\n","episode 220: 16.0\n","episode 230: 58.0\n","episode 240: 44.0\n","episode 250: 98.0\n","episode 260: 49.0\n","episode 270: 94.0\n","episode 280: 116.0\n","episode 290: 200.0\n","episode 300: 59.0\n","episode 310: 112.0\n","episode 320: 89.0\n","episode 330: 125.0\n","episode 340: 62.0\n","episode 350: 188.0\n","episode 360: 55.0\n","episode 370: 36.0\n","episode 380: 165.0\n","episode 390: 150.0\n","episode 400: 109.0\n","episode 410: 49.0\n","episode 420: 41.0\n","episode 430: 149.0\n","episode 440: 87.0\n","episode 450: 114.0\n","episode 460: 178.0\n","episode 470: 134.0\n","episode 480: 98.0\n","episode 490: 200.0\n","episode 500: 150.0\n","episode 510: 79.0\n","episode 520: 78.0\n","episode 530: 125.0\n","episode 540: 200.0\n","episode 550: 115.0\n","episode 560: 93.0\n","episode 570: 98.0\n","episode 580: 171.0\n","episode 590: 200.0\n","episode 600: 200.0\n","episode 610: 200.0\n","episode 620: 200.0\n","episode 630: 69.0\n","episode 640: 200.0\n","episode 650: 135.0\n","episode 660: 180.0\n","episode 670: 122.0\n","episode 680: 200.0\n","episode 690: 126.0\n","episode 700: 133.0\n","episode 710: 121.0\n","episode 720: 200.0\n","episode 730: 200.0\n","episode 740: 200.0\n","episode 750: 200.0\n","episode 760: 200.0\n","episode 770: 147.0\n","episode 780: 175.0\n","episode 790: 200.0\n","episode 800: 130.0\n","episode 810: 200.0\n","episode 820: 180.0\n","episode 830: 200.0\n","episode 840: 50.0\n","episode 850: 200.0\n","episode 860: 200.0\n","episode 870: 176.0\n","episode 880: 200.0\n","episode 890: 200.0\n","episode 900: 16.0\n","episode 910: 200.0\n","episode 920: 200.0\n","episode 930: 200.0\n","episode 940: 154.0\n","episode 950: 182.0\n","episode 960: 200.0\n","episode 970: 200.0\n","episode 980: 200.0\n","episode 990: 200.0\n","episode 1000: 200.0\n","episode 1010: 105.0\n","episode 1020: 200.0\n","episode 1030: 200.0\n","episode 1040: 200.0\n","episode 1050: 182.0\n","episode 1060: 200.0\n","episode 1070: 200.0\n","episode 1080: 182.0\n","episode 1090: 200.0\n","episode 1100: 176.0\n","episode 1110: 195.0\n","episode 1120: 200.0\n","episode 1130: 200.0\n","episode 1140: 200.0\n","episode 1150: 200.0\n","episode 1160: 200.0\n","episode 1170: 200.0\n","episode 1180: 150.0\n","episode 1190: 156.0\n","episode 1200: 174.0\n","episode 1210: 200.0\n","episode 1220: 200.0\n","episode 1230: 200.0\n","episode 1240: 188.0\n","episode 1250: 169.0\n","episode 1260: 200.0\n","episode 1270: 200.0\n","episode 1280: 200.0\n","episode 1290: 190.0\n","episode 1300: 200.0\n","episode 1310: 200.0\n","episode 1320: 177.0\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-61e5457012cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CartPole-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-4-61e5457012cf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, max_episode, max_step)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"episode \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\": \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-61e5457012cf>\u001b[0m in \u001b[0;36mupdate_policy\u001b[0;34m(self, rewards, log_probs)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mpolicy_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_gradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mpolicy_gradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"jaf3gIUrWPzf","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}