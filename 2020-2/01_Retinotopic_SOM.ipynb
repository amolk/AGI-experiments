{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01 Retinotopic SOM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOMgShqVc18sKXasM0vbXar",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amolk/AGI-experiments/blob/master/2020-2/01_Retinotopic_SOM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS3WeHLy5zbx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MEqqNow59rD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "0e0dcfd4-12c4-46ff-8f42-16fc4d97d15c"
      },
      "source": [
        "%%writefile hp.py\n",
        "import numpy\n",
        "import math\n",
        "\n",
        "class LayerHP:\n",
        "  def __init__(self):\n",
        "    self.input_shape = None\n",
        "    self.output_shape = None\n",
        "\n",
        "    self.alpha = 0.1   # learning rate\n",
        "    self.alpha_n = 0.01 # neighbor learning rate\n",
        "    \n",
        "    self.enable_neighborhood_competition = False\n",
        "    self.neighborhood_shape = (4, 4)\n",
        "\n",
        "    # self.enable_neighborhood_smoothing = True\n",
        "    # self.enable_precision = True\n",
        "    # self.enable_precision_weighted_distance = True\n",
        "    # self.enable_per_neuron_learning_rate = True\n",
        "    # self.enable_retinotopy = True\n",
        "    # self.retinotopic_local_grid_shape = (12, 12)\n",
        "\n",
        "  @property\n",
        "  def input_size(self):\n",
        "    return numpy.prod(self.input_shape)\n",
        "\n",
        "  @property\n",
        "  def output_size(self):\n",
        "    return numpy.prod(self.output_shape)\n",
        "\n",
        "class DatasetHP:\n",
        "  def __init__(self):\n",
        "    self.image_batch_size = 2\n",
        "    self.kernel_shape = (5, 5)\n",
        "    self.patch_grid_shape = (10, 10)\n",
        "\n",
        "  @property\n",
        "  def kernel_size(self):\n",
        "    return numpy.prod(self.kernel_shape)\n",
        "\n",
        "  @property\n",
        "  def patch_grid_size(self):\n",
        "    return numpy.prod(self.patch_grid_shape)\n",
        "\n",
        "class HP:\n",
        "  def __init__(self):\n",
        "    self.mask_contrast = 3\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting hp.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPyM7V-h8E4z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "c12ec6fd-cdcb-4718-f755-5dcc547f81e2"
      },
      "source": [
        "%%writefile util.py\n",
        "import torch\n",
        "import pdb\n",
        "from hp import HP\n",
        "from skimage.util.shape import view_as_windows\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class Util:\n",
        "  @staticmethod\n",
        "  def add_gaussian_noise(tensor, mean=0., std=1.):\n",
        "    t = tensor + torch.randn(tensor.size()).to(device) * std + mean\n",
        "    t.to(device)\n",
        "    return t\n",
        "\n",
        "  @staticmethod\n",
        "  def norm_scale(t):\n",
        "    epsilon = 0.00001\n",
        "\n",
        "    assert len(t.shape)==2, \"t shape must be (batch,size)\"\n",
        "    # print(\"mean\", t.mean())\n",
        "    # print(\"std\", t.std())\n",
        "    std = t.std(dim=-1).unsqueeze(-1)\n",
        "    std[std<epsilon] = epsilon\n",
        "    t = (t - t.mean(dim=-1).unsqueeze(-1)) / std\n",
        "    # print(\"max\", t.max())\n",
        "    # print(\"min\", t.min())\n",
        "    scale = (t.max(dim=-1)[0] - t.min(dim=-1)[0]).unsqueeze(-1)\n",
        "    scale[scale<epsilon] = epsilon\n",
        "    t = t / scale\n",
        "\n",
        "    t = t - t.min(dim=-1)[0].unsqueeze(-1)\n",
        "    # print(\"max\", t.max())\n",
        "    # print(\"min\", t.min())\n",
        "\n",
        "    return t\n",
        "\n",
        "  @staticmethod\n",
        "  def get_image_patch_indices(image_shape, kernel_shape, patch_grid_shape):\n",
        "    assert len(image_shape) == 2, \"Must be (image height, image width)\"\n",
        "    assert len(kernel_shape) == 2, \"Only 2D kernels allowed\"\n",
        "    assert len(patch_grid_shape) == 2, \"Only 2D patch grids allowed\"\n",
        "\n",
        "    image_height, image_width = image_shape\n",
        "    kernel_height, kernel_width = kernel_shape\n",
        "    patch_grid_height, patch_grid_width = patch_grid_shape\n",
        "\n",
        "    # We will find patch top-left coordinates\n",
        "    # First patch has top-left coordinates (0, 0)\n",
        "    # Last patch has top-left coordinates (image_height - kernel_height)\n",
        "    # (h, w)th patch has top-left coordinates\n",
        "    #      (h / patch_grid_height) * (image_height - kernel_height) ,\n",
        "    #      (w / patch_grid_width)  * (image_width  - kernel_width)\n",
        "\n",
        "    indices_y, indices_x = torch.meshgrid(\n",
        "        torch.linspace(0, image_height - kernel_height, patch_grid_height).long(),\n",
        "        torch.linspace(0, image_width  - kernel_width,  patch_grid_width ).long()\n",
        "    )\n",
        "    return indices_y, indices_x\n",
        "\n",
        "  @staticmethod\n",
        "  def get_image_patches(image, config):\n",
        "    image_as_windows = view_as_windows(image.numpy(), config.kernel_shape)\n",
        "    return torch.from_numpy(image_as_windows[config.image_patch_indices_y, config.image_patch_indices_x]).to(device)\n",
        "\n",
        "  @staticmethod\n",
        "  def conv_slice(images, kernel_shape, stride):\n",
        "    assert len(images.shape) == 3, \"Must be (image count, image height, image width)\"\n",
        "    images = images.unsqueeze(1)\n",
        "\n",
        "    fold_params = dict(kernel_size=kernel_shape, dilation=1, padding=0, stride=stride)\n",
        "    unfold = torch.nn.Unfold(**fold_params)\n",
        "    print(\"images\", images.shape)\n",
        "    unfolded = unfold(images)\n",
        "    unfolded = unfolded.view(images.shape[0], -1, unfolded.shape[-1])\n",
        "    unfolded = unfolded.transpose(1, 2)\n",
        "    return unfolded\n",
        "\n",
        "  @staticmethod\n",
        "  def var_to_precision(pattern_var, hp):\n",
        "    return torch.exp(-pattern_var * hp.mask_contrast)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting util.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzZVwNIE8MCF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "b0afb3c9-ceb7-412c-a239-070b8115e58a"
      },
      "source": [
        "%%writefile graphics_util.py\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "from util import Util\n",
        "\n",
        "class GraphicsUtil:\n",
        "  @staticmethod\n",
        "  def show_image_grid(images, alpha=None, vmin=0, vmax=1, grid_width=None, grid_height=None):\n",
        "    assert len(images.shape) == 3, \"Must be (image count, image height, image width)\"\n",
        "    (image_count, image_height, image_width) = images.shape\n",
        "\n",
        "    if alpha is not None:\n",
        "      assert images.shape == alpha.shape\n",
        "\n",
        "    if grid_width is None or grid_height is None:\n",
        "      image_grid_size = math.floor(image_count ** 0.5)\n",
        "      if image_grid_size > 15:\n",
        "        image_grid_size = 15\n",
        "\n",
        "      images = images[0:image_grid_size*image_grid_size]\n",
        "      if alpha is not None:\n",
        "        alpha = alpha[0:image_grid_size*image_grid_size]\n",
        "\n",
        "      grid_width = image_grid_size\n",
        "      grid_height = image_grid_size\n",
        "    else:\n",
        "      assert grid_width * grid_height == image_count\n",
        "\n",
        "    gutter_size = 2\n",
        "    composite_image = torch.zeros(((image_height + gutter_size) * grid_height - gutter_size, (image_width + gutter_size) * grid_width - gutter_size))\n",
        "    if alpha is not None:\n",
        "      composite_alpha = torch.ones_like(composite_image)\n",
        "\n",
        "    for row in range(grid_height):\n",
        "      for col in range(grid_width):\n",
        "        r1 = row * (image_height + gutter_size)\n",
        "        r2 = r1 + image_height\n",
        "        c1 = col * (image_width + gutter_size)\n",
        "        c2 = c1 + image_width\n",
        "        index = row * grid_height + col\n",
        "        composite_image[r1:r2, c1:c2] = images[index] \n",
        "        if alpha is not None:\n",
        "          composite_alpha[r1:r2, c1:c2] = alpha[index] \n",
        "\n",
        "    if alpha is None:\n",
        "      composite_alpha = None\n",
        "\n",
        "    plt.figure(figsize=(grid_height * 0.7, grid_width * 0.7))\n",
        "    plt.imshow(composite_image.cpu().numpy(), alpha=composite_alpha, vmin=vmin, vmax=vmax, interpolation='none', cmap=plt.cm.viridis)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting graphics_util.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8-oUxcM8Onn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "2ee088b5-b1c2-4fbb-9e28-7ba975b8d142"
      },
      "source": [
        "%%writefile dataset_loader.py\n",
        "from util import Util\n",
        "from graphics_util import GraphicsUtil\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def load_mnist(train=True, batch_size=64):\n",
        "  kwargs = {'num_workers': 1, 'pin_memory': True} if device==\"cuda\" else {}\n",
        "  loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=train, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                   ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "  \n",
        "  return loader\n",
        "\n",
        "def load_cifar10(train=True, batch_size=64):\n",
        "  kwargs = {'num_workers': 1, 'pin_memory': True} if device==\"cuda\" else {}\n",
        "  loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=train, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                   ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "  \n",
        "  return loader\n",
        "\n",
        "def load_imagefolder(batch_size=64):\n",
        "  loader = torch.utils.data.DataLoader(\n",
        "      datasets.ImageFolder('/content/drive/My Drive/work/datasets/digit',\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                   ])),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "  \n",
        "  return loader\n",
        "\n",
        "def load_dataset(config):\n",
        "  images = next(iter(load_mnist(batch_size=config.image_batch_size)))[0]\n",
        "  images = images[:, 0, :, :]\n",
        "  \n",
        "  # plt.imshow(images[0].permute(1,2,0))\n",
        "  # plt.show()\n",
        "\n",
        "  ib, ih, iw = images.shape\n",
        "  config.image_shape = (ih, iw)\n",
        "  print(\"images\", images.shape)\n",
        "\n",
        "  plt.imshow(images[0], vmin=0, vmax=1)\n",
        "  plt.show()\n",
        "\n",
        "  image_patch_indices_y, image_patch_indices_x = Util.get_image_patch_indices(config.image_shape, config.kernel_shape, config.patch_grid_shape)\n",
        "\n",
        "  config.image_patch_indices_y = image_patch_indices_y\n",
        "  config.image_patch_indices_x = image_patch_indices_x\n",
        "\n",
        "  image0_patches = Util.get_image_patches(images[0], config)\n",
        "  GraphicsUtil.show_image_grid(image0_patches.view(-1, config.kernel_shape[0], config.kernel_shape[0]))\n",
        "\n",
        "  dataset = torch.utils.data.TensorDataset(images)\n",
        "  \n",
        "  loader = torch.utils.data.DataLoader(dataset, batch_size=1, pin_memory=False)\n",
        "  return loader, images"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting dataset_loader.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtBsKYEyBCyH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "fff2da1e-de83-4c5f-d03f-4bfbefa036c3"
      },
      "source": [
        "%aimport hp\n",
        "%aimport util\n",
        "%aimport graphics_util\n",
        "%aimport dataset_loader\n",
        "\n",
        "from hp import DatasetHP\n",
        "from dataset_loader import load_dataset\n",
        "from graphics_util import GraphicsUtil\n",
        "\n",
        "dataset_hp = DatasetHP()\n",
        "original_dataset, original_images = dataset_loader.load_dataset(dataset_hp)\n",
        "# GraphicsUtil.show_image_grid(original_dataset.dataset.tensors[0][100:200].view(-1, HP.image_height, HP.image_width), vmin=0, vmax=1)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "images torch.Size([2, 28, 28])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oe_pIAuBI2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}